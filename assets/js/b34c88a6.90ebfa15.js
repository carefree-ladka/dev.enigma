"use strict";(self.webpackChunkdev_enigma=self.webpackChunkdev_enigma||[]).push([[63704],{181:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>o,contentTitle:()=>a,default:()=>u,frontMatter:()=>t,metadata:()=>l,toc:()=>d});var i=s(74848),r=s(28453);const t={},a="LeetCode-like Online Judge Platform",l={id:"System Design Problems/LeetCode-like Online Judge Platform",title:"LeetCode-like Online Judge Platform",description:"Table of Contents",source:"@site/docs/System Design Problems/LeetCode-like Online Judge Platform.mdx",sourceDirName:"System Design Problems",slug:"/System Design Problems/LeetCode-like Online Judge Platform",permalink:"/docs/System Design Problems/LeetCode-like Online Judge Platform",draft:!1,unlisted:!1,editUrl:"https://github.com/carefree-ladka/docs/System Design Problems/LeetCode-like Online Judge Platform.mdx",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Google Search System Design",permalink:"/docs/System Design Problems/Google Search System Design"},next:{title:"Netflix System Design",permalink:"/docs/System Design Problems/Netflix System Design"}},o={},d=[{value:"Table of Contents",id:"table-of-contents",level:2},{value:"Requirements",id:"requirements",level:2},{value:"Functional Requirements",id:"functional-requirements",level:3},{value:"Non-Functional Requirements",id:"non-functional-requirements",level:3},{value:"Capacity Estimation",id:"capacity-estimation",level:2},{value:"Traffic Estimates",id:"traffic-estimates",level:3},{value:"Storage Estimates",id:"storage-estimates",level:3},{value:"Bandwidth Estimates",id:"bandwidth-estimates",level:3},{value:"Memory Estimates",id:"memory-estimates",level:3},{value:"Server Estimates",id:"server-estimates",level:3},{value:"High-Level Design",id:"high-level-design",level:2},{value:"Architecture Overview",id:"architecture-overview",level:3},{value:"Component Description",id:"component-description",level:3},{value:"Deep Dives",id:"deep-dives",level:2},{value:"Deep Dive 1: Code Execution Service",id:"deep-dive-1-code-execution-service",level:3},{value:"Architecture",id:"architecture",level:4},{value:"Implementation Details",id:"implementation-details",level:4},{value:"Deep Dive 2: Leaderboard Service",id:"deep-dive-2-leaderboard-service",level:3},{value:"Architecture",id:"architecture-1",level:4},{value:"Implementation",id:"implementation",level:4},{value:"Deep Dive 3: Rate Limiting &amp; Anti-Cheating",id:"deep-dive-3-rate-limiting--anti-cheating",level:3},{value:"Rate Limiting Strategy",id:"rate-limiting-strategy",level:4},{value:"Implementation",id:"implementation-1",level:4},{value:"Deep Dive 4: Database Schema &amp; Indexing",id:"deep-dive-4-database-schema--indexing",level:3},{value:"Schema Design",id:"schema-design",level:4},{value:"Database Schema",id:"database-schema",level:2},{value:"Partitioning Strategy",id:"partitioning-strategy",level:3},{value:"API Design",id:"api-design",level:2},{value:"REST API Endpoints",id:"rest-api-endpoints",level:3},{value:"Example API Request/Response",id:"example-api-requestresponse",level:3},{value:"Scalability &amp; Optimization",id:"scalability--optimization",level:2},{value:"Caching Strategy",id:"caching-strategy",level:3},{value:"Cache Layers",id:"cache-layers",level:3},{value:"Database Optimization",id:"database-optimization",level:3},{value:"Horizontal Scaling",id:"horizontal-scaling",level:3},{value:"Performance Optimization",id:"performance-optimization",level:3},{value:"Monitoring &amp; Observability",id:"monitoring--observability",level:3},{value:"Disaster Recovery",id:"disaster-recovery",level:3},{value:"Security Measures",id:"security-measures",level:3},{value:"Cost Optimization",id:"cost-optimization",level:3},{value:"Advanced Features",id:"advanced-features",level:3},{value:"Summary",id:"summary",level:2},{value:"Key Design Decisions",id:"key-design-decisions",level:3},{value:"Trade-offs",id:"trade-offs",level:3},{value:"Estimated Costs (AWS)",id:"estimated-costs-aws",level:3}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"leetcode-like-online-judge-platform",children:"LeetCode-like Online Judge Platform"})}),"\n",(0,i.jsx)(n.h2,{id:"table-of-contents",children:"Table of Contents"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#requirements",children:"Requirements"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#capacity-estimation",children:"Capacity Estimation"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#high-level-design",children:"High-Level Design (HLD)"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#deep-dives",children:"Deep Dives"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#database-schema",children:"Database Schema"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#api-design",children:"API Design"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#scalability-optimization",children:"Scalability & Optimization"})}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"requirements",children:"Requirements"}),"\n",(0,i.jsx)(n.h3,{id:"functional-requirements",children:"Functional Requirements"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Core Features:"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"User Management"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"User registration, login, authentication"}),"\n",(0,i.jsx)(n.li,{children:"User profiles with submission history"}),"\n",(0,i.jsx)(n.li,{children:"Premium/free tier management"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Problem Management"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Browse problems by difficulty, topic, company tags"}),"\n",(0,i.jsx)(n.li,{children:"View problem description, examples, constraints"}),"\n",(0,i.jsx)(n.li,{children:"Search and filter problems"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Code Submission & Execution"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Submit code in multiple languages (Java, Python, C++, JavaScript)"}),"\n",(0,i.jsx)(n.li,{children:"Execute code against test cases"}),"\n",(0,i.jsx)(n.li,{children:"Return results (Accepted, Wrong Answer, TLE, MLE, Runtime Error)"}),"\n",(0,i.jsx)(n.li,{children:"Show execution time and memory usage"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Test Cases"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Run against hidden test cases"}),"\n",(0,i.jsx)(n.li,{children:"Allow users to run custom test cases"}),"\n",(0,i.jsx)(n.li,{children:"Support edge cases and performance tests"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Leaderboard & Statistics"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Track user rankings"}),"\n",(0,i.jsx)(n.li,{children:"Show submission statistics"}),"\n",(0,i.jsx)(n.li,{children:"Display acceptance rate"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Discussion Forum"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Post and view solutions"}),"\n",(0,i.jsx)(n.li,{children:"Comment and upvote discussions"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"non-functional-requirements",children:"Non-Functional Requirements"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Performance"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Code execution < 10 seconds"}),"\n",(0,i.jsx)(n.li,{children:"API response time < 200ms"}),"\n",(0,i.jsx)(n.li,{children:"Support concurrent submissions"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Scalability"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Handle 10M+ users"}),"\n",(0,i.jsx)(n.li,{children:"Process 100K+ submissions/hour during peak"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Security"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Sandbox code execution"}),"\n",(0,i.jsx)(n.li,{children:"Prevent malicious code"}),"\n",(0,i.jsx)(n.li,{children:"Rate limiting on submissions"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Availability"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"99.9% uptime"}),"\n",(0,i.jsx)(n.li,{children:"Fault-tolerant architecture"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Consistency"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Strong consistency for submission results"}),"\n",(0,i.jsx)(n.li,{children:"Eventual consistency for leaderboards"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"capacity-estimation",children:"Capacity Estimation"}),"\n",(0,i.jsx)(n.h3,{id:"traffic-estimates",children:"Traffic Estimates"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Assumptions:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Total users: 10 million"}),"\n",(0,i.jsx)(n.li,{children:"Daily Active Users (DAU): 1 million (10%)"}),"\n",(0,i.jsx)(n.li,{children:"Average submissions per active user: 5/day"}),"\n",(0,i.jsxs)(n.li,{children:["Read",":Write"," ratio: 80:20"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Daily Metrics:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Daily Submissions = 1M users \xd7 5 submissions = 5M submissions/day\nSubmissions per second (avg) = 5M / 86400 = ~58 submissions/sec\nPeak submissions (3x avg) = 174 submissions/sec\n\nProblem views per day = 1M users \xd7 20 views = 20M views/day\nViews per second (avg) = 20M / 86400 = ~231 views/sec\nPeak views = 693 views/sec\n"})}),"\n",(0,i.jsx)(n.h3,{id:"storage-estimates",children:"Storage Estimates"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Code Submissions:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Average code size = 2 KB\nDaily submissions = 5M\nDaily storage = 5M \xd7 2 KB = 10 GB/day\nAnnual storage = 10 GB \xd7 365 = 3.65 TB/year\nWith 3-year retention = ~11 TB\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Test Cases:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Total problems = 3000\nAverage test cases per problem = 100\nAverage test case size = 1 KB\nTotal test case storage = 3000 \xd7 100 \xd7 1 KB = 300 MB\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"User Data:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"User profile = 10 KB per user\n10M users \xd7 10 KB = 100 GB\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Total Storage (3 years):"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Submissions: 11 TB\nTest Cases: 300 MB\nUser Data: 100 GB\nProblem Descriptions: 10 GB\nTotal: ~11.2 TB\n"})}),"\n",(0,i.jsx)(n.h3,{id:"bandwidth-estimates",children:"Bandwidth Estimates"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Incoming (Write):"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Submissions: 58 req/sec \xd7 2 KB = 116 KB/sec\nPeak: 174 req/sec \xd7 2 KB = 348 KB/sec = ~2.8 Mbps\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Outgoing (Read):"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Problem views: 231 req/sec \xd7 50 KB = 11.5 MB/sec\nResults: 58 req/sec \xd7 5 KB = 290 KB/sec\nTotal: ~12 MB/sec = ~96 Mbps\nPeak: ~288 Mbps\n"})}),"\n",(0,i.jsx)(n.h3,{id:"memory-estimates",children:"Memory Estimates"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Cache Requirements:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Popular problems (top 100): 100 \xd7 50 KB = 5 MB\nUser sessions (100K concurrent): 100K \xd7 1 KB = 100 MB\nLeaderboard data: 50 MB\nTotal cache: ~200 MB (with overhead: 500 MB)\n"})}),"\n",(0,i.jsx)(n.h3,{id:"server-estimates",children:"Server Estimates"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Application Servers:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Requests per second (total): 289 req/sec (avg), 867 req/sec (peak)\nRequests per server: 100 req/sec\nRequired servers: 867 / 100 = ~9 servers (with redundancy: 12 servers)\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Execution Workers:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Peak submissions: 174 submissions/sec\nExecution time per submission: 5 sec average\nConcurrent executions needed: 174 \xd7 5 = 870 workers\nWith safety margin: 1000 workers\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"high-level-design",children:"High-Level Design"}),"\n",(0,i.jsx)(n.h3,{id:"architecture-overview",children:"Architecture Overview"}),"\n",(0,i.jsx)(n.mermaid,{value:'graph TB\n    subgraph "Client Layer"\n        WEB[Web Browser]\n        MOBILE[Mobile App]\n    end\n\n    subgraph "CDN & Load Balancer"\n        CDN[CDN<br/>Static Content]\n        LB[Load Balancer<br/>Nginx/HAProxy]\n    end\n\n    subgraph "Application Layer"\n        API1[API Server 1]\n        API2[API Server 2]\n        API3[API Server N]\n    end\n\n    subgraph "Service Layer"\n        AUTH[Auth Service]\n        PROBLEM[Problem Service]\n        SUBMISSION[Submission Service]\n        EXECUTION[Execution Service]\n        LEADERBOARD[Leaderboard Service]\n    end\n\n    subgraph "Message Queue"\n        QUEUE[Message Queue<br/>Kafka/RabbitMQ]\n    end\n\n    subgraph "Execution Workers"\n        WORKER1[Worker Pool 1<br/>Docker Containers]\n        WORKER2[Worker Pool 2<br/>Docker Containers]\n        WORKER3[Worker Pool N<br/>Docker Containers]\n    end\n\n    subgraph "Cache Layer"\n        REDIS1[Redis Cluster]\n        REDIS2[Redis - Leaderboard]\n    end\n\n    subgraph "Database Layer"\n        POSTGRES[(PostgreSQL<br/>Primary)]\n        REPLICA1[(Read Replica 1)]\n        REPLICA2[(Read Replica 2)]\n        MONGO[(MongoDB<br/>Submissions)]\n    end\n\n    subgraph "Storage Layer"\n        S3[Object Storage<br/>S3/MinIO]\n    end\n\n    subgraph "Monitoring"\n        METRICS[Metrics<br/>Prometheus]\n        LOGS[Logs<br/>ELK Stack]\n    end\n\n    WEB --\x3e CDN\n    MOBILE --\x3e CDN\n    CDN --\x3e LB\n    LB --\x3e API1\n    LB --\x3e API2\n    LB --\x3e API3\n\n    API1 --\x3e AUTH\n    API1 --\x3e PROBLEM\n    API1 --\x3e SUBMISSION\n    API2 --\x3e AUTH\n    API2 --\x3e PROBLEM\n    API2 --\x3e SUBMISSION\n    API3 --\x3e LEADERBOARD\n\n    SUBMISSION --\x3e QUEUE\n    QUEUE --\x3e EXECUTION\n    EXECUTION --\x3e WORKER1\n    EXECUTION --\x3e WORKER2\n    EXECUTION --\x3e WORKER3\n\n    AUTH --\x3e REDIS1\n    PROBLEM --\x3e REDIS1\n    LEADERBOARD --\x3e REDIS2\n\n    PROBLEM --\x3e POSTGRES\n    AUTH --\x3e POSTGRES\n    SUBMISSION --\x3e MONGO\n    POSTGRES --\x3e REPLICA1\n    POSTGRES --\x3e REPLICA2\n\n    WORKER1 --\x3e S3\n    WORKER2 --\x3e S3\n    WORKER3 --\x3e S3\n\n    API1 --\x3e METRICS\n    API2 --\x3e METRICS\n    API3 --\x3e METRICS\n    EXECUTION --\x3e LOGS\n    WORKER1 --\x3e LOGS'}),"\n",(0,i.jsx)(n.h3,{id:"component-description",children:"Component Description"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"1. Client Layer"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Web application (React/Vue)"}),"\n",(0,i.jsx)(n.li,{children:"Mobile apps (iOS/Android)"}),"\n",(0,i.jsx)(n.li,{children:"Code editor integration"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"2. Load Balancer"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Distributes traffic across API servers"}),"\n",(0,i.jsx)(n.li,{children:"Health checks and failover"}),"\n",(0,i.jsx)(n.li,{children:"SSL termination"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"3. API Gateway/Application Servers"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"RESTful API endpoints"}),"\n",(0,i.jsx)(n.li,{children:"Request validation"}),"\n",(0,i.jsx)(n.li,{children:"Rate limiting"}),"\n",(0,i.jsx)(n.li,{children:"Authentication/Authorization"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"4. Service Layer"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Microservices architecture"}),"\n",(0,i.jsx)(n.li,{children:"Independent scaling"}),"\n",(0,i.jsx)(n.li,{children:"Service mesh for communication"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"5. Message Queue"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Async processing of submissions"}),"\n",(0,i.jsx)(n.li,{children:"Decouples submission from execution"}),"\n",(0,i.jsx)(n.li,{children:"Ensures reliability"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"6. Execution Workers"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Containerized execution environment"}),"\n",(0,i.jsx)(n.li,{children:"Sandboxed code execution"}),"\n",(0,i.jsx)(n.li,{children:"Resource limits (CPU, memory, time)"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"7. Cache Layer"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Redis for session management"}),"\n",(0,i.jsx)(n.li,{children:"Cache problem data"}),"\n",(0,i.jsx)(n.li,{children:"Store leaderboard in sorted sets"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"8. Database Layer"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"PostgreSQL for relational data"}),"\n",(0,i.jsx)(n.li,{children:"MongoDB for submissions (document store)"}),"\n",(0,i.jsx)(n.li,{children:"Read replicas for scaling reads"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"9. Object Storage"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Store test case files"}),"\n",(0,i.jsx)(n.li,{children:"Store large submission outputs"}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"deep-dives",children:"Deep Dives"}),"\n",(0,i.jsx)(n.h3,{id:"deep-dive-1-code-execution-service",children:"Deep Dive 1: Code Execution Service"}),"\n",(0,i.jsx)(n.h4,{id:"architecture",children:"Architecture"}),"\n",(0,i.jsx)(n.mermaid,{value:'graph LR\n    subgraph "Submission Flow"\n        USER[User Submits Code] --\x3e API[API Server]\n        API --\x3e VALIDATE[Validate Request]\n        VALIDATE --\x3e QUEUE[Kafka Queue]\n        QUEUE --\x3e DISPATCHER[Job Dispatcher]\n        DISPATCHER --\x3e POOL[Worker Pool Manager]\n        POOL --\x3e WORKER1[Worker Container 1]\n        POOL --\x3e WORKER2[Worker Container 2]\n        POOL --\x3e WORKER3[Worker Container N]\n\n        WORKER1 --\x3e SANDBOX[Sandbox Environment<br/>Docker/gVisor]\n        SANDBOX --\x3e COMPILE[Compile Code]\n        COMPILE --\x3e EXECUTE[Run Test Cases]\n        EXECUTE --\x3e RESULT[Collect Results]\n        RESULT --\x3e DB[(Store Results)]\n        RESULT --\x3e NOTIFY[Notify User<br/>WebSocket]\n    end\n\n    subgraph "Security Layers"\n        SANDBOX --\x3e SECCOMP[Seccomp Filters]\n        SANDBOX --\x3e CGROUP[CGroups Limits]\n        SANDBOX --\x3e NAMESPACE[Namespace Isolation]\n    end'}),"\n",(0,i.jsx)(n.h4,{id:"implementation-details",children:"Implementation Details"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"1. Submission Processing"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Pseudo-code for submission service\nclass SubmissionService:\n    def submit_code(self, user_id, problem_id, code, language):\n        # 1. Validate submission\n        if not self.validate_submission(code, language):\n            return {"error": "Invalid submission"}\n\n        # 2. Create submission record\n        submission = {\n            "submission_id": generate_uuid(),\n            "user_id": user_id,\n            "problem_id": problem_id,\n            "code": code,\n            "language": language,\n            "status": "QUEUED",\n            "created_at": timestamp()\n        }\n\n        # 3. Store in database\n        db.submissions.insert(submission)\n\n        # 4. Publish to message queue\n        kafka.publish("code-execution", {\n            "submission_id": submission["submission_id"],\n            "problem_id": problem_id,\n            "code": code,\n            "language": language,\n            "test_cases": self.get_test_cases(problem_id)\n        })\n\n        return {"submission_id": submission["submission_id"]}\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"2. Worker Execution"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Pseudo-code for execution worker\nclass ExecutionWorker:\n    def execute_submission(self, job):\n        submission_id = job["submission_id"]\n\n        try:\n            # 1. Create isolated container\n            container = docker.create_container(\n                image=f"judge-{job[\'language\']}",\n                mem_limit="256m",\n                cpu_quota=50000,  # 50% of one core\n                network_disabled=True,\n                security_opt=["no-new-privileges"],\n                user="nobody"\n            )\n\n            # 2. Copy code into container\n            container.put_archive("/workspace/", job["code"])\n\n            # 3. Compile code (if needed)\n            if job["language"] in ["java", "cpp", "c"]:\n                compile_result = container.exec_run(\n                    cmd=f"compile.sh",\n                    timeout=10\n                )\n                if compile_result.exit_code != 0:\n                    return self.save_result(submission_id, "COMPILATION_ERROR")\n\n            # 4. Run test cases\n            results = []\n            for test_case in job["test_cases"]:\n                result = self.run_test_case(container, test_case)\n                results.append(result)\n\n                # Early exit on wrong answer\n                if result["status"] != "ACCEPTED":\n                    break\n\n            # 5. Aggregate results\n            final_status = self.aggregate_results(results)\n            self.save_result(submission_id, final_status, results)\n\n        finally:\n            # Clean up container\n            container.remove(force=True)\n\n    def run_test_case(self, container, test_case):\n        start_time = time.time()\n\n        # Execute with timeout\n        exec_result = container.exec_run(\n            cmd=f"python /workspace/solution.py",\n            stdin=test_case["input"],\n            timeout=test_case["time_limit"]\n        )\n\n        execution_time = time.time() - start_time\n\n        # Check result\n        if execution_time > test_case["time_limit"]:\n            return {"status": "TIME_LIMIT_EXCEEDED"}\n\n        if exec_result.exit_code != 0:\n            return {"status": "RUNTIME_ERROR"}\n\n        if self.compare_output(exec_result.output, test_case["expected"]):\n            return {\n                "status": "ACCEPTED",\n                "time": execution_time,\n                "memory": exec_result.memory_used\n            }\n        else:\n            return {"status": "WRONG_ANSWER"}\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"3. Security Measures"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:"# Docker container security configuration\ncontainer_config:\n  # Resource limits\n  memory: 256MB\n  cpu_quota: 0.5 cores\n  pids_limit: 50\n\n  # Security options\n  security_opt:\n    - no-new-privileges\n    - seccomp=strict.json\n\n  # Capabilities (drop all, add only needed)\n  cap_drop:\n    - ALL\n\n  # Read-only root filesystem\n  read_only: true\n\n  # Temporary filesystem for /tmp\n  tmpfs:\n    /tmp: size=10M\n\n  # Network disabled\n  network_mode: none\n\n  # User namespace\n  user: 1000:1000\n\n  # Time limit\n  timeout: 10s\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h3,{id:"deep-dive-2-leaderboard-service",children:"Deep Dive 2: Leaderboard Service"}),"\n",(0,i.jsx)(n.h4,{id:"architecture-1",children:"Architecture"}),"\n",(0,i.jsx)(n.mermaid,{value:'graph TB\n    subgraph "Leaderboard System"\n        EVENT[Submission Event] --\x3e PROCESSOR[Event Processor]\n        PROCESSOR --\x3e REDIS[Redis Sorted Set]\n        REDIS --\x3e RANK[Rank Calculator]\n\n        subgraph "Redis Data Structure"\n            GLOBAL[Global Leaderboard<br/>ZSET: score \u2192 user_id]\n            DAILY[Daily Leaderboard<br/>ZSET with TTL]\n            PROBLEM[Problem-wise<br/>ZSET per problem]\n        end\n\n        RANK --\x3e GLOBAL\n        RANK --\x3e DAILY\n        RANK --\x3e PROBLEM\n\n        API[API Request] --\x3e CACHE{Cache Hit?}\n        CACHE --\x3e|Yes| RETURN[Return Cached]\n        CACHE --\x3e|No| COMPUTE[Compute from Redis]\n        COMPUTE --\x3e UPDATE_CACHE[Update Cache]\n        UPDATE_CACHE --\x3e RETURN\n    end'}),"\n",(0,i.jsx)(n.h4,{id:"implementation",children:"Implementation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Leaderboard service implementation\nclass LeaderboardService:\n    def __init__(self):\n        self.redis = RedisClient()\n        self.cache = Cache()\n\n    def update_user_score(self, user_id, problem_id, solved):\n        """Update user score when they solve a problem"""\n        if solved:\n            # 1. Update global leaderboard\n            current_score = self.redis.zscore("leaderboard:global", user_id) or 0\n            new_score = current_score + self.calculate_points(problem_id)\n            self.redis.zadd("leaderboard:global", {user_id: new_score})\n\n            # 2. Update daily leaderboard (with 24h TTL)\n            self.redis.zadd(\n                f"leaderboard:daily:{today()}",\n                {user_id: new_score}\n            )\n            self.redis.expire(f"leaderboard:daily:{today()}", 86400)\n\n            # 3. Update problem-specific leaderboard\n            self.redis.zadd(\n                f"leaderboard:problem:{problem_id}",\n                {user_id: time.time()}  # Score = submission time\n            )\n\n            # 4. Invalidate cache\n            self.cache.delete(f"rank:{user_id}")\n\n    def get_top_users(self, limit=100, offset=0):\n        """Get top N users from leaderboard"""\n        # Check cache first\n        cache_key = f"leaderboard:top:{limit}:{offset}"\n        cached = self.cache.get(cache_key)\n        if cached:\n            return cached\n\n        # Get from Redis\n        result = self.redis.zrevrange(\n            "leaderboard:global",\n            offset,\n            offset + limit - 1,\n            withscores=True\n        )\n\n        # Format response\n        leaderboard = [\n            {\n                "rank": offset + i + 1,\n                "user_id": user_id,\n                "score": score,\n                "user_info": self.get_user_info(user_id)\n            }\n            for i, (user_id, score) in enumerate(result)\n        ]\n\n        # Cache for 1 minute\n        self.cache.set(cache_key, leaderboard, ttl=60)\n        return leaderboard\n\n    def get_user_rank(self, user_id):\n        """Get rank of a specific user"""\n        cache_key = f"rank:{user_id}"\n        cached = self.cache.get(cache_key)\n        if cached:\n            return cached\n\n        # Get rank from Redis (0-indexed, so add 1)\n        rank = self.redis.zrevrank("leaderboard:global", user_id)\n        if rank is None:\n            return None\n\n        rank_info = {\n            "rank": rank + 1,\n            "score": self.redis.zscore("leaderboard:global", user_id),\n            "total_users": self.redis.zcard("leaderboard:global")\n        }\n\n        self.cache.set(cache_key, rank_info, ttl=300)\n        return rank_info\n'})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h3,{id:"deep-dive-3-rate-limiting--anti-cheating",children:"Deep Dive 3: Rate Limiting & Anti-Cheating"}),"\n",(0,i.jsx)(n.h4,{id:"rate-limiting-strategy",children:"Rate Limiting Strategy"}),"\n",(0,i.jsx)(n.mermaid,{value:'graph LR\n    REQUEST[User Request] --\x3e LIMITER{Rate Limiter}\n\n    subgraph "Multi-Level Rate Limiting"\n        LIMITER --\x3e IP[IP-based<br/>100 req/min]\n        LIMITER --\x3e USER[User-based<br/>10 submissions/min]\n        LIMITER --\x3e GLOBAL[Global<br/>10K req/sec]\n    end\n\n    IP --\x3e|Allowed| PROCESS[Process Request]\n    USER --\x3e|Allowed| PROCESS\n    GLOBAL --\x3e|Allowed| PROCESS\n\n    IP --\x3e|Exceeded| REJECT[429 Too Many Requests]\n    USER --\x3e|Exceeded| REJECT\n    GLOBAL --\x3e|Exceeded| REJECT'}),"\n",(0,i.jsx)(n.h4,{id:"implementation-1",children:"Implementation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Token bucket rate limiter\nclass RateLimiter:\n    def __init__(self):\n        self.redis = RedisClient()\n\n    def is_allowed(self, key, max_requests, window_seconds):\n        """Token bucket algorithm using Redis"""\n        current = time.time()\n        window_key = f"rate_limit:{key}:{int(current // window_seconds)}"\n\n        # Atomic increment\n        pipe = self.redis.pipeline()\n        pipe.incr(window_key)\n        pipe.expire(window_key, window_seconds)\n        result = pipe.execute()\n\n        request_count = result[0]\n        return request_count <= max_requests\n\n    def check_submission_limit(self, user_id):\n        """Check multiple rate limits for submission"""\n        checks = [\n            ("submission", 10, 60),      # 10 per minute\n            ("submission", 50, 3600),    # 50 per hour\n            ("submission", 200, 86400)   # 200 per day\n        ]\n\n        for prefix, limit, window in checks:\n            if not self.is_allowed(f"{prefix}:{user_id}", limit, window):\n                return False, f"Rate limit exceeded: {limit} per {window}s"\n\n        return True, None\n\n# Anti-cheating measures\nclass AntiCheatService:\n    def detect_plagiarism(self, submission):\n        """Detect code plagiarism"""\n        # 1. Normalize code (remove comments, whitespace)\n        normalized = self.normalize_code(submission.code)\n\n        # 2. Generate hash\n        code_hash = hashlib.sha256(normalized.encode()).hexdigest()\n\n        # 3. Check against recent submissions\n        recent_key = f"submissions:recent:{submission.problem_id}"\n        recent_hashes = self.redis.lrange(recent_key, 0, 1000)\n\n        if code_hash in recent_hashes:\n            return True, "Potential plagiarism detected"\n\n        # 4. Store hash\n        self.redis.lpush(recent_key, code_hash)\n        self.redis.ltrim(recent_key, 0, 1000)\n        self.redis.expire(recent_key, 86400)\n\n        return False, None\n\n    def detect_cheating_patterns(self, user_id):\n        """Detect suspicious patterns"""\n        patterns = []\n\n        # 1. Too fast submissions\n        submissions = self.get_recent_submissions(user_id, minutes=10)\n        if len(submissions) > 20:\n            patterns.append("Suspiciously high submission rate")\n\n        # 2. Instant correct solutions\n        instant_solves = [s for s in submissions\n                         if s.first_attempt and s.time < 10]\n        if len(instant_solves) > 5:\n            patterns.append("Multiple instant solutions")\n\n        # 3. Identical submission from multiple accounts\n        # (check using code hash similarity)\n\n        return patterns\n'})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h3,{id:"deep-dive-4-database-schema--indexing",children:"Deep Dive 4: Database Schema & Indexing"}),"\n",(0,i.jsx)(n.h4,{id:"schema-design",children:"Schema Design"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'-- Users table\nCREATE TABLE users (\n    user_id BIGSERIAL PRIMARY KEY,\n    username VARCHAR(50) UNIQUE NOT NULL,\n    email VARCHAR(255) UNIQUE NOT NULL,\n    password_hash VARCHAR(255) NOT NULL,\n    is_premium BOOLEAN DEFAULT FALSE,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    last_login TIMESTAMP,\n    INDEX idx_username (username),\n    INDEX idx_email (email)\n);\n\n-- Problems table\nCREATE TABLE problems (\n    problem_id BIGSERIAL PRIMARY KEY,\n    title VARCHAR(255) NOT NULL,\n    slug VARCHAR(255) UNIQUE NOT NULL,\n    difficulty ENUM(\'easy\', \'medium\', \'hard\') NOT NULL,\n    description TEXT NOT NULL,\n    acceptance_rate DECIMAL(5,2),\n    total_submissions INT DEFAULT 0,\n    total_accepted INT DEFAULT 0,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    INDEX idx_difficulty (difficulty),\n    INDEX idx_slug (slug),\n    INDEX idx_acceptance_rate (acceptance_rate)\n);\n\n-- Problem tags (many-to-many)\nCREATE TABLE problem_tags (\n    problem_id BIGINT REFERENCES problems(problem_id),\n    tag VARCHAR(50) NOT NULL,\n    PRIMARY KEY (problem_id, tag),\n    INDEX idx_tag (tag)\n);\n\n-- Test cases (stored separately for security)\nCREATE TABLE test_cases (\n    test_case_id BIGSERIAL PRIMARY KEY,\n    problem_id BIGINT REFERENCES problems(problem_id),\n    input_file_path VARCHAR(500),\n    output_file_path VARCHAR(500),\n    is_sample BOOLEAN DEFAULT FALSE,\n    time_limit INT DEFAULT 2000, -- milliseconds\n    memory_limit INT DEFAULT 256, -- MB\n    INDEX idx_problem_id (problem_id)\n);\n\n-- User submissions (MongoDB for flexibility)\n{\n    "_id": ObjectId("..."),\n    "submission_id": "uuid",\n    "user_id": 12345,\n    "problem_id": 1,\n    "language": "python",\n    "code": "def solution()...",\n    "status": "ACCEPTED",\n    "test_results": [\n        {\n            "test_case_id": 1,\n            "status": "ACCEPTED",\n            "execution_time": 45,\n            "memory_used": 12\n        }\n    ],\n    "total_time": 152,\n    "total_memory": 45,\n    "created_at": ISODate("..."),\n    "indexes": [\n        {"user_id": 1, "created_at": -1},\n        {"problem_id": 1, "status": 1},\n        {"created_at": 1}\n    ]\n}\n\n-- User problem status (PostgreSQL)\nCREATE TABLE user_problem_status (\n    user_id BIGINT REFERENCES users(user_id),\n    problem_id BIGINT REFERENCES problems(problem_id),\n    status ENUM(\'attempted\', \'solved\') NOT NULL,\n    attempts INT DEFAULT 1,\n    first_solved_at TIMESTAMP,\n    last_attempted_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    PRIMARY KEY (user_id, problem_id),\n    INDEX idx_user_solved (user_id, status)\n);\n\n-- User statistics (materialized view)\nCREATE MATERIALIZED VIEW user_statistics AS\nSELECT\n    u.user_id,\n    u.username,\n    COUNT(DISTINCT ups.problem_id) FILTER (WHERE ups.status = \'solved\') as solved_count,\n    COUNT(DISTINCT ups.problem_id) FILTER (WHERE ups.status = \'attempted\') as attempted_count,\n    COUNT(DISTINCT ups.problem_id) FILTER (\n        WHERE ups.status = \'solved\' AND p.difficulty = \'easy\'\n    ) as easy_solved,\n    COUNT(DISTINCT ups.problem_id) FILTER (\n        WHERE ups.status = \'solved\' AND p.difficulty = \'medium\'\n    ) as medium_solved,\n    COUNT(DISTINCT ups.problem_id) FILTER (\n        WHERE ups.status = \'solved\' AND p.difficulty = \'hard\'\n    ) as hard_solved\nFROM users u\nLEFT JOIN user_problem_status ups ON u.user_id = ups.user_id\nLEFT JOIN problems p ON ups.problem_id = p.problem_id\nGROUP BY u.user_id, u.username;\n\n-- Refresh periodically\nREFRESH MATERIALIZED VIEW CONCURRENTLY user_statistics;\n'})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"database-schema",children:"Database Schema"}),"\n",(0,i.jsx)(n.h3,{id:"partitioning-strategy",children:"Partitioning Strategy"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"-- Partition submissions by date for better performance\nCREATE TABLE submissions_2024_01 PARTITION OF submissions\n    FOR VALUES FROM ('2024-01-01') TO ('2024-02-01');\n\nCREATE TABLE submissions_2024_02 PARTITION OF submissions\n    FOR VALUES FROM ('2024-02-01') TO ('2024-03-01');\n-- ... etc\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"api-design",children:"API Design"}),"\n",(0,i.jsx)(n.h3,{id:"rest-api-endpoints",children:"REST API Endpoints"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:"# Authentication\nPOST   /api/v1/auth/register\nPOST   /api/v1/auth/login\nPOST   /api/v1/auth/logout\nGET    /api/v1/auth/me\n\n# Problems\nGET    /api/v1/problems                    # List problems\nGET    /api/v1/problems/:slug              # Get problem details\nGET    /api/v1/problems/:slug/submissions  # Get user submissions\nPOST   /api/v1/problems/:slug/submit       # Submit solution\nPOST   /api/v1/problems/:slug/run          # Run custom test\n\n# Submissions\nGET    /api/v1/submissions/:id             # Get submission details\nGET    /api/v1/submissions/:id/status      # Poll submission status\n\n# Leaderboard\nGET    /api/v1/leaderboard/global          # Global leaderboard\nGET    /api/v1/leaderboard/daily           # Daily leaderboard\nGET    /api/v1/users/:id/rank              # User rank\n\n# User\nGET    /api/v1/users/:id/profile           # User profile\nGET    /api/v1/users/:id/statistics        # User stats\nGET    /api/v1/users/:id/submissions       # User submission history\n"})}),"\n",(0,i.jsx)(n.h3,{id:"example-api-requestresponse",children:"Example API Request/Response"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'// POST /api/v1/problems/two-sum/submit\n{\n    "language": "python",\n    "code": "class Solution:\\n    def twoSum(self, nums, target):\\n        ..."\n}\n\n// Response\n{\n    "submission_id": "550e8400-e29b-41d4-a716-446655440000",\n    "status": "QUEUED",\n    "message": "Your submission is being processed"\n}\n\n// GET /api/v1/submissions/550e8400-e29b-41d4-a716-446655440000\n{\n    "submission_id": "550e8400-e29b-41d4-a716-446655440000",\n    "status": "ACCEPTED",\n    "runtime": 152,\n    "memory": 45.2,\n    "language": "python",\n    "passed_tests": 58,\n    "total_tests": 58,\n    "percentile": {\n        "runtime": 85.2,\n        "memory": 72.1\n    }\n}\n'})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"scalability--optimization",children:"Scalability & Optimization"}),"\n",(0,i.jsx)(n.h3,{id:"caching-strategy",children:"Caching Strategy"}),"\n",(0,i.jsx)(n.mermaid,{value:"graph TB\n    REQUEST[User Request] --\x3e CDN{CDN Cache}\n    CDN --\x3e|HIT| RETURN1[Return]\n    CDN --\x3e|MISS| LB[Load Balancer]\n\n    LB --\x3e APP[App Server]\n    APP --\x3e REDIS{Redis Cache}\n    REDIS --\x3e|HIT| RETURN2[Return]\n    REDIS --\x3e|MISS| DB[(Database)]\n\n    DB --\x3e REDIS\n    REDIS --\x3e APP\n    APP --\x3e RETURN2"}),"\n",(0,i.jsx)(n.h3,{id:"cache-layers",children:"Cache Layers"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Multi-level caching implementation\nclass CacheManager:\n    def __init__(self):\n        self.l1_cache = LRUCache(1000)  # In-memory\n        self.l2_cache = RedisCache()    # Distributed\n\n    def get_problem(self, problem_id):\n        # L1: In-memory cache\n        cached = self.l1_cache.get(f"problem:{problem_id}")\n        if cached:\n            return cached\n\n        # L2: Redis cache\n        cached = self.l2_cache.get(f"problem:{problem_id}")\n        if cached:\n            self.l1_cache.set(f"problem:{problem_id}", cached)\n            return cached\n\n        # L3: Database\n        problem = db.get_problem(problem_id)\n\n        # Populate caches\n        self.l2_cache.set(f"problem:{problem_id}", problem, ttl=3600)\n        self.l1_cache.set(f"problem:{problem_id}", problem)\n\n        return problem\n'})}),"\n",(0,i.jsx)(n.h3,{id:"database-optimization",children:"Database Optimization"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"1. Read Replicas"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Master: Handle writes"}),"\n",(0,i.jsx)(n.li,{children:"Replicas: Handle reads (problem lists, user stats)"}),"\n",(0,i.jsx)(n.li,{children:"Reduces load on master"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"2. Sharding Strategy"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Shard submissions by user_id\ndef get_shard(user_id):\n    return user_id % NUM_SHARDS\n\n# Consistent hashing for problem distribution\nclass ConsistentHash:\n    def __init__(self, nodes):\n        self.ring = {}\n        self.sorted_keys = []\n        for node in nodes:\n            self.add_node(node)\n\n    def add_node(self, node):\n        for i in range(150):  # Virtual nodes\n            key = hashlib.md5(f"{node}:{i}".encode()).hexdigest()\n            self.ring[key] = node\n            self.sorted_keys.append(key)\n        self.sorted_keys.sort()\n\n    def get_node(self, key):\n        hash_key = hashlib.md5(str(key).encode()).hexdigest()\n        for ring_key in self.sorted_keys:\n            if hash_key <= ring_key:\n                return self.ring[ring_key]\n        return self.ring[self.sorted_keys[0]]\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"3. Indexing Strategy"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:"-- Compound indexes for common queries\nCREATE INDEX idx_user_status ON user_problem_status(user_id, status, last_attempted_at);\nCREATE INDEX idx_problem_difficulty ON problems(difficulty, acceptance_rate);\nCREATE INDEX idx_submission_user_time ON submissions(user_id, created_at DESC);\n\n-- Partial index for active users\nCREATE INDEX idx_active_users ON users(user_id)\nWHERE last_login > NOW() - INTERVAL '30 days';\n\n-- Full-text search index\nCREATE INDEX idx_problem_search ON problems\nUSING GIN(to_tsvector('english', title || ' ' || description));\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h3,{id:"horizontal-scaling",children:"Horizontal Scaling"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Auto-scaling Configuration"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:"# Kubernetes HPA (Horizontal Pod Autoscaler)\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: api-server-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: api-server\n  minReplicas: 3\n  maxReplicas: 20\n  metrics:\n    - type: Resource\n      resource:\n        name: cpu\n        target:\n          type: Utilization\n          averageUtilization: 70\n    - type: Resource\n      resource:\n        name: memory\n        target:\n          type: Utilization\n          averageUtilization: 80\n\n---\n# Worker pool auto-scaling\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: execution-worker-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: execution-worker\n  minReplicas: 10\n  maxReplicas: 100\n  metrics:\n    - type: External\n      external:\n        metric:\n          name: queue_depth\n          selector:\n            matchLabels:\n              queue_name: code-execution\n        target:\n          type: AverageValue\n          averageValue: '10' # Scale up when queue has 10+ items per worker\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h3,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"1. Connection Pooling"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Database connection pooling\nfrom sqlalchemy import create_engine, pool\n\nengine = create_engine(\n    \"postgresql://user:pass@host/db\",\n    poolclass=pool.QueuePool,\n    pool_size=20,        # Persistent connections\n    max_overflow=10,     # Additional connections when needed\n    pool_timeout=30,     # Wait 30s for connection\n    pool_recycle=3600,   # Recycle connections after 1 hour\n    pool_pre_ping=True   # Verify connection before use\n)\n\n# Redis connection pooling\nredis_pool = redis.ConnectionPool(\n    host='redis-host',\n    port=6379,\n    max_connections=50,\n    decode_responses=True\n)\nredis_client = redis.Redis(connection_pool=redis_pool)\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"2. Batch Processing"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Batch update leaderboard\nclass BatchLeaderboardUpdater:\n    def __init__(self):\n        self.batch = []\n        self.batch_size = 100\n        self.redis = RedisClient()\n\n    def add_score_update(self, user_id, problem_id, points):\n        self.batch.append({\n            "user_id": user_id,\n            "problem_id": problem_id,\n            "points": points\n        })\n\n        if len(self.batch) >= self.batch_size:\n            self.flush()\n\n    def flush(self):\n        if not self.batch:\n            return\n\n        # Pipeline Redis commands\n        pipe = self.redis.pipeline()\n\n        for update in self.batch:\n            pipe.zincrby(\n                "leaderboard:global",\n                update["points"],\n                update["user_id"]\n            )\n            pipe.zadd(\n                f"leaderboard:problem:{update[\'problem_id\']}",\n                {update["user_id"]: time.time()}\n            )\n\n        # Execute all at once\n        pipe.execute()\n        self.batch.clear()\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"3. Async Processing"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Async submission processing\nimport asyncio\nimport aiohttp\n\nclass AsyncSubmissionProcessor:\n    async def process_submissions(self, submissions):\n        tasks = []\n        async with aiohttp.ClientSession() as session:\n            for submission in submissions:\n                task = self.process_single(session, submission)\n                tasks.append(task)\n\n            # Process all concurrently\n            results = await asyncio.gather(*tasks, return_exceptions=True)\n\n        return results\n\n    async def process_single(self, session, submission):\n        # Fetch test cases\n        test_cases = await self.fetch_test_cases(session, submission.problem_id)\n\n        # Execute code\n        result = await self.execute_code(submission, test_cases)\n\n        # Save results\n        await self.save_results(session, submission.id, result)\n\n        return result\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h3,{id:"monitoring--observability",children:"Monitoring & Observability"}),"\n",(0,i.jsx)(n.mermaid,{value:'graph TB\n    subgraph "Monitoring Stack"\n        APP[Application] --\x3e METRICS[Prometheus]\n        APP --\x3e LOGS[Fluentd/ELK]\n        APP --\x3e TRACES[Jaeger]\n\n        METRICS --\x3e GRAFANA[Grafana Dashboard]\n        LOGS --\x3e KIBANA[Kibana]\n        TRACES --\x3e JAEGER_UI[Jaeger UI]\n\n        GRAFANA --\x3e ALERT[AlertManager]\n        ALERT --\x3e SLACK[Slack/PagerDuty]\n    end'}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Key Metrics to Monitor"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Application metrics\nfrom prometheus_client import Counter, Histogram, Gauge\n\n# Request metrics\nrequest_count = Counter(\n    'api_requests_total',\n    'Total API requests',\n    ['method', 'endpoint', 'status']\n)\n\nrequest_duration = Histogram(\n    'api_request_duration_seconds',\n    'API request duration',\n    ['method', 'endpoint']\n)\n\n# Submission metrics\nsubmission_queue_size = Gauge(\n    'submission_queue_size',\n    'Number of submissions in queue'\n)\n\nexecution_duration = Histogram(\n    'code_execution_duration_seconds',\n    'Code execution duration',\n    ['language', 'status']\n)\n\nactive_workers = Gauge(\n    'active_execution_workers',\n    'Number of active execution workers'\n)\n\n# Database metrics\ndb_connection_pool = Gauge(\n    'db_connection_pool_size',\n    'Database connection pool size',\n    ['state']  # active, idle, waiting\n)\n\n# Cache metrics\ncache_hits = Counter(\n    'cache_hits_total',\n    'Cache hit count',\n    ['cache_type']  # l1, l2, cdn\n)\n\ncache_misses = Counter(\n    'cache_misses_total',\n    'Cache miss count',\n    ['cache_type']\n)\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Alerting Rules"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:"# Prometheus alerting rules\ngroups:\n  - name: leetcode_alerts\n    interval: 30s\n    rules:\n      # High error rate\n      - alert: HighErrorRate\n        expr: rate(api_requests_total{status=~\"5..\"}[5m]) > 0.05\n        for: 5m\n        labels:\n          severity: critical\n        annotations:\n          summary: 'High error rate detected'\n          description: 'Error rate is {{ $value }} errors/sec'\n\n      # Queue backup\n      - alert: SubmissionQueueBackup\n        expr: submission_queue_size > 1000\n        for: 10m\n        labels:\n          severity: warning\n        annotations:\n          summary: 'Submission queue is backing up'\n          description: 'Queue size is {{ $value }}'\n\n      # Database connection pool exhaustion\n      - alert: DBConnectionPoolExhausted\n        expr: db_connection_pool_size{state=\"waiting\"} > 5\n        for: 2m\n        labels:\n          severity: critical\n        annotations:\n          summary: 'Database connection pool exhausted'\n          description: '{{ $value }} connections waiting'\n\n      # Low cache hit rate\n      - alert: LowCacheHitRate\n        expr: |\n          rate(cache_hits_total[5m]) /\n          (rate(cache_hits_total[5m]) + rate(cache_misses_total[5m])) < 0.8\n        for: 10m\n        labels:\n          severity: warning\n        annotations:\n          summary: 'Low cache hit rate'\n          description: 'Cache hit rate is {{ $value }}'\n\n      # Slow API responses\n      - alert: SlowAPIResponses\n        expr: histogram_quantile(0.95, api_request_duration_seconds_bucket) > 1.0\n        for: 5m\n        labels:\n          severity: warning\n        annotations:\n          summary: 'API responses are slow'\n          description: '95th percentile is {{ $value }}s'\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h3,{id:"disaster-recovery",children:"Disaster Recovery"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Backup Strategy"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:"# PostgreSQL backup configuration\nbackup_schedule:\n  full_backup:\n    frequency: daily\n    time: '02:00 UTC'\n    retention: 30 days\n    destination: s3://backups/postgresql/\n\n  incremental_backup:\n    frequency: hourly\n    retention: 7 days\n\n  point_in_time_recovery:\n    enabled: true\n    wal_archiving: true\n    retention: 7 days\n\n# MongoDB backup\nmongodb_backup:\n  type: continuous\n  oplog_size: 10GB\n  snapshot_frequency: 6 hours\n  retention: 30 days\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Disaster Recovery Plan"})}),"\n",(0,i.jsx)(n.mermaid,{value:"graph TB\n    DISASTER[Disaster Detected] --\x3e ASSESS[Assess Impact]\n    ASSESS --\x3e SWITCH{Data Loss?}\n\n    SWITCH --\x3e|No| FAILOVER[Failover to Standby]\n    FAILOVER --\x3e VERIFY[Verify Systems]\n\n    SWITCH --\x3e|Yes| RESTORE[Restore from Backup]\n    RESTORE --\x3e REPLAY[Replay WAL/Oplog]\n    REPLAY --\x3e VERIFY\n\n    VERIFY --\x3e TRAFFIC[Route Traffic]\n    TRAFFIC --\x3e MONITOR[Monitor Systems]\n\n    MONITOR --\x3e POSTMORTEM[Post-mortem Analysis]"}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h3,{id:"security-measures",children:"Security Measures"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"1. Code Execution Sandbox"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-dockerfile",children:'# Minimal execution container\nFROM scratch\nCOPY --from=builder /bin/executor /executor\nCOPY --from=builder /lib/x86_64-linux-gnu/libc.so.6 /lib/x86_64-linux-gnu/\nCOPY --from=builder /lib64/ld-linux-x86-64.so.2 /lib64/\n\n# No shell, no package manager, no network tools\nUSER 65534:65534\nENTRYPOINT ["/executor"]\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"2. Input Validation"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Input sanitization\nclass InputValidator:\n    MAX_CODE_LENGTH = 100_000  # 100KB\n    MAX_TEST_CASE_SIZE = 10_000_000  # 10MB\n    ALLOWED_LANGUAGES = {'python', 'java', 'cpp', 'javascript', 'go'}\n\n    def validate_submission(self, code, language):\n        # Language check\n        if language not in self.ALLOWED_LANGUAGES:\n            raise ValidationError(\"Invalid language\")\n\n        # Size check\n        if len(code) > self.MAX_CODE_LENGTH:\n            raise ValidationError(\"Code too long\")\n\n        # Dangerous patterns\n        dangerous_patterns = [\n            r'import\\s+os',\n            r'import\\s+subprocess',\n            r'eval\\s*\\(',\n            r'exec\\s*\\(',\n            r'__import__',\n            r'open\\s*\\(',\n            r'file\\s*\\(',\n        ]\n\n        for pattern in dangerous_patterns:\n            if re.search(pattern, code, re.IGNORECASE):\n                raise SecurityError(f\"Forbidden pattern detected: {pattern}\")\n\n        return True\n\n    def sanitize_input(self, test_input):\n        # Remove potential injection attempts\n        sanitized = test_input.replace('\\x00', '')  # Null bytes\n\n        # Size check\n        if len(sanitized) > self.MAX_TEST_CASE_SIZE:\n            raise ValidationError(\"Test case too large\")\n\n        return sanitized\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"3. Authentication & Authorization"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# JWT-based authentication\nclass AuthService:\n    def __init__(self):\n        self.secret_key = os.environ['JWT_SECRET']\n        self.algorithm = 'HS256'\n        self.token_expiry = 3600  # 1 hour\n\n    def generate_token(self, user_id, email):\n        payload = {\n            'user_id': user_id,\n            'email': email,\n            'iat': datetime.utcnow(),\n            'exp': datetime.utcnow() + timedelta(seconds=self.token_expiry)\n        }\n        return jwt.encode(payload, self.secret_key, algorithm=self.algorithm)\n\n    def verify_token(self, token):\n        try:\n            payload = jwt.decode(token, self.secret_key, algorithms=[self.algorithm])\n            return payload\n        except jwt.ExpiredSignatureError:\n            raise AuthError(\"Token expired\")\n        except jwt.InvalidTokenError:\n            raise AuthError(\"Invalid token\")\n\n    def check_permission(self, user_id, action, resource):\n        # Role-based access control\n        user_roles = self.get_user_roles(user_id)\n\n        permissions = {\n            'admin': ['*'],\n            'premium': ['submit', 'view_solutions', 'video_tutorials'],\n            'free': ['submit', 'view_problems']\n        }\n\n        for role in user_roles:\n            if action in permissions.get(role, []) or '*' in permissions.get(role, []):\n                return True\n\n        return False\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h3,{id:"cost-optimization",children:"Cost Optimization"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"1. Resource Management"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Dynamic worker pool sizing\nclass WorkerPoolManager:\n    def __init__(self):\n        self.min_workers = 10\n        self.max_workers = 100\n        self.current_workers = self.min_workers\n        self.target_queue_depth = 10\n\n    def adjust_pool_size(self):\n        queue_depth = self.get_queue_depth()\n\n        if queue_depth > self.target_queue_depth * self.current_workers:\n            # Scale up\n            new_size = min(\n                self.current_workers + 10,\n                self.max_workers\n            )\n            self.scale_to(new_size)\n\n        elif queue_depth < self.target_queue_depth * self.current_workers * 0.5:\n            # Scale down\n            new_size = max(\n                self.current_workers - 5,\n                self.min_workers\n            )\n            self.scale_to(new_size)\n\n    def scale_to(self, target_size):\n        if target_size > self.current_workers:\n            # Add workers\n            self.add_workers(target_size - self.current_workers)\n        else:\n            # Remove workers (gracefully)\n            self.remove_workers(self.current_workers - target_size)\n\n        self.current_workers = target_size\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"2. Spot Instances for Workers"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:"# Use spot instances for execution workers (70% cost saving)\nworker_pool:\n  on_demand_instances: 20% # For reliability\n  spot_instances: 80% # For cost savings\n\n  spot_instance_config:\n    fallback_to_on_demand: true\n    max_price: 0.10 # per hour\n    interruption_handling:\n      drain_tasks: true\n      graceful_shutdown_seconds: 30\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h3,{id:"advanced-features",children:"Advanced Features"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"1. Contest Mode"})}),"\n",(0,i.jsx)(n.mermaid,{value:"graph LR\n    START[Contest Starts] --\x3e FREEZE[Problem Release]\n    FREEZE --\x3e SUBMIT[Accept Submissions]\n    SUBMIT --\x3e REALTIME[Real-time Leaderboard]\n    REALTIME --\x3e END[Contest Ends]\n    END --\x3e FINAL[Final Rankings]\n    FINAL --\x3e PLAGIARISM[Plagiarism Check]\n    PLAGIARISM --\x3e ANNOUNCE[Announce Winners]"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"2. Interactive Problems"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Interactive problem execution\nclass InteractiveProblemExecutor:\n    def execute_interactive(self, submission, judge_code):\n        # Start user solution\n        user_process = self.start_process(submission.code)\n\n        # Start judge\n        judge_process = self.start_process(judge_code)\n\n        try:\n            # Communication loop\n            for _ in range(MAX_INTERACTIONS):\n                # Judge sends query\n                query = judge_process.read_line()\n                user_process.write_line(query)\n\n                # User responds\n                response = user_process.read_line()\n                judge_process.write_line(response)\n\n                # Check if judge is satisfied\n                if judge_process.is_done():\n                    break\n\n            # Get final verdict from judge\n            verdict = judge_process.get_verdict()\n            return verdict\n\n        finally:\n            user_process.kill()\n            judge_process.kill()\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"3. SQL/Database Problems"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# SQL problem execution\nclass SQLProblemExecutor:\n    def execute_sql(self, submission_query, expected_schema):\n        # Create isolated database\n        with self.create_temp_database() as db:\n            # Load test data\n            db.execute(self.get_setup_queries())\n\n            try:\n                # Execute user query\n                result = db.execute(submission_query)\n\n                # Validate result\n                if self.validate_result(result, expected_schema):\n                    return {"status": "ACCEPTED", "result": result}\n                else:\n                    return {"status": "WRONG_ANSWER"}\n\n            except SQLError as e:\n                return {"status": "RUNTIME_ERROR", "message": str(e)}\n'})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(n.h3,{id:"key-design-decisions",children:"Key Design Decisions"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Microservices Architecture"}),": Independent scaling and deployment"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Message Queue"}),": Decouples submission from execution"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Containerized Execution"}),": Security and resource isolation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Redis for Leaderboard"}),": Fast sorted set operations"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Hybrid Database"}),": PostgreSQL for relations, MongoDB for documents"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Multi-level Caching"}),": Reduces database load"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Horizontal Scaling"}),": Handles peak loads"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"trade-offs",children:"Trade-offs"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Aspect"}),(0,i.jsx)(n.th,{children:"Choice"}),(0,i.jsx)(n.th,{children:"Trade-off"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Execution"}),(0,i.jsx)(n.td,{children:"Synchronous with queue"}),(0,i.jsx)(n.td,{children:"Better UX but needs WebSocket"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Database"}),(0,i.jsx)(n.td,{children:"Polyglot persistence"}),(0,i.jsx)(n.td,{children:"Complexity vs performance"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Caching"}),(0,i.jsx)(n.td,{children:"Aggressive caching"}),(0,i.jsx)(n.td,{children:"Consistency vs speed"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Workers"}),(0,i.jsx)(n.td,{children:"Containerized"}),(0,i.jsx)(n.td,{children:"Security vs overhead"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Leaderboard"}),(0,i.jsx)(n.td,{children:"Redis sorted sets"}),(0,i.jsx)(n.td,{children:"Memory vs speed"})]})]})]}),"\n",(0,i.jsx)(n.h3,{id:"estimated-costs-aws",children:"Estimated Costs (AWS)"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Application Servers (12 \xd7 c5.2xlarge): $2,500/month\nExecution Workers (1000 \xd7 t3.small spot): $3,600/month\nRDS PostgreSQL (db.r5.4xlarge): $2,400/month\nElastiCache Redis (cache.r5.2xlarge): $800/month\nS3 Storage (11TB): $250/month\nCloudFront CDN: $300/month\nLoad Balancers: $200/month\nMonitoring & Logs: $150/month\n\nTotal: ~$10,200/month\n"})}),"\n",(0,i.jsx)(n.p,{children:"This design handles 10M users with 5M daily submissions efficiently while maintaining security, performance, and reliability!"})]})}function u(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},28453:(e,n,s)=>{s.d(n,{R:()=>a,x:()=>l});var i=s(96540);const r={},t=i.createContext(r);function a(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);