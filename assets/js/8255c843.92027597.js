"use strict";(self.webpackChunkdev_enigma=self.webpackChunkdev_enigma||[]).push([[60497],{28453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>o});var r=t(96540);const a={},s=r.createContext(a);function i(e){const n=r.useContext(s);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:i(e.components),r.createElement(s.Provider,{value:n},e.children)}},85355:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>u,frontMatter:()=>s,metadata:()=>o,toc:()=>c});var r=t(74848),a=t(28453);const s={},i="Stream Processing & Message Queue Systems: System Design Guide",o={id:"Backend System Design/Stream Processing & Message Queue Systems: System Design Guide",title:"Stream Processing & Message Queue Systems: System Design Guide",description:"A comprehensive guide to Apache Kafka, Apache Flink, and Apache Spark from a High-Level Design perspective.",source:"@site/docs/Backend System Design/Stream Processing & Message Queue Systems: System Design Guide.mdx",sourceDirName:"Backend System Design",slug:"/Backend System Design/Stream Processing & Message Queue Systems: System Design Guide",permalink:"/docs/Backend System Design/Stream Processing & Message Queue Systems: System Design Guide",draft:!1,unlisted:!1,editUrl:"https://github.com/carefree-ladka/docs/Backend System Design/Stream Processing & Message Queue Systems: System Design Guide.mdx",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Retry, Backoff, Jitter & Resilience Patterns",permalink:"/docs/Backend System Design/Retry, Backoff, Jitter & Resilience Patterns"},next:{title:"System Design Concepts",permalink:"/docs/Backend System Design/System Design Concepts"}},l={},c=[{value:"Table of Contents",id:"table-of-contents",level:2},{value:"Apache Kafka",id:"apache-kafka",level:2},{value:"What is Kafka?",id:"what-is-kafka",level:3},{value:"Core Concepts",id:"core-concepts",level:3},{value:"Architecture Diagram",id:"architecture-diagram",level:3},{value:"Problems Kafka Solves",id:"problems-kafka-solves",level:3},{value:"When to Use Kafka",id:"when-to-use-kafka",level:3},{value:"Spring Boot Kafka Producer Example",id:"spring-boot-kafka-producer-example",level:3},{value:"Spring Boot Kafka Consumer Example",id:"spring-boot-kafka-consumer-example",level:3},{value:"Apache Flink",id:"apache-flink",level:2},{value:"What is Flink?",id:"what-is-flink",level:3},{value:"Core Concepts",id:"core-concepts-1",level:3},{value:"Flink Architecture",id:"flink-architecture",level:3},{value:"Problems Flink Solves",id:"problems-flink-solves",level:3},{value:"When to Use Flink",id:"when-to-use-flink",level:3},{value:"Flink Job Example (Java)",id:"flink-job-example-java",level:3},{value:"Flink with Spring Boot Integration",id:"flink-with-spring-boot-integration",level:3},{value:"Apache Spark",id:"apache-spark",level:2},{value:"What is Spark?",id:"what-is-spark",level:3},{value:"Core Concepts",id:"core-concepts-2",level:3},{value:"Spark Architecture",id:"spark-architecture",level:3},{value:"Problems Spark Solves",id:"problems-spark-solves",level:3},{value:"When to Use Spark",id:"when-to-use-spark",level:3},{value:"Spark Batch Processing Example",id:"spark-batch-processing-example",level:3},{value:"Spark Structured Streaming Example",id:"spark-structured-streaming-example",level:3},{value:"Comparison &amp; When to Use What",id:"comparison",level:2},{value:"Feature Comparison Table",id:"feature-comparison-table",level:3},{value:"Decision Matrix",id:"decision-matrix",level:3},{value:"Common Architecture Patterns",id:"common-architecture-patterns",level:3},{value:"Pattern 1: Lambda Architecture",id:"pattern-1-lambda-architecture",level:4},{value:"Pattern 2: Kappa Architecture (Streaming-First)",id:"pattern-2-kappa-architecture-streaming-first",level:4},{value:"Real-World Design Patterns",id:"design-patterns",level:2},{value:"Pattern 1: E-commerce Order Processing System",id:"pattern-1-e-commerce-order-processing-system",level:3},{value:"Architecture Overview",id:"architecture-overview",level:4},{value:"Technology Choices",id:"technology-choices",level:4},{value:"Spring Boot Implementation",id:"spring-boot-implementation",level:4},{value:"Pattern 2: Real-time Fraud Detection System",id:"pattern-2-real-time-fraud-detection-system",level:3},{value:"Architecture",id:"architecture",level:4},{value:"Flink Fraud Detection Job",id:"flink-fraud-detection-job",level:4},{value:"Pattern 3: IoT Sensor Data Analytics",id:"pattern-3-iot-sensor-data-analytics",level:3},{value:"Architecture",id:"architecture-1",level:4},{value:"Spring Boot Kafka Producer (IoT Gateway)",id:"spring-boot-kafka-producer-iot-gateway",level:4},{value:"Spark Batch Processing (Daily Analytics)",id:"spark-batch-processing-daily-analytics",level:4},{value:"Pattern 4: Real-time Recommendation System",id:"pattern-4-real-time-recommendation-system",level:3},{value:"Architecture",id:"architecture-2",level:4},{value:"Flink Real-time Feature Computation",id:"flink-real-time-feature-computation",level:4},{value:"Spring Boot Recommendation API",id:"spring-boot-recommendation-api",level:4},{value:"Best Practices &amp; Design Considerations",id:"best-practices--design-considerations",level:2},{value:"Kafka Best Practices",id:"kafka-best-practices",level:3},{value:"1. Topic Design",id:"1-topic-design",level:4},{value:"2. Partition Strategy",id:"2-partition-strategy",level:4},{value:"3. Consumer Group Strategy",id:"3-consumer-group-strategy",level:4},{value:"4. Error Handling",id:"4-error-handling",level:4},{value:"Flink Best Practices",id:"flink-best-practices",level:3},{value:"1. Checkpointing Configuration",id:"1-checkpointing-configuration",level:4},{value:"2. Watermark Strategy",id:"2-watermark-strategy",level:4},{value:"3. State Management",id:"3-state-management",level:4},{value:"Spark Best Practices",id:"spark-best-practices",level:3},{value:"1. Memory Configuration",id:"1-memory-configuration",level:4},{value:"2. Data Partitioning",id:"2-data-partitioning",level:4},{value:"3. Caching Strategy",id:"3-caching-strategy",level:4},{value:"4. Broadcast Joins",id:"4-broadcast-joins",level:4},{value:"Monitoring &amp; Operations",id:"monitoring--operations",level:2},{value:"Kafka Monitoring",id:"kafka-monitoring",level:3},{value:"Flink Monitoring",id:"flink-monitoring",level:3},{value:"Conclusion",id:"conclusion",level:2},{value:"Quick Decision Guide",id:"quick-decision-guide",level:3},{value:"Common Combinations",id:"common-combinations",level:3},{value:"Further Reading",id:"further-reading",level:3}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"stream-processing--message-queue-systems-system-design-guide",children:"Stream Processing & Message Queue Systems: System Design Guide"})}),"\n",(0,r.jsx)(n.p,{children:"A comprehensive guide to Apache Kafka, Apache Flink, and Apache Spark from a High-Level Design perspective."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"table-of-contents",children:"Table of Contents"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"#apache-kafka",children:"Apache Kafka"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"#apache-flink",children:"Apache Flink"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"#apache-spark",children:"Apache Spark"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"#comparison",children:"Comparison & When to Use What"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"#design-patterns",children:"Real-World Design Patterns"})}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"apache-kafka",children:"Apache Kafka"}),"\n",(0,r.jsx)(n.h3,{id:"what-is-kafka",children:"What is Kafka?"}),"\n",(0,r.jsxs)(n.p,{children:["Apache Kafka is a ",(0,r.jsx)(n.strong,{children:"distributed event streaming platform"})," and ",(0,r.jsx)(n.strong,{children:"message broker"})," that acts as a high-throughput, fault-tolerant publish-subscribe messaging system. Think of it as a distributed commit log that stores streams of records in a fault-tolerant, durable way."]}),"\n",(0,r.jsx)(n.h3,{id:"core-concepts",children:"Core Concepts"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Topics"}),": Categories/feeds where records are published (like database tables)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Producers"}),": Applications that publish data to topics"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Consumers"}),": Applications that subscribe to topics and process data"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Partitions"}),": Topics split into partitions for parallelism and scalability"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Brokers"}),": Kafka servers that store data"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Consumer Groups"}),": Multiple consumers working together to process data in parallel"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Offsets"}),": Sequential IDs for messages, enabling replay and tracking"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"architecture-diagram",children:"Architecture Diagram"}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    subgraph "Kafka Cluster"\n        B1[Broker 1]\n        B2[Broker 2]\n        B3[Broker 3]\n        ZK[ZooKeeper/KRaft]\n    end\n\n    P1[Producer 1]\n    P2[Producer 2]\n    P3[Producer 3]\n\n    subgraph "Consumer Group 1"\n        C1[Consumer 1]\n        C2[Consumer 2]\n    end\n\n    subgraph "Consumer Group 2"\n        C3[Consumer 3]\n    end\n\n    P1 --\x3e B1\n    P2 --\x3e B2\n    P3 --\x3e B3\n\n    B1 --\x3e C1\n    B2 --\x3e C2\n    B1 --\x3e C3\n    B2 --\x3e C3\n    B3 --\x3e C3'}),"\n",(0,r.jsx)(n.h3,{id:"problems-kafka-solves",children:"Problems Kafka Solves"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"System Decoupling"}),": Services don't need direct connections to each other"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Buffering & Speed Mismatch"}),": Handles different processing speeds between producers and consumers"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Message Replay"}),": Can replay messages from any point in time (data retention)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Scalability"}),": Handles millions of messages per second horizontally"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Durability"}),": Messages persisted to disk, survive system failures"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Fault Tolerance"}),": Replicates data across multiple brokers"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Real-time Data Pipeline"}),": Moves data between systems with minimal latency"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"when-to-use-kafka",children:"When to Use Kafka"}),"\n",(0,r.jsxs)(n.p,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"Use Kafka When:"})]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Building event-driven architectures"}),"\n",(0,r.jsx)(n.li,{children:"Need to decouple microservices"}),"\n",(0,r.jsx)(n.li,{children:"Require message replay capability"}),"\n",(0,r.jsx)(n.li,{children:"Building real-time data pipelines"}),"\n",(0,r.jsx)(n.li,{children:"Collecting logs/metrics from multiple sources"}),"\n",(0,r.jsx)(n.li,{children:"Implementing event sourcing patterns"}),"\n",(0,r.jsx)(n.li,{children:"Need high throughput (millions of msg/sec)"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["\u274c ",(0,r.jsx)(n.strong,{children:"Don't Use Kafka When:"})]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Simple request-response communication is sufficient"}),"\n",(0,r.jsx)(n.li,{children:"No need for message persistence"}),"\n",(0,r.jsx)(n.li,{children:"Low message volume (hundreds per second)"}),"\n",(0,r.jsx)(n.li,{children:"Need complex routing logic (use RabbitMQ)"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"spring-boot-kafka-producer-example",children:"Spring Boot Kafka Producer Example"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-java",children:'@Service\npublic class OrderEventProducer {\n\n    @Autowired\n    private KafkaTemplate<String, OrderEvent> kafkaTemplate;\n\n    private static final String TOPIC = "order-events";\n\n    public void publishOrderCreated(Order order) {\n        OrderEvent event = OrderEvent.builder()\n            .orderId(order.getId())\n            .customerId(order.getCustomerId())\n            .timestamp(Instant.now())\n            .eventType("ORDER_CREATED")\n            .build();\n\n        // Send with key for partition routing\n        kafkaTemplate.send(TOPIC, order.getCustomerId().toString(), event)\n            .whenComplete((result, ex) -> {\n                if (ex == null) {\n                    log.info("Order event sent: {}", event.getOrderId());\n                } else {\n                    log.error("Failed to send order event", ex);\n                }\n            });\n    }\n}\n\n@Configuration\npublic class KafkaProducerConfig {\n\n    @Value("${kafka.bootstrap-servers}")\n    private String bootstrapServers;\n\n    @Bean\n    public ProducerFactory<String, OrderEvent> producerFactory() {\n        Map<String, Object> config = new HashMap<>();\n        config.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);\n        config.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n        config.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, JsonSerializer.class);\n        config.put(ProducerConfig.ACKS_CONFIG, "all"); // Wait for all replicas\n        config.put(ProducerConfig.RETRIES_CONFIG, 3);\n        config.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);\n        return new DefaultKafkaProducerFactory<>(config);\n    }\n\n    @Bean\n    public KafkaTemplate<String, OrderEvent> kafkaTemplate() {\n        return new KafkaTemplate<>(producerFactory());\n    }\n}\n'})}),"\n",(0,r.jsx)(n.h3,{id:"spring-boot-kafka-consumer-example",children:"Spring Boot Kafka Consumer Example"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-java",children:'@Service\n@Slf4j\npublic class OrderEventConsumer {\n\n    @Autowired\n    private InventoryService inventoryService;\n\n    @Autowired\n    private NotificationService notificationService;\n\n    @KafkaListener(\n        topics = "order-events",\n        groupId = "inventory-service-group",\n        concurrency = "3" // 3 concurrent consumers\n    )\n    public void consumeOrderEvent(\n        @Payload OrderEvent event,\n        @Header(KafkaHeaders.RECEIVED_PARTITION) int partition,\n        @Header(KafkaHeaders.OFFSET) long offset\n    ) {\n        log.info("Received event: {} from partition: {} offset: {}",\n            event.getOrderId(), partition, offset);\n\n        try {\n            // Process the event\n            if ("ORDER_CREATED".equals(event.getEventType())) {\n                inventoryService.reserveItems(event.getOrderId());\n                notificationService.sendOrderConfirmation(event.getCustomerId());\n            }\n        } catch (Exception e) {\n            log.error("Error processing order event", e);\n            // Handle error - could send to DLQ (Dead Letter Queue)\n            throw e; // Will trigger retry or send to DLT\n        }\n    }\n}\n\n@Configuration\npublic class KafkaConsumerConfig {\n\n    @Value("${kafka.bootstrap-servers}")\n    private String bootstrapServers;\n\n    @Bean\n    public ConsumerFactory<String, OrderEvent> consumerFactory() {\n        Map<String, Object> config = new HashMap<>();\n        config.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);\n        config.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n        config.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, JsonDeserializer.class);\n        config.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");\n        config.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false); // Manual commit\n        config.put(JsonDeserializer.TRUSTED_PACKAGES, "*");\n        return new DefaultKafkaConsumerFactory<>(config);\n    }\n\n    @Bean\n    public ConcurrentKafkaListenerContainerFactory<String, OrderEvent> kafkaListenerContainerFactory() {\n        ConcurrentKafkaListenerContainerFactory<String, OrderEvent> factory =\n            new ConcurrentKafkaListenerContainerFactory<>();\n        factory.setConsumerFactory(consumerFactory());\n        factory.getContainerProperties().setAckMode(AckMode.RECORD);\n        return factory;\n    }\n}\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"apache-flink",children:"Apache Flink"}),"\n",(0,r.jsx)(n.h3,{id:"what-is-flink",children:"What is Flink?"}),"\n",(0,r.jsxs)(n.p,{children:["Apache Flink is a ",(0,r.jsx)(n.strong,{children:"distributed stream processing framework"})," designed for stateful computations over unbounded (streaming) and bounded (batch) data streams with ",(0,r.jsx)(n.strong,{children:"exactly-once processing guarantees"})," and ",(0,r.jsx)(n.strong,{children:"millisecond latency"}),"."]}),"\n",(0,r.jsx)(n.h3,{id:"core-concepts-1",children:"Core Concepts"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"DataStream API"}),": For streaming data processing"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"State Management"}),": Maintains computation state across millions of events"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Event Time Processing"}),": Handles out-of-order events correctly using watermarks"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Checkpointing"}),": Fault tolerance through periodic state snapshots"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Windows"}),": Time-based or count-based aggregations (tumbling, sliding, session)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Exactly-Once Semantics"}),": Guarantees each event processed exactly once"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Watermarks"}),": Tracks event time progress for handling late events"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"flink-architecture",children:"Flink Architecture"}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    subgraph "Flink Cluster"\n        JM[JobManager]\n        TM1[TaskManager 1]\n        TM2[TaskManager 2]\n        TM3[TaskManager 3]\n\n        subgraph "State Backend"\n            SB1[RocksDB]\n            SB2[Checkpoint Storage]\n        end\n    end\n\n    Source[Data Source<br/>Kafka/Kinesis]\n    Sink[Data Sink<br/>Database/Kafka]\n\n    Source --\x3e JM\n    JM --\x3e TM1\n    JM --\x3e TM2\n    JM --\x3e TM3\n\n    TM1 --\x3e SB1\n    TM2 --\x3e SB1\n    TM3 --\x3e SB1\n\n    SB1 --\x3e SB2\n\n    TM1 --\x3e Sink\n    TM2 --\x3e Sink\n    TM3 --\x3e Sink'}),"\n",(0,r.jsx)(n.h3,{id:"problems-flink-solves",children:"Problems Flink Solves"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Low-Latency Stream Processing"}),": Processes events in milliseconds"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Stateful Computations"}),": Maintains state across billions of events efficiently"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Exactly-Once Guarantees"}),": No duplicate processing or data loss"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Complex Event Processing (CEP)"}),": Pattern detection across event streams"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Out-of-Order Event Handling"}),": Correctly processes events that arrive late"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Large State Management"}),": Handles terabytes of state efficiently"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Event Time Processing"}),": Uses event timestamp, not processing time"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"when-to-use-flink",children:"When to Use Flink"}),"\n",(0,r.jsxs)(n.p,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"Use Flink When:"})]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Need real-time/near-real-time processing (milliseconds to seconds)"}),"\n",(0,r.jsx)(n.li,{children:"Require exactly-once processing guarantees"}),"\n",(0,r.jsx)(n.li,{children:"Complex event processing and pattern detection"}),"\n",(0,r.jsx)(n.li,{children:"Stateful stream processing (aggregations, joins, sessionization)"}),"\n",(0,r.jsx)(n.li,{children:"Handling out-of-order events is critical"}),"\n",(0,r.jsx)(n.li,{children:"Need high throughput AND low latency"}),"\n",(0,r.jsx)(n.li,{children:"Building real-time fraud detection, monitoring, or alerting"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["\u274c ",(0,r.jsx)(n.strong,{children:"Don't Use Flink When:"})]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Simple stateless transformations (Kafka Streams might be simpler)"}),"\n",(0,r.jsx)(n.li,{children:"Batch processing only (Spark is better)"}),"\n",(0,r.jsx)(n.li,{children:"Second-level latency is acceptable (Spark Streaming might be simpler)"}),"\n",(0,r.jsx)(n.li,{children:"Team lacks stream processing expertise"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"flink-job-example-java",children:"Flink Job Example (Java)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-java",children:'// Flink Job for Real-time Fraud Detection\npublic class FraudDetectionJob {\n\n    public static void main(String[] args) throws Exception {\n        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\n        // Enable checkpointing for fault tolerance\n        env.enableCheckpointing(60000); // Checkpoint every minute\n        env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);\n\n        // Configure Kafka source\n        KafkaSource<Transaction> kafkaSource = KafkaSource.<Transaction>builder()\n            .setBootstrapServers("localhost:9092")\n            .setTopics("transactions")\n            .setGroupId("fraud-detection-group")\n            .setStartingOffsets(OffsetsInitializer.earliest())\n            .setValueOnlyDeserializer(new TransactionDeserializer())\n            .build();\n\n        DataStream<Transaction> transactions = env\n            .fromSource(kafkaSource, WatermarkStrategy\n                .<Transaction>forBoundedOutOfOrderness(Duration.ofSeconds(10))\n                .withTimestampAssigner((event, timestamp) -> event.getTimestamp()),\n                "Kafka Source");\n\n        // Detect fraud: more than 3 transactions in 10 minutes\n        DataStream<Alert> alerts = transactions\n            .keyBy(Transaction::getUserId)\n            .window(TumblingEventTimeWindows.of(Time.minutes(10)))\n            .process(new FraudDetectionFunction());\n\n        // Pattern detection: Small transaction followed by large one within 5 minutes\n        Pattern<Transaction, ?> fraudPattern = Pattern\n            .<Transaction>begin("small")\n            .where(new SimpleCondition<Transaction>() {\n                @Override\n                public boolean filter(Transaction txn) {\n                    return txn.getAmount() < 10;\n                }\n            })\n            .next("large")\n            .where(new SimpleCondition<Transaction>() {\n                @Override\n                public boolean filter(Transaction txn) {\n                    return txn.getAmount() > 1000;\n                }\n            })\n            .within(Time.minutes(5));\n\n        PatternStream<Transaction> patternStream = CEP.pattern(\n            transactions.keyBy(Transaction::getUserId),\n            fraudPattern\n        );\n\n        DataStream<Alert> patternAlerts = patternStream\n            .select(new PatternSelectFunction<Transaction, Alert>() {\n                @Override\n                public Alert select(Map<String, List<Transaction>> pattern) {\n                    Transaction small = pattern.get("small").get(0);\n                    Transaction large = pattern.get("large").get(0);\n                    return new Alert(\n                        small.getUserId(),\n                        "Suspicious pattern detected",\n                        AlertSeverity.HIGH\n                    );\n                }\n            });\n\n        // Send alerts to Kafka\n        KafkaSink<Alert> kafkaSink = KafkaSink.<Alert>builder()\n            .setBootstrapServers("localhost:9092")\n            .setRecordSerializer(KafkaRecordSerializationSchema.builder()\n                .setTopic("fraud-alerts")\n                .setValueSerializationSchema(new AlertSerializer())\n                .build())\n            .setDeliveryGuarantee(DeliveryGuarantee.EXACTLY_ONCE)\n            .build();\n\n        alerts.sinkTo(kafkaSink);\n        patternAlerts.sinkTo(kafkaSink);\n\n        env.execute("Fraud Detection Job");\n    }\n}\n\n// Window function to count transactions\nclass FraudDetectionFunction extends ProcessWindowFunction<Transaction, Alert, String, TimeWindow> {\n\n    @Override\n    public void process(\n        String userId,\n        Context context,\n        Iterable<Transaction> transactions,\n        Collector<Alert> out\n    ) {\n        List<Transaction> txnList = new ArrayList<>();\n        transactions.forEach(txnList::add);\n\n        if (txnList.size() > 3) {\n            out.collect(new Alert(\n                userId,\n                String.format("User made %d transactions in 10 minutes", txnList.size()),\n                AlertSeverity.MEDIUM\n            ));\n        }\n\n        // Check for amount threshold\n        double totalAmount = txnList.stream()\n            .mapToDouble(Transaction::getAmount)\n            .sum();\n\n        if (totalAmount > 10000) {\n            out.collect(new Alert(\n                userId,\n                String.format("Total transaction amount $%.2f exceeds threshold", totalAmount),\n                AlertSeverity.HIGH\n            ));\n        }\n    }\n}\n'})}),"\n",(0,r.jsx)(n.h3,{id:"flink-with-spring-boot-integration",children:"Flink with Spring Boot Integration"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-java",children:'@Service\n@Slf4j\npublic class FlinkJobService {\n\n    @Value("${flink.jobmanager.address}")\n    private String jobManagerAddress;\n\n    public void submitFraudDetectionJob() throws Exception {\n        Configuration config = new Configuration();\n        config.setString("rest.address", jobManagerAddress);\n        config.setInteger("rest.port", 8081);\n\n        StreamExecutionEnvironment env = StreamExecutionEnvironment\n            .createRemoteEnvironment("localhost", 8081, config);\n\n        // Configure and submit job\n        env.enableCheckpointing(60000);\n\n        // Job definition here...\n\n        env.execute("Fraud Detection Job");\n    }\n\n    @Scheduled(fixedDelay = 300000) // Check every 5 minutes\n    public void monitorJobHealth() {\n        // Monitor Flink job status via REST API\n        RestTemplate restTemplate = new RestTemplate();\n        String url = String.format("http://%s:8081/jobs", jobManagerAddress);\n\n        try {\n            ResponseEntity<JobsResponse> response = restTemplate.getForEntity(\n                url, JobsResponse.class\n            );\n\n            if (response.getStatusCode() == HttpStatus.OK) {\n                JobsResponse jobs = response.getBody();\n                log.info("Running jobs: {}", jobs.getRunningJobs().size());\n            }\n        } catch (Exception e) {\n            log.error("Failed to check Flink job status", e);\n        }\n    }\n}\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"apache-spark",children:"Apache Spark"}),"\n",(0,r.jsx)(n.h3,{id:"what-is-spark",children:"What is Spark?"}),"\n",(0,r.jsxs)(n.p,{children:["Apache Spark is a ",(0,r.jsx)(n.strong,{children:"unified analytics engine"})," for large-scale data processing, supporting both ",(0,r.jsx)(n.strong,{children:"batch"})," and ",(0,r.jsx)(n.strong,{children:"stream processing"})," with in-memory computing capabilities. It's optimized for iterative algorithms and interactive data analysis."]}),"\n",(0,r.jsx)(n.h3,{id:"core-concepts-2",children:"Core Concepts"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"RDD"})," (Resilient Distributed Dataset): Immutable distributed collection (low-level API)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"DataFrame/Dataset"}),": Structured data APIs with query optimization (high-level API)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Spark Streaming"}),": Micro-batch stream processing (older API)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Structured Streaming"}),": True streaming with DataFrame API (newer, recommended)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"DAG"})," (Directed Acyclic Graph): Optimized execution plan"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"In-Memory Processing"}),": Caches intermediate data in RAM for speed"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Lazy Evaluation"}),": Transformations computed only when action is called"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Spark SQL"}),": SQL queries on structured data"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"spark-architecture",children:"Spark Architecture"}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    subgraph "Spark Application"\n        Driver[Driver Program<br/>SparkContext]\n\n        subgraph "Cluster Manager"\n            CM[YARN/Mesos/K8s]\n        end\n\n        subgraph "Worker Node 1"\n            E1[Executor]\n            T1[Task]\n            T2[Task]\n            Cache1[Cache]\n        end\n\n        subgraph "Worker Node 2"\n            E2[Executor]\n            T3[Task]\n            T4[Task]\n            Cache2[Cache]\n        end\n\n        subgraph "Worker Node 3"\n            E3[Executor]\n            T5[Task]\n            T6[Task]\n            Cache3[Cache]\n        end\n    end\n\n    Storage[Storage<br/>HDFS/S3]\n\n    Driver --\x3e CM\n    CM --\x3e E1\n    CM --\x3e E2\n    CM --\x3e E3\n\n    E1 --\x3e T1\n    E1 --\x3e T2\n    E2 --\x3e T3\n    E2 --\x3e T4\n    E3 --\x3e T5\n    E3 --\x3e T6\n\n    T1 --\x3e Cache1\n    T3 --\x3e Cache2\n    T5 --\x3e Cache3\n\n    Storage --\x3e T1\n    Storage --\x3e T3\n    Storage --\x3e T5'}),"\n",(0,r.jsx)(n.h3,{id:"problems-spark-solves",children:"Problems Spark Solves"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Large-Scale Batch Processing"}),": Process terabytes/petabytes of data"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ETL Pipelines"}),": Complex extract, transform, load operations"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"In-Memory Computing"}),": 100x faster than MapReduce for iterative algorithms"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Unified Processing"}),": Single framework for batch, streaming, ML, and SQL"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Machine Learning at Scale"}),": Distributed ML model training (MLlib)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Interactive Analytics"}),": Fast ad-hoc queries on big data"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Data Lake Processing"}),": Read/write multiple formats (Parquet, ORC, JSON, CSV)"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"when-to-use-spark",children:"When to Use Spark"}),"\n",(0,r.jsxs)(n.p,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"Use Spark When:"})]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Processing large volumes of data (GB to PB)"}),"\n",(0,r.jsx)(n.li,{children:"Building batch ETL pipelines"}),"\n",(0,r.jsx)(n.li,{children:"Training machine learning models on big data"}),"\n",(0,r.jsx)(n.li,{children:"Complex multi-stage data transformations"}),"\n",(0,r.jsx)(n.li,{children:"Need SQL interface for big data"}),"\n",(0,r.jsx)(n.li,{children:"Interactive data exploration and analytics"}),"\n",(0,r.jsx)(n.li,{children:"Streaming with second-level latency is acceptable"}),"\n",(0,r.jsx)(n.li,{children:"Working with data lakes (S3, HDFS)"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["\u274c ",(0,r.jsx)(n.strong,{children:"Don't Use Spark When:"})]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Real-time processing with millisecond latency required (use Flink)"}),"\n",(0,r.jsx)(n.li,{children:"Small datasets (< 1GB) - overhead not worth it"}),"\n",(0,r.jsx)(n.li,{children:"Simple transformations - use simpler tools"}),"\n",(0,r.jsx)(n.li,{children:"Need transactional guarantees (use databases)"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"spark-batch-processing-example",children:"Spark Batch Processing Example"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-java",children:'@Service\n@Slf4j\npublic class DataProcessingService {\n\n    private SparkSession spark;\n\n    @PostConstruct\n    public void init() {\n        spark = SparkSession.builder()\n            .appName("ETL Pipeline")\n            .master("yarn") // or "local[*]" for local mode\n            .config("spark.sql.warehouse.dir", "/user/hive/warehouse")\n            .config("spark.executor.memory", "4g")\n            .config("spark.executor.cores", "2")\n            .config("spark.dynamicAllocation.enabled", "true")\n            .enableHiveSupport()\n            .getOrCreate();\n    }\n\n    /**\n     * Daily ETL job to process user activity logs\n     */\n    public void processUserActivityLogs(String date) {\n        log.info("Processing user activity logs for date: {}", date);\n\n        // Read raw JSON logs from S3\n        Dataset<Row> rawLogs = spark.read()\n            .option("inferSchema", "true")\n            .json(String.format("s3://data-lake/raw/user-activity/%s/*", date));\n\n        // Data cleaning and transformation\n        Dataset<Row> cleanedLogs = rawLogs\n            .filter(col("userId").isNotNull())\n            .filter(col("timestamp").isNotNull())\n            .withColumn("date", to_date(col("timestamp")))\n            .withColumn("hour", hour(col("timestamp")))\n            .withColumn("session_id",\n                concat(col("userId"), lit("_"), date_format(col("timestamp"), "yyyyMMddHH")))\n            .drop("_corrupt_record");\n\n        // Register as temp view for SQL queries\n        cleanedLogs.createOrReplaceTempView("user_activity");\n\n        // Aggregate metrics using Spark SQL\n        Dataset<Row> dailyMetrics = spark.sql("""\n            SELECT\n                date,\n                userId,\n                COUNT(*) as total_events,\n                COUNT(DISTINCT session_id) as sessions,\n                SUM(CASE WHEN event_type = \'page_view\' THEN 1 ELSE 0 END) as page_views,\n                SUM(CASE WHEN event_type = \'click\' THEN 1 ELSE 0 END) as clicks,\n                SUM(CASE WHEN event_type = \'purchase\' THEN 1 ELSE 0 END) as purchases,\n                SUM(CAST(amount as DOUBLE)) as total_revenue\n            FROM user_activity\n            GROUP BY date, userId\n        """);\n\n        // Calculate advanced metrics\n        Dataset<Row> enrichedMetrics = dailyMetrics\n            .withColumn("conversion_rate",\n                when(col("page_views").gt(0),\n                    col("purchases").divide(col("page_views")))\n                .otherwise(0))\n            .withColumn("avg_revenue_per_session",\n                when(col("sessions").gt(0),\n                    col("total_revenue").divide(col("sessions")))\n                .otherwise(0));\n\n        // Write to data warehouse (partitioned by date)\n        enrichedMetrics\n            .write()\n            .mode(SaveMode.Overwrite)\n            .partitionBy("date")\n            .format("parquet")\n            .option("compression", "snappy")\n            .save("s3://data-warehouse/user-metrics/");\n\n        // Also write to Hive table\n        enrichedMetrics\n            .write()\n            .mode(SaveMode.Overwrite)\n            .insertInto("analytics.user_daily_metrics");\n\n        log.info("Successfully processed {} records", enrichedMetrics.count());\n    }\n\n    /**\n     * Feature engineering for ML model\n     */\n    public void prepareMLFeatures(String startDate, String endDate) {\n        // Read historical data\n        Dataset<Row> userMetrics = spark.read()\n            .parquet("s3://data-warehouse/user-metrics/")\n            .filter(col("date").between(startDate, endDate));\n\n        // Create features using window functions\n        WindowSpec userWindow = Window\n            .partitionBy("userId")\n            .orderBy("date")\n            .rowsBetween(-6, 0); // 7-day rolling window\n\n        Dataset<Row> features = userMetrics\n            .withColumn("avg_7day_revenue", avg("total_revenue").over(userWindow))\n            .withColumn("avg_7day_sessions", avg("sessions").over(userWindow))\n            .withColumn("total_7day_purchases", sum("purchases").over(userWindow))\n            .withColumn("days_since_last_purchase",\n                datediff(current_date(), max("date").over(userWindow)))\n            .filter(col("total_7day_purchases").gt(0)); // Only active users\n\n        // Split data for training and validation\n        Dataset<Row>[] splits = features.randomSplit(new double[]{0.8, 0.2}, 42);\n        Dataset<Row> trainingData = splits[0];\n        Dataset<Row> validationData = splits[1];\n\n        // Save prepared datasets\n        trainingData.write()\n            .mode(SaveMode.Overwrite)\n            .parquet("s3://ml-data/churn-prediction/training/");\n\n        validationData.write()\n            .mode(SaveMode.Overwrite)\n            .parquet("s3://ml-data/churn-prediction/validation/");\n    }\n}\n'})}),"\n",(0,r.jsx)(n.h3,{id:"spark-structured-streaming-example",children:"Spark Structured Streaming Example"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-java",children:'@Service\n@Slf4j\npublic class StreamingAnalyticsService {\n\n    private SparkSession spark;\n\n    @PostConstruct\n    public void init() {\n        spark = SparkSession.builder()\n            .appName("Real-time Analytics")\n            .master("yarn")\n            .config("spark.sql.streaming.checkpointLocation", "/tmp/checkpoint")\n            .getOrCreate();\n    }\n\n    /**\n     * Real-time dashboard metrics from Kafka\n     */\n    public void startRealtimeMetrics() {\n        // Read from Kafka\n        Dataset<Row> kafkaStream = spark.readStream()\n            .format("kafka")\n            .option("kafka.bootstrap.servers", "localhost:9092")\n            .option("subscribe", "user-events")\n            .option("startingOffsets", "latest")\n            .load();\n\n        // Parse JSON value\n        Dataset<Row> events = kafkaStream\n            .selectExpr("CAST(value AS STRING) as json")\n            .select(from_json(col("json"), getEventSchema()).as("data"))\n            .select("data.*")\n            .withColumn("event_time", to_timestamp(col("timestamp")));\n\n        // Windowed aggregations (5-minute tumbling windows)\n        Dataset<Row> windowedMetrics = events\n            .withWatermark("event_time", "10 minutes") // Handle late data\n            .groupBy(\n                window(col("event_time"), "5 minutes"),\n                col("page_url")\n            )\n            .agg(\n                count("*").as("total_events"),\n                countDistinct("user_id").as("unique_users"),\n                sum(when(col("event_type").equalTo("click"), 1).otherwise(0)).as("clicks")\n            );\n\n        // Write to console for monitoring (in production, write to database)\n        StreamingQuery consoleQuery = windowedMetrics.writeStream()\n            .outputMode("update")\n            .format("console")\n            .option("truncate", false)\n            .start();\n\n        // Write to PostgreSQL for dashboard\n        StreamingQuery dbQuery = windowedMetrics.writeStream()\n            .outputMode("update")\n            .foreachBatch((batchDF, batchId) -> {\n                batchDF.write()\n                    .format("jdbc")\n                    .option("url", "jdbc:postgresql://localhost:5432/analytics")\n                    .option("dbtable", "realtime_metrics")\n                    .option("user", "postgres")\n                    .option("password", "password")\n                    .mode(SaveMode.Append)\n                    .save();\n            })\n            .start();\n\n        // Session analysis (session window)\n        Dataset<Row> sessions = events\n            .withWatermark("event_time", "10 minutes")\n            .groupBy(\n                col("user_id"),\n                session_window(col("event_time"), "30 minutes") // 30 min inactivity gap\n            )\n            .agg(\n                count("*").as("events_in_session"),\n                min("event_time").as("session_start"),\n                max("event_time").as("session_end")\n            )\n            .withColumn("session_duration_minutes",\n                (unix_timestamp(col("session_end")).minus(unix_timestamp(col("session_start")))).divide(60));\n\n        // Write session data to Kafka for downstream processing\n        StreamingQuery kafkaQuery = sessions\n            .selectExpr("user_id as key", "to_json(struct(*)) as value")\n            .writeStream()\n            .format("kafka")\n            .option("kafka.bootstrap.servers", "localhost:9092")\n            .option("topic", "user-sessions")\n            .option("checkpointLocation", "/tmp/checkpoint/sessions")\n            .start();\n\n        try {\n            consoleQuery.awaitTermination();\n        } catch (StreamingQueryException e) {\n            log.error("Streaming query failed", e);\n        }\n    }\n\n    private StructType getEventSchema() {\n        return new StructType()\n            .add("user_id", DataTypes.StringType)\n            .add("event_type", DataTypes.StringType)\n            .add("page_url", DataTypes.StringType)\n            .add("timestamp", DataTypes.StringType);\n    }\n}\n\n@Configuration\npublic class SparkConfig {\n\n    @Bean\n    public SparkSession sparkSession() {\n        return SparkSession.builder()\n            .appName("Spring Boot Spark App")\n            .master("local[*]") // Use "yarn" in production\n            .config("spark.sql.adaptive.enabled", "true")\n            .config("spark.sql.adaptive.coalescePartitions.enabled", "true")\n            .getOrCreate();\n    }\n\n    @PreDestroy\n    public void cleanup() {\n        SparkSession spark = SparkSession.active();\n        if (spark != null) {\n            spark.stop();\n        }\n    }\n}\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"comparison",children:"Comparison & When to Use What"}),"\n",(0,r.jsx)(n.h3,{id:"feature-comparison-table",children:"Feature Comparison Table"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Feature"}),(0,r.jsx)(n.th,{children:"Kafka"}),(0,r.jsx)(n.th,{children:"Flink"}),(0,r.jsx)(n.th,{children:"Spark"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Primary Purpose"})}),(0,r.jsx)(n.td,{children:"Message Broker"}),(0,r.jsx)(n.td,{children:"Stream Processing"}),(0,r.jsx)(n.td,{children:"Batch + Streaming"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Processing Model"})}),(0,r.jsx)(n.td,{children:"-"}),(0,r.jsx)(n.td,{children:"True Streaming"}),(0,r.jsx)(n.td,{children:"Micro-batch (Streaming)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Latency"})}),(0,r.jsx)(n.td,{children:"Milliseconds"}),(0,r.jsx)(n.td,{children:"Milliseconds"}),(0,r.jsx)(n.td,{children:"Seconds"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Throughput"})}),(0,r.jsx)(n.td,{children:"Very High"}),(0,r.jsx)(n.td,{children:"High"}),(0,r.jsx)(n.td,{children:"Very High"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"State Management"})}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{children:"Yes (Advanced)"}),(0,r.jsx)(n.td,{children:"Yes (Limited)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Exactly-Once"})}),(0,r.jsx)(n.td,{children:"Yes (with config)"}),(0,r.jsx)(n.td,{children:"Yes (Native)"}),(0,r.jsx)(n.td,{children:"Yes (Structured Streaming)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Windowing"})}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{children:"Advanced"}),(0,r.jsx)(n.td,{children:"Good"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Complex Event Processing"})}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{children:"Yes"}),(0,r.jsx)(n.td,{children:"Limited"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Batch Processing"})}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{children:"Yes"}),(0,r.jsx)(n.td,{children:"Excellent"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Machine Learning"})}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{children:"Limited"}),(0,r.jsx)(n.td,{children:"Excellent (MLlib)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"SQL Support"})}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{children:"Yes"}),(0,r.jsx)(n.td,{children:"Excellent"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Event Time Processing"})}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{children:"Excellent"}),(0,r.jsx)(n.td,{children:"Good"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Out-of-Order Events"})}),(0,r.jsx)(n.td,{children:"N/A"}),(0,r.jsx)(n.td,{children:"Handles Well"}),(0,r.jsx)(n.td,{children:"Handles (with watermarks)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Learning Curve"})}),(0,r.jsx)(n.td,{children:"Moderate"}),(0,r.jsx)(n.td,{children:"Steep"}),(0,r.jsx)(n.td,{children:"Moderate"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Operational Complexity"})}),(0,r.jsx)(n.td,{children:"Moderate"}),(0,r.jsx)(n.td,{children:"High"}),(0,r.jsx)(n.td,{children:"Moderate"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Use Cases"})}),(0,r.jsx)(n.td,{children:"Data Pipeline"}),(0,r.jsx)(n.td,{children:"Real-time Analytics"}),(0,r.jsx)(n.td,{children:"Batch ETL, ML"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"decision-matrix",children:"Decision Matrix"}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TD\n    Start[Choose Technology]\n\n    Start --\x3e Q1{Need Message<br/>Persistence?}\n    Q1 --\x3e|Yes| Q2{Real-time<br/>Processing?}\n    Q1 --\x3e|No| Q3{Batch or<br/>Streaming?}\n\n    Q2 --\x3e|No, just transport| Kafka[Use Kafka]\n    Q2 --\x3e|Yes| Q4{Latency<br/>Requirement?}\n\n    Q4 --\x3e|Milliseconds| Q5{Complex CEP or<br/>Stateful Logic?}\n    Q4 --\x3e|Seconds OK| SparkStreaming[Use Spark<br/>Structured Streaming]\n\n    Q5 --\x3e|Yes| Flink[Use Flink]\n    Q5 --\x3e|Simple transforms| KafkaStreams[Use Kafka Streams<br/>or Flink]\n\n    Q3 --\x3e|Batch| Q6{Data Size?}\n    Q3 --\x3e|Streaming| Q4\n\n    Q6 --\x3e|Large TB/PB| Q7{Need ML?}\n    Q6 --\x3e|Small GB| SimpleTools[Use Pandas/ETL tools]\n\n    Q7 --\x3e|Yes| Spark[Use Spark]\n    Q7 --\x3e|No| SparkOrOther[Use Spark or<br/>other ETL tools]\n\n    style Kafka fill:#ff9999\n    style Flink fill:#99ccff\n    style Spark fill:#99ff99\n    style SparkStreaming fill:#99ff99"}),"\n",(0,r.jsx)(n.h3,{id:"common-architecture-patterns",children:"Common Architecture Patterns"}),"\n",(0,r.jsx)(n.h4,{id:"pattern-1-lambda-architecture",children:"Pattern 1: Lambda Architecture"}),"\n",(0,r.jsx)(n.mermaid,{value:'graph LR\n    subgraph "Data Sources"\n        DS[Data Sources]\n    end\n\n    subgraph "Ingestion Layer"\n        K[Kafka]\n    end\n\n    subgraph "Speed Layer Real-time"\n        F[Flink]\n        RT[Real-time Views]\n    end\n\n    subgraph "Batch Layer"\n        S[Spark Batch]\n        BV[Batch Views]\n    end\n\n    subgraph "Serving Layer"\n        DB[(Database)]\n        API[API]\n    end\n\n    DS --\x3e K\n    K --\x3e F\n    K --\x3e S\n    F --\x3e RT\n    S --\x3e BV\n    RT --\x3e DB\n    BV --\x3e DB\n    DB --\x3e API'}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"When to use:"})," Need both real-time and accurate historical analytics"]}),"\n",(0,r.jsx)(n.h4,{id:"pattern-2-kappa-architecture-streaming-first",children:"Pattern 2: Kappa Architecture (Streaming-First)"}),"\n",(0,r.jsx)(n.mermaid,{value:'graph LR\n    subgraph "Data Sources"\n        DS[Data Sources]\n    end\n\n    subgraph "Stream Processing"\n        K[Kafka]\n        F[Flink/Spark Streaming]\n    end\n\n    subgraph "Storage"\n        DB[(Database)]\n        DL[Data Lake]\n    end\n\n    subgraph "Serving"\n        API[API]\n    end\n\n    DS --\x3e K\n    K --\x3e F\n    F --\x3e DB\n    F --\x3e DL\n    DB --\x3e API\n    DL --\x3e API'}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"When to use:"})," All data can be processed as streams, simpler than Lambda"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"design-patterns",children:"Real-World Design Patterns"}),"\n",(0,r.jsx)(n.h3,{id:"pattern-1-e-commerce-order-processing-system",children:"Pattern 1: E-commerce Order Processing System"}),"\n",(0,r.jsx)(n.h4,{id:"architecture-overview",children:"Architecture Overview"}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    subgraph "Frontend"\n        Web[Web App]\n        Mobile[Mobile App]\n    end\n\n    subgraph "API Gateway"\n        GW[API Gateway]\n    end\n\n    subgraph "Services"\n        OS[Order Service]\n    end\n\n    subgraph "Message Broker"\n        K1[Kafka: orders]\n        K2[Kafka: payments]\n        K3[Kafka: notifications]\n    end\n\n    subgraph "Consumer Services"\n        IS[Inventory Service]\n        PS[Payment Service]\n        NS[Notification Service]\n        AS[Analytics Service]\n    end\n\n    subgraph "Stream Processing"\n        F[Flink: Fraud Detection]\n    end\n\n    subgraph "Data Warehouse"\n        SP[Spark: Daily ETL]\n        DW[(Data Warehouse)]\n    end\n\n    Web --\x3e GW\n    Mobile --\x3e GW\n    GW --\x3e OS\n\n    OS --\x3e K1\n    K1 --\x3e IS\n    K1 --\x3e PS\n    K1 --\x3e AS\n    K1 --\x3e F\n\n    PS --\x3e K2\n    K2 --\x3e NS\n\n    NS --\x3e K3\n\n    F --\x3e K3\n\n    K1 --\x3e SP\n    K2 --\x3e SP\n    SP --\x3e DW'}),"\n",(0,r.jsx)(n.h4,{id:"technology-choices",children:"Technology Choices"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Kafka"})," is used for:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Order Events"}),": Decouple order service from downstream services"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Event Sourcing"}),": Complete audit trail of all order state changes"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Replay"}),": Reprocess orders if a service was down"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Scale"}),": Handle Black Friday traffic spikes"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Flink"})," is used for:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Real-time Fraud Detection"}),": Process each transaction in milliseconds"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Pattern Detection"}),": Unusual purchasing patterns (e.g., multiple high-value orders)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Stateful Processing"}),": Track user behavior across sessions"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Exactly-Once"}),": No duplicate fraud alerts"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Spark"})," is used for:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Daily Analytics"}),": Process all order data for business reports"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Recommendation Training"}),": Train product recommendation models"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Data Lake"}),": Consolidate data from multiple Kafka topics"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Historical Analysis"}),": Complex queries on months of data"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"spring-boot-implementation",children:"Spring Boot Implementation"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-java",children:'// Order Service - Kafka Producer\n@RestController\n@RequestMapping("/api/orders")\n@Slf4j\npublic class OrderController {\n\n    @Autowired\n    private OrderEventProducer eventProducer;\n\n    @Autowired\n    private OrderRepository orderRepository;\n\n    @PostMapping\n    @Transactional\n    public ResponseEntity<OrderResponse> createOrder(@RequestBody OrderRequest request) {\n        // Save order to database\n        Order order = Order.builder()\n            .customerId(request.getCustomerId())\n            .items(request.getItems())\n            .totalAmount(request.getTotalAmount())\n            .status(OrderStatus.PENDING)\n            .createdAt(Instant.now())\n            .build();\n\n        order = orderRepository.save(order);\n\n        // Publish event to Kafka (async, non-blocking)\n        OrderCreatedEvent event = OrderCreatedEvent.builder()\n            .orderId(order.getId())\n            .customerId(order.getCustomerId())\n            .items(order.getItems())\n            .totalAmount(order.getTotalAmount())\n            .timestamp(order.getCreatedAt())\n            .build();\n\n        eventProducer.publishOrderCreated(event);\n\n        log.info("Order created: {}", order.getId());\n\n        return ResponseEntity.ok(OrderResponse.from(order));\n    }\n}\n\n// Inventory Service - Kafka Consumer\n@Service\n@Slf4j\npublic class InventoryEventConsumer {\n\n    @Autowired\n    private InventoryService inventoryService;\n\n    @Autowired\n    private KafkaTemplate<String, OrderStatusEvent> kafkaTemplate;\n\n    @KafkaListener(\n        topics = "orders",\n        groupId = "inventory-service",\n        containerFactory = "kafkaListenerContainerFactory"\n    )\n    @Transactional\n    public void handleOrderCreated(OrderCreatedEvent event) {\n        log.info("Processing order for inventory: {}", event.getOrderId());\n\n        try {\n            // Check and reserve inventory\n            boolean available = inventoryService.checkAvailability(event.getItems());\n\n            if (available) {\n                inventoryService.reserveItems(event.getOrderId(), event.getItems());\n\n                // Publish success event\n                OrderStatusEvent statusEvent = OrderStatusEvent.builder()\n                    .orderId(event.getOrderId())\n                    .status("INVENTORY_RESERVED")\n                    .timestamp(Instant.now())\n                    .build();\n\n                kafkaTemplate.send("order-status", event.getOrderId(), statusEvent);\n                log.info("Inventory reserved for order: {}", event.getOrderId());\n            } else {\n                // Publish failure event\n                OrderStatusEvent statusEvent = OrderStatusEvent.builder()\n                    .orderId(event.getOrderId())\n                    .status("INVENTORY_UNAVAILABLE")\n                    .reason("One or more items out of stock")\n                    .timestamp(Instant.now())\n                    .build();\n\n                kafkaTemplate.send("order-status", event.getOrderId(), statusEvent);\n                log.warn("Insufficient inventory for order: {}", event.getOrderId());\n            }\n        } catch (Exception e) {\n            log.error("Error processing order inventory", e);\n            // Error will trigger retry or DLQ\n            throw e;\n        }\n    }\n}\n\n// Payment Service - Saga Pattern Implementation\n@Service\n@Slf4j\npublic class PaymentEventConsumer {\n\n    @Autowired\n    private PaymentService paymentService;\n\n    @Autowired\n    private KafkaTemplate<String, PaymentEvent> kafkaTemplate;\n\n    @KafkaListener(\n        topics = "order-status",\n        groupId = "payment-service"\n    )\n    public void handleOrderStatusChange(OrderStatusEvent event) {\n        // Only process when inventory is reserved\n        if ("INVENTORY_RESERVED".equals(event.getStatus())) {\n            log.info("Processing payment for order: {}", event.getOrderId());\n\n            try {\n                PaymentResult result = paymentService.processPayment(\n                    event.getOrderId(),\n                    event.getCustomerId(),\n                    event.getAmount()\n                );\n\n                PaymentEvent paymentEvent = PaymentEvent.builder()\n                    .orderId(event.getOrderId())\n                    .paymentId(result.getPaymentId())\n                    .status(result.isSuccess() ? "PAYMENT_SUCCESS" : "PAYMENT_FAILED")\n                    .amount(event.getAmount())\n                    .timestamp(Instant.now())\n                    .build();\n\n                kafkaTemplate.send("payments", event.getOrderId(), paymentEvent);\n\n                if (result.isSuccess()) {\n                    log.info("Payment successful for order: {}", event.getOrderId());\n                } else {\n                    log.warn("Payment failed for order: {}", event.getOrderId());\n                    // Publish compensating event to release inventory\n                    publishInventoryRelease(event.getOrderId());\n                }\n            } catch (Exception e) {\n                log.error("Error processing payment", e);\n                publishInventoryRelease(event.getOrderId());\n                throw e;\n            }\n        }\n    }\n\n    private void publishInventoryRelease(String orderId) {\n        CompensationEvent compensationEvent = CompensationEvent.builder()\n            .orderId(orderId)\n            .action("RELEASE_INVENTORY")\n            .timestamp(Instant.now())\n            .build();\n\n        kafkaTemplate.send("compensations", orderId, compensationEvent);\n    }\n}\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"pattern-2-real-time-fraud-detection-system",children:"Pattern 2: Real-time Fraud Detection System"}),"\n",(0,r.jsx)(n.h4,{id:"architecture",children:"Architecture"}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    subgraph "Transaction Sources"\n        ATM[ATM Transactions]\n        POS[POS Terminals]\n        Online[Online Purchases]\n    end\n\n    subgraph "Ingestion"\n        K[Kafka: transactions]\n    end\n\n    subgraph "Real-time Processing"\n        F1[Flink: Rule Engine]\n        F2[Flink: ML Scoring]\n        F3[Flink: Pattern Detection]\n    end\n\n    subgraph "State Store"\n        RS[(Redis: User Profiles)]\n    end\n\n    subgraph "ML Model"\n        MS[Model Server]\n    end\n\n    subgraph "Alerting"\n        KA[Kafka: alerts]\n        Alert[Alert Service]\n    end\n\n    subgraph "Case Management"\n        CM[Case Management UI]\n        DB[(PostgreSQL)]\n    end\n\n    ATM --\x3e K\n    POS --\x3e K\n    Online --\x3e K\n\n    K --\x3e F1\n    K --\x3e F2\n    K --\x3e F3\n\n    F1 --\x3e RS\n    F2 --\x3e MS\n\n    F1 --\x3e KA\n    F2 --\x3e KA\n    F3 --\x3e KA\n\n    KA --\x3e Alert\n    Alert --\x3e DB\n    DB --\x3e CM'}),"\n",(0,r.jsx)(n.h4,{id:"flink-fraud-detection-job",children:"Flink Fraud Detection Job"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-java",children:'public class FraudDetectionPipeline {\n\n    public static void main(String[] args) throws Exception {\n        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\n        // Configure for exactly-once processing\n        env.enableCheckpointing(60000);\n        env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);\n        env.getCheckpointConfig().setMinPauseBetweenCheckpoints(30000);\n\n        // Read transactions from Kafka\n        KafkaSource<Transaction> source = KafkaSource.<Transaction>builder()\n            .setBootstrapServers("localhost:9092")\n            .setTopics("transactions")\n            .setGroupId("fraud-detection")\n            .setStartingOffsets(OffsetsInitializer.latest())\n            .setValueOnlyDeserializer(new TransactionDeserializer())\n            .build();\n\n        DataStream<Transaction> transactions = env\n            .fromSource(source, WatermarkStrategy\n                .<Transaction>forBoundedOutOfOrderness(Duration.ofSeconds(5))\n                .withTimestampAssigner((txn, ts) -> txn.getTimestamp()),\n                "Kafka Source");\n\n        // Rule 1: Velocity check - multiple transactions in short time\n        DataStream<FraudAlert> velocityAlerts = transactions\n            .keyBy(Transaction::getCardNumber)\n            .window(SlidingEventTimeWindows.of(Time.minutes(5), Time.minutes(1)))\n            .process(new VelocityCheckFunction());\n\n        // Rule 2: Geographic impossibility\n        DataStream<FraudAlert> geoAlerts = transactions\n            .keyBy(Transaction::getCardNumber)\n            .process(new GeographicCheckFunction());\n\n        // Rule 3: Amount threshold\n        DataStream<FraudAlert> amountAlerts = transactions\n            .filter(txn -> txn.getAmount() > 5000)\n            .keyBy(Transaction::getCardNumber)\n            .process(new AmountThresholdFunction());\n\n        // Rule 4: Pattern detection - testing followed by large purchase\n        Pattern<Transaction, ?> testPattern = Pattern\n            .<Transaction>begin("smallTransactions")\n            .where(new IterativeCondition<Transaction>() {\n                @Override\n                public boolean filter(Transaction txn, Context<Transaction> ctx) throws Exception {\n                    return txn.getAmount() < 1.0;\n                }\n            })\n            .timesOrMore(3) // 3 or more small transactions\n            .consecutive()\n            .within(Time.minutes(10))\n            .next("largeTransaction")\n            .where(new SimpleCondition<Transaction>() {\n                @Override\n                public boolean filter(Transaction txn) {\n                    return txn.getAmount() > 1000;\n                }\n            });\n\n        PatternStream<Transaction> patternStream = CEP.pattern(\n            transactions.keyBy(Transaction::getCardNumber),\n            testPattern\n        );\n\n        DataStream<FraudAlert> patternAlerts = patternStream\n            .process(new PatternProcessFunction<Transaction, FraudAlert>() {\n                @Override\n                public void processMatch(\n                    Map<String, List<Transaction>> match,\n                    Context ctx,\n                    Collector<FraudAlert> out\n                ) {\n                    List<Transaction> small = match.get("smallTransactions");\n                    Transaction large = match.get("largeTransaction").get(0);\n\n                    out.collect(FraudAlert.builder()\n                        .cardNumber(large.getCardNumber())\n                        .alertType("CARD_TESTING")\n                        .severity(AlertSeverity.HIGH)\n                        .description(String.format(\n                            "%d small transactions followed by $%.2f purchase",\n                            small.size(), large.getAmount()\n                        ))\n                        .transactionId(large.getId())\n                        .timestamp(large.getTimestamp())\n                        .build());\n                }\n            });\n\n        // Combine all alerts\n        DataStream<FraudAlert> allAlerts = velocityAlerts\n            .union(geoAlerts)\n            .union(amountAlerts)\n            .union(patternAlerts);\n\n        // Enrich with ML model scoring\n        DataStream<EnrichedAlert> enrichedAlerts = allAlerts\n            .keyBy(FraudAlert::getCardNumber)\n            .process(new MLScoringFunction());\n\n        // Filter high-risk alerts and send to Kafka\n        enrichedAlerts\n            .filter(alert -> alert.getRiskScore() > 0.7)\n            .sinkTo(KafkaSink.<EnrichedAlert>builder()\n                .setBootstrapServers("localhost:9092")\n                .setRecordSerializer(KafkaRecordSerializationSchema.builder()\n                    .setTopic("fraud-alerts")\n                    .setValueSerializationSchema(new AlertSerializer())\n                    .build())\n                .setDeliveryGuarantee(DeliveryGuarantee.EXACTLY_ONCE)\n                .build());\n\n        // Write all alerts to database for audit\n        enrichedAlerts\n            .addSink(new JdbcSink<>(\n                "INSERT INTO fraud_alerts (card_number, alert_type, risk_score, timestamp) " +\n                "VALUES (?, ?, ?, ?)",\n                (ps, alert) -> {\n                    ps.setString(1, alert.getCardNumber());\n                    ps.setString(2, alert.getAlertType());\n                    ps.setDouble(3, alert.getRiskScore());\n                    ps.setTimestamp(4, Timestamp.from(Instant.ofEpochMilli(alert.getTimestamp())));\n                },\n                new JdbcConnectionOptions.JdbcConnectionOptionsBuilder()\n                    .withUrl("jdbc:postgresql://localhost:5432/fraud_db")\n                    .withDriverName("org.postgresql.Driver")\n                    .withUsername("postgres")\n                    .withPassword("password")\n                    .build()\n            ));\n\n        env.execute("Fraud Detection Pipeline");\n    }\n}\n\n// Velocity check: Too many transactions in short time\nclass VelocityCheckFunction extends ProcessWindowFunction<Transaction, FraudAlert, String, TimeWindow> {\n\n    private static final int THRESHOLD = 5; // Max 5 transactions per 5 minutes\n\n    @Override\n    public void process(\n        String cardNumber,\n        Context context,\n        Iterable<Transaction> transactions,\n        Collector<FraudAlert> out\n    ) {\n        List<Transaction> txnList = StreamSupport\n            .stream(transactions.spliterator(), false)\n            .collect(Collectors.toList());\n\n        if (txnList.size() > THRESHOLD) {\n            out.collect(FraudAlert.builder()\n                .cardNumber(cardNumber)\n                .alertType("HIGH_VELOCITY")\n                .severity(AlertSeverity.MEDIUM)\n                .description(String.format(\n                    "%d transactions in 5 minutes (threshold: %d)",\n                    txnList.size(), THRESHOLD\n                ))\n                .transactionId(txnList.get(txnList.size() - 1).getId())\n                .timestamp(System.currentTimeMillis())\n                .build());\n        }\n    }\n}\n\n// Geographic impossibility check\nclass GeographicCheckFunction extends KeyedProcessFunction<String, Transaction, FraudAlert> {\n\n    private ValueState<Transaction> lastTransactionState;\n\n    @Override\n    public void open(Configuration parameters) {\n        ValueStateDescriptor<Transaction> descriptor = new ValueStateDescriptor<>(\n            "lastTransaction",\n            Transaction.class\n        );\n        lastTransactionState = getRuntimeContext().getState(descriptor);\n    }\n\n    @Override\n    public void processElement(\n        Transaction current,\n        Context ctx,\n        Collector<FraudAlert> out\n    ) throws Exception {\n        Transaction last = lastTransactionState.value();\n\n        if (last != null) {\n            // Calculate distance between locations\n            double distance = calculateDistance(\n                last.getLatitude(), last.getLongitude(),\n                current.getLatitude(), current.getLongitude()\n            );\n\n            // Calculate time difference in hours\n            long timeDiffMs = current.getTimestamp() - last.getTimestamp();\n            double timeDiffHours = timeDiffMs / (1000.0 * 60 * 60);\n\n            // Check if physically impossible (speed > 800 km/h)\n            double requiredSpeed = distance / timeDiffHours;\n\n            if (requiredSpeed > 800) {\n                out.collect(FraudAlert.builder()\n                    .cardNumber(current.getCardNumber())\n                    .alertType("GEOGRAPHIC_IMPOSSIBILITY")\n                    .severity(AlertSeverity.HIGH)\n                    .description(String.format(\n                        "Transaction %.0f km away in %.1f hours (%.0f km/h required)",\n                        distance, timeDiffHours, requiredSpeed\n                    ))\n                    .transactionId(current.getId())\n                    .timestamp(current.getTimestamp())\n                    .build());\n            }\n        }\n\n        lastTransactionState.update(current);\n    }\n\n    private double calculateDistance(double lat1, double lon1, double lat2, double lon2) {\n        // Haversine formula\n        double R = 6371; // Earth\'s radius in km\n        double dLat = Math.toRadians(lat2 - lat1);\n        double dLon = Math.toRadians(lon2 - lon1);\n        double a = Math.sin(dLat / 2) * Math.sin(dLat / 2) +\n                   Math.cos(Math.toRadians(lat1)) * Math.cos(Math.toRadians(lat2)) *\n                   Math.sin(dLon / 2) * Math.sin(dLon / 2);\n        double c = 2 * Math.atan2(Math.sqrt(a), Math.sqrt(1 - a));\n        return R * c;\n    }\n}\n\n// ML model scoring function\nclass MLScoringFunction extends KeyedProcessFunction<String, FraudAlert, EnrichedAlert> {\n\n    private transient RestTemplate restTemplate;\n\n    @Override\n    public void open(Configuration parameters) {\n        restTemplate = new RestTemplate();\n    }\n\n    @Override\n    public void processElement(\n        FraudAlert alert,\n        Context ctx,\n        Collector<EnrichedAlert> out\n    ) throws Exception {\n        // Call ML model service for risk scoring\n        try {\n            String url = "http://ml-model-service:8080/score";\n            ScoringRequest request = new ScoringRequest(alert);\n\n            ResponseEntity<ScoringResponse> response = restTemplate.postForEntity(\n                url, request, ScoringResponse.class\n            );\n\n            double riskScore = response.getBody().getRiskScore();\n\n            out.collect(EnrichedAlert.builder()\n                .alert(alert)\n                .riskScore(riskScore)\n                .modelVersion(response.getBody().getModelVersion())\n                .build());\n        } catch (Exception e) {\n            // Fallback: use rule-based score if ML service unavailable\n            double fallbackScore = calculateFallbackScore(alert);\n            out.collect(EnrichedAlert.builder()\n                .alert(alert)\n                .riskScore(fallbackScore)\n                .modelVersion("fallback")\n                .build());\n        }\n    }\n\n    private double calculateFallbackScore(FraudAlert alert) {\n        switch (alert.getSeverity()) {\n            case HIGH: return 0.8;\n            case MEDIUM: return 0.5;\n            case LOW: return 0.3;\n            default: return 0.1;\n        }\n    }\n}\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"pattern-3-iot-sensor-data-analytics",children:"Pattern 3: IoT Sensor Data Analytics"}),"\n",(0,r.jsx)(n.h4,{id:"architecture-1",children:"Architecture"}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    subgraph "IoT Devices"\n        S1[Temperature Sensors]\n        S2[Pressure Sensors]\n        S3[Vibration Sensors]\n    end\n\n    subgraph "Edge Gateway"\n        EG[IoT Gateway]\n    end\n\n    subgraph "Ingestion"\n        K[Kafka: sensor-data]\n    end\n\n    subgraph "Stream Processing"\n        F[Flink: Anomaly Detection]\n    end\n\n    subgraph "Real-time Alerts"\n        KA[Kafka: alerts]\n        Dash[Monitoring Dashboard]\n    end\n\n    subgraph "Historical Analysis"\n        SP[Spark: Daily Aggregation]\n        DL[(Data Lake S3)]\n    end\n\n    subgraph "Machine Learning"\n        ML[Spark ML: Predictive Maintenance]\n        Models[(Model Registry)]\n    end\n\n    S1 --\x3e EG\n    S2 --\x3e EG\n    S3 --\x3e EG\n\n    EG --\x3e K\n    K --\x3e F\n    K --\x3e SP\n\n    F --\x3e KA\n    KA --\x3e Dash\n\n    SP --\x3e DL\n    DL --\x3e ML\n    ML --\x3e Models'}),"\n",(0,r.jsx)(n.h4,{id:"spring-boot-kafka-producer-iot-gateway",children:"Spring Boot Kafka Producer (IoT Gateway)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-java",children:'@Service\n@Slf4j\npublic class SensorDataProducer {\n\n    @Autowired\n    private KafkaTemplate<String, SensorReading> kafkaTemplate;\n\n    private static final String TOPIC = "sensor-data";\n\n    /**\n     * Batch send sensor readings for efficiency\n     */\n    public void sendSensorReadings(List<SensorReading> readings) {\n        List<CompletableFuture<SendResult<String, SensorReading>>> futures = readings.stream()\n            .map(reading -> kafkaTemplate.send(\n                TOPIC,\n                reading.getSensorId(), // Key for partitioning\n                reading\n            ))\n            .map(ListenableFuture::completable)\n            .collect(Collectors.toList());\n\n        // Wait for all sends to complete\n        CompletableFuture.allOf(futures.toArray(new CompletableFuture[0]))\n            .thenRun(() -> log.info("Successfully sent {} sensor readings", readings.size()))\n            .exceptionally(ex -> {\n                log.error("Failed to send sensor readings", ex);\n                return null;\n            });\n    }\n\n    /**\n     * Process readings from MQTT broker\n     */\n    @Scheduled(fixedDelay = 5000) // Every 5 seconds\n    public void processIncomingReadings() {\n        // Simulating batch of readings from MQTT\n        List<SensorReading> batch = new ArrayList<>();\n\n        // In real implementation, this would read from MQTT broker\n        // For demo, generating sample data\n        for (int i = 0; i < 100; i++) {\n            SensorReading reading = SensorReading.builder()\n                .sensorId("sensor-" + (i % 10))\n                .sensorType(SensorType.TEMPERATURE)\n                .value(20.0 + Math.random() * 10)\n                .unit("celsius")\n                .timestamp(Instant.now().toEpochMilli())\n                .location(new Location(40.7128 + Math.random(), -74.0060 + Math.random()))\n                .build();\n\n            batch.add(reading);\n        }\n\n        sendSensorReadings(batch);\n    }\n}\n\n@Configuration\npublic class KafkaProducerConfig {\n\n    @Bean\n    public ProducerFactory<String, SensorReading> sensorProducerFactory() {\n        Map<String, Object> config = new HashMap<>();\n        config.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");\n        config.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n        config.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, JsonSerializer.class);\n\n        // Optimization for IoT high-throughput scenarios\n        config.put(ProducerConfig.BATCH_SIZE_CONFIG, 16384);\n        config.put(ProducerConfig.LINGER_MS_CONFIG, 10);\n        config.put(ProducerConfig.COMPRESSION_TYPE_CONFIG, "snappy");\n        config.put(ProducerConfig.ACKS_CONFIG, "1"); // Leader acknowledgment only\n\n        return new DefaultKafkaProducerFactory<>(config);\n    }\n}\n'})}),"\n",(0,r.jsx)(n.h4,{id:"spark-batch-processing-daily-analytics",children:"Spark Batch Processing (Daily Analytics)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-java",children:'@Service\n@Slf4j\npublic class SensorAnalyticsService {\n\n    @Autowired\n    private SparkSession spark;\n\n    /**\n     * Daily aggregation job for sensor data\n     */\n    public void runDailyAggregation(String date) {\n        log.info("Running daily aggregation for date: {}", date);\n\n        // Read from Kafka (batch mode - all messages for the day)\n        Dataset<Row> sensorData = spark.read()\n            .format("kafka")\n            .option("kafka.bootstrap.servers", "localhost:9092")\n            .option("subscribe", "sensor-data")\n            .option("startingOffsets", getStartOffset(date))\n            .option("endingOffsets", getEndOffset(date))\n            .load()\n            .selectExpr("CAST(key AS STRING) as sensorId", "CAST(value AS STRING) as json")\n            .select(col("sensorId"), from_json(col("json"), getSensorSchema()).as("data"))\n            .select("sensorId", "data.*");\n\n        // Calculate daily statistics\n        Dataset<Row> dailyStats = sensorData\n            .groupBy("sensorId", "sensorType")\n            .agg(\n                count("*").as("reading_count"),\n                avg("value").as("avg_value"),\n                min("value").as("min_value"),\n                max("value").as("max_value"),\n                stddev("value").as("stddev_value"),\n                percentile_approx(col("value"), lit(0.5)).as("median_value"),\n                percentile_approx(col("value"), lit(0.95)).as("p95_value"),\n                percentile_approx(col("value"), lit(0.99)).as("p99_value")\n            )\n            .withColumn("date", lit(date));\n\n        // Detect anomalies using statistical methods\n        Dataset<Row> anomalies = sensorData\n            .join(dailyStats, Seq("sensorId", "sensorType"))\n            .withColumn("z_score",\n                abs(col("value").minus(col("avg_value"))).divide(col("stddev_value")))\n            .filter(col("z_score").gt(3.0)) // More than 3 standard deviations\n            .select(\n                col("sensorId"),\n                col("timestamp"),\n                col("value"),\n                col("z_score"),\n                lit("STATISTICAL_ANOMALY").as("anomaly_type")\n            );\n\n        // Hourly patterns\n        Dataset<Row> hourlyPatterns = sensorData\n            .withColumn("hour", hour(from_unixtime(col("timestamp").divide(1000))))\n            .groupBy("sensorId", "sensorType", "hour")\n            .agg(\n                avg("value").as("hourly_avg"),\n                count("*").as("hourly_count")\n            )\n            .withColumn("date", lit(date));\n\n        // Write results to data lake\n        dailyStats\n            .write()\n            .mode(SaveMode.Overwrite)\n            .partitionBy("date")\n            .parquet("s3://data-lake/sensor-stats/");\n\n        anomalies\n            .write()\n            .mode(SaveMode.Append)\n            .partitionBy("date")\n            .parquet("s3://data-lake/anomalies/");\n\n        hourlyPatterns\n            .write()\n            .mode(SaveMode.Overwrite)\n            .partitionBy("date")\n            .parquet("s3://data-lake/hourly-patterns/");\n\n        log.info("Daily aggregation completed. Processed {} records", sensorData.count());\n    }\n\n    /**\n     * Predictive maintenance model training\n     */\n    public void trainPredictiveMaintenanceModel() {\n        log.info("Training predictive maintenance model");\n\n        // Load historical sensor data and failure events\n        Dataset<Row> sensorHistory = spark.read()\n            .parquet("s3://data-lake/sensor-stats/");\n\n        Dataset<Row> failureEvents = spark.read()\n            .parquet("s3://data-lake/failure-events/");\n\n        // Create time window features (7 days before failure)\n        WindowSpec window7d = Window\n            .partitionBy("sensorId")\n            .orderBy("date")\n            .rowsBetween(-6, 0);\n\n        Dataset<Row> features = sensorHistory\n            .withColumn("avg_7d", avg("avg_value").over(window7d))\n            .withColumn("max_7d", max("max_value").over(window7d))\n            .withColumn("stddev_7d", avg("stddev_value").over(window7d))\n            .withColumn("trend_7d",\n                col("avg_value").minus(lag("avg_value", 7).over(window7d)))\n            .filter(col("avg_7d").isNotNull());\n\n        // Join with failure events (label data)\n        Dataset<Row> labeledData = features\n            .join(failureEvents,\n                features.col("sensorId").equalTo(failureEvents.col("sensorId"))\n                .and(features.col("date").between(\n                    date_sub(failureEvents.col("failure_date"), 7),\n                    failureEvents.col("failure_date")\n                )),\n                "left"\n            )\n            .withColumn("label", when(col("failure_date").isNotNull(), 1.0).otherwise(0.0))\n            .select("sensorId", "date", "avg_7d", "max_7d", "stddev_7d", "trend_7d", "label");\n\n        // Prepare features vector\n        VectorAssembler assembler = new VectorAssembler()\n            .setInputCols(new String[]{"avg_7d", "max_7d", "stddev_7d", "trend_7d"})\n            .setOutputCol("features");\n\n        Dataset<Row> assembledData = assembler.transform(labeledData);\n\n        // Split data\n        Dataset<Row>[] splits = assembledData.randomSplit(new double[]{0.8, 0.2}, 42);\n        Dataset<Row> trainingData = splits[0];\n        Dataset<Row> testData = splits[1];\n\n        // Train Random Forest model\n        RandomForestClassifier rf = new RandomForestClassifier()\n            .setLabelCol("label")\n            .setFeaturesCol("features")\n            .setNumTrees(100)\n            .setMaxDepth(10)\n            .setFeatureSubsetStrategy("sqrt");\n\n        RandomForestClassificationModel model = rf.fit(trainingData);\n\n        // Evaluate model\n        Dataset<Row> predictions = model.transform(testData);\n\n        BinaryClassificationEvaluator evaluator = new BinaryClassificationEvaluator()\n            .setLabelCol("label")\n            .setRawPredictionCol("rawPrediction")\n            .setMetricName("areaUnderROC");\n\n        double auc = evaluator.evaluate(predictions);\n        log.info("Model AUC: {}", auc);\n\n        // Save model\n        model.write().overwrite().save("s3://models/predictive-maintenance/latest");\n\n        // Feature importance\n        double[] importances = model.featureImportances().toArray();\n        String[] featureNames = new String[]{"avg_7d", "max_7d", "stddev_7d", "trend_7d"};\n\n        log.info("Feature Importances:");\n        for (int i = 0; i < featureNames.length; i++) {\n            log.info("  {}: {}", featureNames[i], importances[i]);\n        }\n    }\n\n    private String getStartOffset(String date) {\n        // Calculate Kafka offset for start of day\n        return "earliest"; // Simplified for example\n    }\n\n    private String getEndOffset(String date) {\n        // Calculate Kafka offset for end of day\n        return "latest"; // Simplified for example\n    }\n\n    private StructType getSensorSchema() {\n        return new StructType()\n            .add("sensorId", DataTypes.StringType)\n            .add("sensorType", DataTypes.StringType)\n            .add("value", DataTypes.DoubleType)\n            .add("unit", DataTypes.StringType)\n            .add("timestamp", DataTypes.LongType)\n            .add("location", new StructType()\n                .add("latitude", DataTypes.DoubleType)\n                .add("longitude", DataTypes.DoubleType));\n    }\n}\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"pattern-4-real-time-recommendation-system",children:"Pattern 4: Real-time Recommendation System"}),"\n",(0,r.jsx)(n.h4,{id:"architecture-2",children:"Architecture"}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    subgraph "User Interaction"\n        Web[Web Application]\n        Mobile[Mobile App]\n    end\n\n    subgraph "API Layer"\n        API[Recommendation API]\n    end\n\n    subgraph "Event Collection"\n        K1[Kafka: user-events]\n    end\n\n    subgraph "Real-time Processing"\n        F[Flink: Session Analysis]\n        Redis[(Redis: User Context)]\n    end\n\n    subgraph "Feature Store"\n        FS[(Feature Store)]\n    end\n\n    subgraph "Serving"\n        MS[Model Server]\n    end\n\n    subgraph "Offline Training"\n        K2[Kafka: training-events]\n        SP[Spark: Feature Engineering]\n        ML[Spark ML: Model Training]\n        S3[(S3: Models)]\n    end\n\n    Web --\x3e API\n    Mobile --\x3e API\n\n    API --\x3e K1\n    API --\x3e MS\n\n    K1 --\x3e F\n    F --\x3e Redis\n    F --\x3e K2\n\n    MS --\x3e FS\n    MS --\x3e Redis\n\n    K2 --\x3e SP\n    SP --\x3e ML\n    ML --\x3e S3\n    S3 --\x3e MS'}),"\n",(0,r.jsx)(n.h4,{id:"flink-real-time-feature-computation",children:"Flink Real-time Feature Computation"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-java",children:'public class RealtimeFeatureJob {\n\n    public static void main(String[] args) throws Exception {\n        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n        env.enableCheckpointing(60000);\n\n        // Read user events\n        KafkaSource<UserEvent> source = KafkaSource.<UserEvent>builder()\n            .setBootstrapServers("localhost:9092")\n            .setTopics("user-events")\n            .setGroupId("realtime-features")\n            .setStartingOffsets(OffsetsInitializer.latest())\n            .setValueOnlyDeserializer(new UserEventDeserializer())\n            .build();\n\n        DataStream<UserEvent> events = env\n            .fromSource(source, WatermarkStrategy\n                .<UserEvent>forBoundedOutOfOrderness(Duration.ofSeconds(10))\n                .withTimestampAssigner((event, ts) -> event.getTimestamp()),\n                "Kafka Source");\n\n        // Compute real-time features per user\n        DataStream<UserFeatures> userFeatures = events\n            .keyBy(UserEvent::getUserId)\n            .window(SlidingEventTimeWindows.of(Time.minutes(30), Time.minutes(5)))\n            .aggregate(new UserFeatureAggregator());\n\n        // Compute item popularity (trending items)\n        DataStream<ItemPopularity> itemPopularity = events\n            .filter(e -> "VIEW".equals(e.getEventType()) || "CLICK".equals(e.getEventType()))\n            .keyBy(UserEvent::getItemId)\n            .window(TumblingEventTimeWindows.of(Time.minutes(10)))\n            .aggregate(new ItemPopularityAggregator());\n\n        // User session features\n        DataStream<SessionFeatures> sessionFeatures = events\n            .keyBy(UserEvent::getUserId)\n            .window(SessionWindows.withGap(Time.minutes(30)))\n            .process(new SessionFeatureFunction());\n\n        // Write features to Redis for serving\n        userFeatures.addSink(new RedisSink<>(\n            new FlinkJedisPoolConfig.Builder()\n                .setHost("localhost")\n                .setPort(6379)\n                .build(),\n            new UserFeatureRedisMapper()\n        ));\n\n        itemPopularity.addSink(new RedisSink<>(\n            new FlinkJedisPoolConfig.Builder()\n                .setHost("localhost")\n                .setPort(6379)\n                .build(),\n            new ItemPopularityRedisMapper()\n        ));\n\n        // Send to Kafka for offline training\n        userFeatures\n            .sinkTo(KafkaSink.<UserFeatures>builder()\n                .setBootstrapServers("localhost:9092")\n                .setRecordSerializer(KafkaRecordSerializationSchema.builder()\n                    .setTopic("training-events")\n                    .setValueSerializationSchema(new UserFeatureSerializer())\n                    .build())\n                .build());\n\n        env.execute("Realtime Feature Job");\n    }\n}\n\n// Aggregate user behavior in sliding windows\nclass UserFeatureAggregator implements AggregateFunction<UserEvent, UserFeatureAccumulator, UserFeatures> {\n\n    @Override\n    public UserFeatureAccumulator createAccumulator() {\n        return new UserFeatureAccumulator();\n    }\n\n    @Override\n    public UserFeatureAccumulator add(UserEvent event, UserFeatureAccumulator acc) {\n        acc.userId = event.getUserId();\n\n        switch (event.getEventType()) {\n            case "VIEW":\n                acc.viewCount++;\n                acc.viewedItems.add(event.getItemId());\n                break;\n            case "CLICK":\n                acc.clickCount++;\n                acc.clickedItems.add(event.getItemId());\n                break;\n            case "ADD_TO_CART":\n                acc.cartAddCount++;\n                break;\n            case "PURCHASE":\n                acc.purchaseCount++;\n                acc.totalSpent += event.getAmount();\n                break;\n        }\n\n        acc.categories.add(event.getCategory());\n        acc.lastEventTime = Math.max(acc.lastEventTime, event.getTimestamp());\n\n        return acc;\n    }\n\n    @Override\n    public UserFeatures getResult(UserFeatureAccumulator acc) {\n        return UserFeatures.builder()\n            .userId(acc.userId)\n            .viewCount(acc.viewCount)\n            .clickCount(acc.clickCount)\n            .cartAddCount(acc.cartAddCount)\n            .purchaseCount(acc.purchaseCount)\n            .uniqueItemsViewed(acc.viewedItems.size())\n            .uniqueItemsClicked(acc.clickedItems.size())\n            .uniqueCategories(acc.categories.size())\n            .totalSpent(acc.totalSpent)\n            .clickThroughRate(acc.viewCount > 0 ? (double) acc.clickCount / acc.viewCount : 0)\n            .conversionRate(acc.clickCount > 0 ? (double) acc.purchaseCount / acc.clickCount : 0)\n            .avgOrderValue(acc.purchaseCount > 0 ? acc.totalSpent / acc.purchaseCount : 0)\n            .timestamp(acc.lastEventTime)\n            .build();\n    }\n\n    @Override\n    public UserFeatureAccumulator merge(UserFeatureAccumulator a, UserFeatureAccumulator b) {\n        a.viewCount += b.viewCount;\n        a.clickCount += b.clickCount;\n        a.cartAddCount += b.cartAddCount;\n        a.purchaseCount += b.purchaseCount;\n        a.totalSpent += b.totalSpent;\n        a.viewedItems.addAll(b.viewedItems);\n        a.clickedItems.addAll(b.clickedItems);\n        a.categories.addAll(b.categories);\n        a.lastEventTime = Math.max(a.lastEventTime, b.lastEventTime);\n        return a;\n    }\n}\n\n// Session analysis\nclass SessionFeatureFunction extends ProcessWindowFunction<UserEvent, SessionFeatures, String, TimeWindow> {\n\n    @Override\n    public void process(\n        String userId,\n        Context context,\n        Iterable<UserEvent> events,\n        Collector<SessionFeatures> out\n    ) {\n        List<UserEvent> eventList = StreamSupport\n            .stream(events.spliterator(), false)\n            .sorted(Comparator.comparingLong(UserEvent::getTimestamp))\n            .collect(Collectors.toList());\n\n        if (eventList.isEmpty()) return;\n\n        long sessionStart = eventList.get(0).getTimestamp();\n        long sessionEnd = eventList.get(eventList.size() - 1).getTimestamp();\n        long durationSeconds = (sessionEnd - sessionStart) / 1000;\n\n        Set<String> viewedItems = new HashSet<>();\n        Set<String> categories = new HashSet<>();\n        int bounceIndicator = eventList.size() == 1 ? 1 : 0;\n\n        for (UserEvent event : eventList) {\n            if ("VIEW".equals(event.getEventType())) {\n                viewedItems.add(event.getItemId());\n            }\n            categories.add(event.getCategory());\n        }\n\n        boolean hasConversion = eventList.stream()\n            .anyMatch(e -> "PURCHASE".equals(e.getEventType()));\n\n        out.collect(SessionFeatures.builder()\n            .userId(userId)\n            .sessionStart(sessionStart)\n            .sessionEnd(sessionEnd)\n            .durationSeconds(durationSeconds)\n            .eventCount(eventList.size())\n            .uniqueItemsViewed(viewedItems.size())\n            .uniqueCategories(categories.size())\n            .bounced(bounceIndicator)\n            .converted(hasConversion ? 1 : 0)\n            .build());\n    }\n}\n'})}),"\n",(0,r.jsx)(n.h4,{id:"spring-boot-recommendation-api",children:"Spring Boot Recommendation API"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-java",children:'@RestController\n@RequestMapping("/api/recommendations")\n@Slf4j\npublic class RecommendationController {\n\n    @Autowired\n    private RecommendationService recommendationService;\n\n    @Autowired\n    private KafkaTemplate<String, UserEvent> kafkaTemplate;\n\n    @Autowired\n    private RedisTemplate<String, UserFeatures> redisTemplate;\n\n    @GetMapping("/{userId}")\n    public ResponseEntity<RecommendationResponse> getRecommendations(\n        @PathVariable String userId,\n        @RequestParam(defaultValue = "10") int limit\n    ) {\n        log.info("Getting recommendations for user: {}", userId);\n\n        // Get real-time features from Redis\n        String featureKey = "user:features:" + userId;\n        UserFeatures realtimeFeatures = redisTemplate.opsForValue().get(featureKey);\n\n        if (realtimeFeatures == null) {\n            realtimeFeatures = UserFeatures.builder()\n                .userId(userId)\n                .viewCount(0)\n                .clickCount(0)\n                .build();\n        }\n\n        // Get recommendations from model server\n        List<ItemRecommendation> recommendations = recommendationService\n            .getRecommendations(userId, realtimeFeatures, limit);\n\n        // Track this request as an event\n        UserEvent event = UserEvent.builder()\n            .userId(userId)\n            .eventType("RECOMMENDATION_REQUEST")\n            .timestamp(System.currentTimeMillis())\n            .metadata(Map.of("count", limit))\n            .build();\n\n        kafkaTemplate.send("user-events", userId, event);\n\n        return ResponseEntity.ok(RecommendationResponse.builder()\n            .userId(userId)\n            .recommendations(recommendations)\n            .generatedAt(Instant.now())\n            .build());\n    }\n\n    @PostMapping("/events")\n    public ResponseEntity<Void> trackEvent(@RequestBody UserEvent event) {\n        // Validate and enrich event\n        event.setTimestamp(System.currentTimeMillis());\n\n        // Send to Kafka for real-time processing\n        kafkaTemplate.send("user-events", event.getUserId(), event);\n\n        log.debug("Tracked event: {} for user: {}", event.getEventType(), event.getUserId());\n\n        return ResponseEntity.accepted().build();\n    }\n}\n\n@Service\n@Slf4j\npublic class RecommendationService {\n\n    @Autowired\n    private RestTemplate restTemplate;\n\n    @Autowired\n    private RedisTemplate<String, ItemPopularity> redisTemplate;\n\n    @Value("${model.server.url}")\n    private String modelServerUrl;\n\n    public List<ItemRecommendation> getRecommendations(\n        String userId,\n        UserFeatures features,\n        int limit\n    ) {\n        try {\n            // Call ML model server\n            ModelRequest request = ModelRequest.builder()\n                .userId(userId)\n                .features(features)\n                .limit(limit)\n                .build();\n\n            ResponseEntity<ModelResponse> response = restTemplate.postForEntity(\n                modelServerUrl + "/predict",\n                request,\n                ModelResponse.class\n            );\n\n            if (response.getStatusCode() == HttpStatus.OK) {\n                return response.getBody().getRecommendations();\n            }\n        } catch (Exception e) {\n            log.error("Error calling model server", e);\n        }\n\n        // Fallback: return trending items\n        return getTrendingItems(limit);\n    }\n\n    private List<ItemRecommendation> getTrendingItems(int limit) {\n        // Get trending items from Redis\n        Set<String> trendingKeys = redisTemplate.keys("item:popularity:*");\n\n        List<ItemPopularity> trending = trendingKeys.stream()\n            .map(key -> redisTemplate.opsForValue().get(key))\n            .filter(Objects::nonNull)\n            .sorted(Comparator.comparingInt(ItemPopularity::getScore).reversed())\n            .limit(limit)\n            .collect(Collectors.toList());\n\n        return trending.stream()\n            .map(item -> ItemRecommendation.builder()\n                .itemId(item.getItemId())\n                .score(item.getScore())\n                .reason("Trending")\n                .build())\n            .collect(Collectors.toList());\n    }\n}\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"best-practices--design-considerations",children:"Best Practices & Design Considerations"}),"\n",(0,r.jsx)(n.h3,{id:"kafka-best-practices",children:"Kafka Best Practices"}),"\n",(0,r.jsx)(n.h4,{id:"1-topic-design",children:"1. Topic Design"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-java",children:"// Good: Specific, well-partitioned topics\nkafka.topics:\n  - orders.created (10 partitions)\n  - orders.updated (10 partitions)\n  - payments.completed (5 partitions)\n\n// Bad: Single catch-all topic\nkafka.topics:\n  - all.events (100 partitions) // Hard to manage\n"})}),"\n",(0,r.jsx)(n.h4,{id:"2-partition-strategy",children:"2. Partition Strategy"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-java",children:'@Service\npublic class OrderEventProducer {\n\n    // Good: Partition by customer ID for ordering guarantees\n    public void publishOrder(Order order) {\n        kafkaTemplate.send(\n            "orders",\n            order.getCustomerId(), // Key ensures same partition\n            order\n        );\n    }\n\n    // Bad: Random partitioning loses ordering\n    public void publishOrderBad(Order order) {\n        kafkaTemplate.send("orders", order); // No key\n    }\n}\n'})}),"\n",(0,r.jsx)(n.h4,{id:"3-consumer-group-strategy",children:"3. Consumer Group Strategy"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-java",children:'// Good: One consumer group per use case\n@KafkaListener(topics = "orders", groupId = "inventory-service")\npublic void inventoryConsumer(Order order) { }\n\n@KafkaListener(topics = "orders", groupId = "analytics-service")\npublic void analyticsConsumer(Order order) { }\n\n// Each group gets ALL messages independently\n'})}),"\n",(0,r.jsx)(n.h4,{id:"4-error-handling",children:"4. Error Handling"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-java",children:'@Configuration\npublic class KafkaErrorConfig {\n\n    @Bean\n    public ConcurrentKafkaListenerContainerFactory<String, Order> kafkaListenerContainerFactory() {\n        ConcurrentKafkaListenerContainerFactory<String, Order> factory =\n            new ConcurrentKafkaListenerContainerFactory<>();\n\n        // Configure error handler\n        factory.setCommonErrorHandler(new DefaultErrorHandler(\n            new DeadLetterPublishingRecoverer(kafkaTemplate(),\n                (record, ex) -> {\n                    // Send to DLQ after 3 retries\n                    return new TopicPartition("orders.dlq", record.partition());\n                }),\n            new FixedBackOff(1000L, 3L) // 3 retries with 1 second delay\n        ));\n\n        return factory;\n    }\n}\n'})}),"\n",(0,r.jsx)(n.h3,{id:"flink-best-practices",children:"Flink Best Practices"}),"\n",(0,r.jsx)(n.h4,{id:"1-checkpointing-configuration",children:"1. Checkpointing Configuration"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-java",children:'StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\n// Enable checkpointing\nenv.enableCheckpointing(60000); // Every 60 seconds\n\n// Configure checkpoint behavior\nenv.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);\nenv.getCheckpointConfig().setMinPauseBetweenCheckpoints(30000);\nenv.getCheckpointConfig().setCheckpointTimeout(180000);\nenv.getCheckpointConfig().setMaxConcurrentCheckpoints(1);\n\n// Retain checkpoints on cancellation\nenv.getCheckpointConfig().setExternalizedCheckpointCleanup(\n    ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION\n);\n\n// Configure state backend\nenv.setStateBackend(new RocksDBStateBackend("hdfs:///flink/checkpoints"));\n'})}),"\n",(0,r.jsx)(n.h4,{id:"2-watermark-strategy",children:"2. Watermark Strategy"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-java",children:'// Good: Account for late data\nWatermarkStrategy<Event> strategy = WatermarkStrategy\n    .<Event>forBoundedOutOfOrderness(Duration.ofSeconds(10))\n    .withTimestampAssigner((event, timestamp) -> event.getEventTime())\n    .withIdleness(Duration.ofMinutes(1)); // Handle idle partitions\n\n// Apply to source\nDataStream<Event> stream = env\n    .fromSource(kafkaSource, strategy, "Kafka Source");\n'})}),"\n",(0,r.jsx)(n.h4,{id:"3-state-management",children:"3. State Management"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-java",children:'public class StatefulFunction extends KeyedProcessFunction<String, Event, Result> {\n\n    // Use ValueState for single values\n    private ValueState<Long> countState;\n\n    // Use MapState for key-value pairs\n    private MapState<String, Integer> itemCountState;\n\n    // Use ListState for lists\n    private ListState<Event> recentEventsState;\n\n    @Override\n    public void open(Configuration parameters) {\n        // Configure state with TTL to prevent unbounded growth\n        StateTtlConfig ttlConfig = StateTtlConfig\n            .newBuilder(Time.days(7))\n            .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite)\n            .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired)\n            .build();\n\n        ValueStateDescriptor<Long> countDescriptor = new ValueStateDescriptor<>(\n            "count",\n            Long.class\n        );\n        countDescriptor.enableTimeToLive(ttlConfig);\n\n        countState = getRuntimeContext().getState(countDescriptor);\n    }\n}\n'})}),"\n",(0,r.jsx)(n.h3,{id:"spark-best-practices",children:"Spark Best Practices"}),"\n",(0,r.jsx)(n.h4,{id:"1-memory-configuration",children:"1. Memory Configuration"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-java",children:'SparkSession spark = SparkSession.builder()\n    .appName("ETL Job")\n    .config("spark.executor.memory", "4g")\n    .config("spark.executor.cores", "2")\n    .config("spark.driver.memory", "2g")\n\n    // Optimize memory usage\n    .config("spark.memory.fraction", "0.8")\n    .config("spark.memory.storageFraction", "0.3")\n\n    // Enable adaptive query execution\n    .config("spark.sql.adaptive.enabled", "true")\n    .config("spark.sql.adaptive.coalescePartitions.enabled", "true")\n\n    // Configure shuffle\n    .config("spark.sql.shuffle.partitions", "200")\n    .config("spark.shuffle.compress", "true")\n\n    .getOrCreate();\n'})}),"\n",(0,r.jsx)(n.h4,{id:"2-data-partitioning",children:"2. Data Partitioning"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-java",children:'// Good: Partition by frequently queried columns\ndf.write()\n    .partitionBy("date", "country")\n    .mode(SaveMode.Overwrite)\n    .parquet("s3://bucket/data/");\n\n// Read with partition pruning\nDataset<Row> filtered = spark.read()\n    .parquet("s3://bucket/data/")\n    .filter("date = \'2024-01-01\' AND country = \'US\'");\n// Only reads relevant partitions!\n'})}),"\n",(0,r.jsx)(n.h4,{id:"3-caching-strategy",children:"3. Caching Strategy"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-java",children:'// Good: Cache frequently used datasets\nDataset<Row> users = spark.read().parquet("s3://users/");\nusers.cache(); // or .persist(StorageLevel.MEMORY_AND_DISK())\n\n// Use it multiple times\nDataset<Row> activeUsers = users.filter("last_login > \'2024-01-01\'");\nDataset<Row> premiumUsers = users.filter("subscription = \'premium\'");\n\n// Don\'t forget to unpersist when done\nusers.unpersist();\n'})}),"\n",(0,r.jsx)(n.h4,{id:"4-broadcast-joins",children:"4. Broadcast Joins"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-java",children:'// Good: Broadcast small datasets for efficient joins\nDataset<Row> largeDF = spark.read().parquet("s3://large-data/");\nDataset<Row> smallDF = spark.read().parquet("s3://small-lookup/");\n\n// Broadcast the small dataset\nDataset<Row> result = largeDF.join(\n    broadcast(smallDF),\n    largeDF.col("id").equalTo(smallDF.col("id"))\n);\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"monitoring--operations",children:"Monitoring & Operations"}),"\n",(0,r.jsx)(n.h3,{id:"kafka-monitoring",children:"Kafka Monitoring"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-java",children:'@Component\n@Slf4j\npublic class KafkaMonitor {\n\n    @Autowired\n    private AdminClient adminClient;\n\n    @Scheduled(fixedDelay = 60000)\n    public void monitorConsumerLag() {\n        try {\n            // Get all consumer groups\n            ListConsumerGroupsResult groups = adminClient.listConsumerGroups();\n\n            for (ConsumerGroupListing group : groups.all().get()) {\n                String groupId = group.groupId();\n\n                // Get lag for each partition\n                Map<TopicPartition, OffsetAndMetadata> offsets = adminClient\n                    .listConsumerGroupOffsets(groupId)\n                    .partitionsToOffsetAndMetadata()\n                    .get();\n\n                for (Map.Entry<TopicPartition, OffsetAndMetadata> entry : offsets.entrySet()) {\n                    TopicPartition partition = entry.getKey();\n                    long currentOffset = entry.getValue().offset();\n\n                    // Get end offset (latest)\n                    long endOffset = getEndOffset(partition);\n                    long lag = endOffset - currentOffset;\n\n                    if (lag > 10000) {\n                        log.warn("High lag detected: Group={}, Topic={}, Partition={}, Lag={}",\n                            groupId, partition.topic(), partition.partition(), lag);\n                    }\n                }\n            }\n        } catch (Exception e) {\n            log.error("Error monitoring Kafka", e);\n        }\n    }\n\n    private long getEndOffset(TopicPartition partition) {\n        // Implementation to get end offset\n        return 0L;\n    }\n}\n'})}),"\n",(0,r.jsx)(n.h3,{id:"flink-monitoring",children:"Flink Monitoring"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-java",children:'@Service\n@Slf4j\npublic class FlinkJobMonitor {\n\n    @Value("${flink.jobmanager.url}")\n    private String jobManagerUrl;\n\n    @Scheduled(fixedDelay = 30000)\n    public void monitorJobHealth() {\n        RestTemplate restTemplate = new RestTemplate();\n\n        try {\n            // Get job overview\n            String url = jobManagerUrl + "/jobs/overview";\n            ResponseEntity<JobOverview> response = restTemplate.getForEntity(\n                url, JobOverview.class\n            );\n\n            JobOverview overview = response.getBody();\n\n            for (Job job : overview.getJobs()) {\n                if ("FAILED".equals(job.getState())) {\n                    log.error("Flink job failed: {}", job.getId());\n                    // Send alert\n                }\n\n                // Check checkpoint success rate\n                String checkpointUrl = jobManagerUrl + "/jobs/" + job.getId() + "/checkpoints";\n                ResponseEntity<CheckpointStats> checkpointResponse = restTemplate.getForEntity(\n                    checkpointUrl, CheckpointStats.class\n                );\n\n                CheckpointStats stats = checkpointResponse.getBody();\n                double successRate = (double) stats.getNumCompletedCheckpoints() /\n                    stats.getNumCheckpoints();\n\n                if (successRate < 0.95) {\n                    log.warn("Low checkpoint success rate for job {}: {}%",\n                        job.getId(), successRate * 100);\n                }\n            }\n        } catch (Exception e) {\n            log.error("Error monitoring Flink jobs", e);\n        }\n    }\n}\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,r.jsx)(n.h3,{id:"quick-decision-guide",children:"Quick Decision Guide"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Use Kafka when:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"You need a message broker"}),"\n",(0,r.jsx)(n.li,{children:"Building event-driven architecture"}),"\n",(0,r.jsx)(n.li,{children:"Require message persistence and replay"}),"\n",(0,r.jsx)(n.li,{children:"Need to decouple services"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Use Flink when:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Need millisecond latency"}),"\n",(0,r.jsx)(n.li,{children:"Complex event processing required"}),"\n",(0,r.jsx)(n.li,{children:"Stateful stream processing"}),"\n",(0,r.jsx)(n.li,{children:"Exactly-once guarantees critical"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Use Spark when:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Large-scale batch processing"}),"\n",(0,r.jsx)(n.li,{children:"Building ETL pipelines"}),"\n",(0,r.jsx)(n.li,{children:"Training ML models"}),"\n",(0,r.jsx)(n.li,{children:"Second-level latency acceptable for streaming"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"common-combinations",children:"Common Combinations"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Kafka + Flink"}),": Real-time analytics, fraud detection"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Kafka + Spark"}),": Batch ETL with streaming ingestion"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Kafka + Flink + Spark"}),": Lambda architecture (realtime + batch)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"All Three"}),": Complete data platform"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"further-reading",children:"Further Reading"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://kafka.apache.org/documentation/",children:"Kafka Documentation"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://flink.apache.org/docs/stable/",children:"Flink Documentation"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://spark.apache.org/docs/latest/",children:"Spark Documentation"})}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://dataintensive.net/",children:"Designing Data-Intensive Applications"})," by Martin Kleppmann"]}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}}}]);