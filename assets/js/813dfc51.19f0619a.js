"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[6784],{28453:(e,n,i)=>{i.d(n,{R:()=>l,x:()=>a});var t=i(96540);const s={},r=t.createContext(s);function l(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:l(e.components),t.createElement(r.Provider,{value:n},e.children)}},83504:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>h,frontMatter:()=>r,metadata:()=>a,toc:()=>o});var t=i(74848),s=i(28453);const r={},l="Rate Limiter System Design",a={id:"Backend System Design/Rate Limiter System Design",title:"Rate Limiter System Design",description:"Table of Contents",source:"@site/docs/Backend System Design/Rate Limiter System Design.mdx",sourceDirName:"Backend System Design",slug:"/Backend System Design/Rate Limiter System Design",permalink:"/js.enigma/docs/Backend System Design/Rate Limiter System Design",draft:!1,unlisted:!1,editUrl:"https://github.com/carefree-ladka/docs/Backend System Design/Rate Limiter System Design.mdx",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Object Storage",permalink:"/js.enigma/docs/Backend System Design/Object Storage"},next:{title:"Retry, Backoff, Jitter & Resilience Patterns",permalink:"/js.enigma/docs/Backend System Design/Retry, Backoff, Jitter & Resilience Patterns"}},c={},o=[{value:"Table of Contents",id:"table-of-contents",level:2},{value:"Requirements (~5 minutes)",id:"requirements-5-minutes",level:2},{value:"1) Functional Requirements",id:"1-functional-requirements",level:3},{value:"2) Non-functional Requirements",id:"2-non-functional-requirements",level:3},{value:"3) Capacity Estimation",id:"3-capacity-estimation",level:3},{value:"Core Entities (~2 minutes)",id:"core-entities-2-minutes",level:2},{value:"API or System Interface (~5 minutes)",id:"api-or-system-interface-5-minutes",level:2},{value:"Protocol Choice: gRPC",id:"protocol-choice-grpc",level:3},{value:"Core API Endpoints",id:"core-api-endpoints",level:3},{value:"Data Flow (~5 minutes)",id:"data-flow-5-minutes",level:2},{value:"Rate Limit Check Flow",id:"rate-limit-check-flow",level:3},{value:"Configuration Update Flow",id:"configuration-update-flow",level:3},{value:"Bucket State Synchronization Flow (Distributed)",id:"bucket-state-synchronization-flow-distributed",level:3},{value:"High Level Design (~10-15 minutes)",id:"high-level-design-10-15-minutes",level:2},{value:"Design Approach",id:"design-approach",level:3},{value:"System Architecture",id:"system-architecture",level:3},{value:"Detailed Component Design",id:"detailed-component-design",level:3},{value:"Algorithm Implementations",id:"algorithm-implementations",level:3},{value:"Database Schema",id:"database-schema",level:3},{value:"Technology Stack",id:"technology-stack",level:3},{value:"Deep Dives (~10 minutes)",id:"deep-dives-10-minutes",level:2},{value:"1. Algorithm Comparison and Selection",id:"1-algorithm-comparison-and-selection",level:3},{value:"2. Distributed State Management",id:"2-distributed-state-management",level:3},{value:"3. High Availability and Fault Tolerance",id:"3-high-availability-and-fault-tolerance",level:3},{value:"4. Performance Optimizations",id:"4-performance-optimizations",level:3},{value:"5. Monitoring and Observability",id:"5-monitoring-and-observability",level:3},{value:"6. Configuration Management",id:"6-configuration-management",level:3},{value:"Summary",id:"summary",level:2}];function d(e){const n={a:"a",blockquote:"blockquote",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"rate-limiter-system-design",children:"Rate Limiter System Design"})}),"\n",(0,t.jsx)(n.h2,{id:"table-of-contents",children:"Table of Contents"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"#requirements-5-minutes",children:"Requirements (~5 minutes)"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#1-functional-requirements",children:"Functional Requirements"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#2-non-functional-requirements",children:"Non-functional Requirements"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#3-capacity-estimation",children:"Capacity Estimation"})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#core-entities-2-minutes",children:"Core Entities (~2 minutes)"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#api-or-system-interface-5-minutes",children:"API or System Interface (~5 minutes)"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#data-flow-5-minutes",children:"Data Flow (~5 minutes)"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#high-level-design-10-15-minutes",children:"High Level Design (~10-15 minutes)"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#deep-dives-10-minutes",children:"Deep Dives (~10 minutes)"})}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"requirements-5-minutes",children:"Requirements (~5 minutes)"}),"\n",(0,t.jsx)(n.h3,{id:"1-functional-requirements",children:"1) Functional Requirements"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Key Questions Asked:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Q: What types of rate limiting do we need to support?"}),"\n",(0,t.jsx)(n.li,{children:"A: User-based, IP-based, and API key-based rate limiting"}),"\n",(0,t.jsx)(n.li,{children:"Q: Should it be a standalone service or library?"}),"\n",(0,t.jsx)(n.li,{children:"A: Standalone service that other services can call"}),"\n",(0,t.jsx)(n.li,{children:"Q: Do we need different rate limits for different endpoints?"}),"\n",(0,t.jsx)(n.li,{children:"A: Yes, different APIs may have different limits"}),"\n",(0,t.jsx)(n.li,{children:"Q: What happens when rate limit is exceeded?"}),"\n",(0,t.jsx)(n.li,{children:"A: Return HTTP 429 (Too Many Requests) with retry information"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Core Functional Requirements:"})}),"\n",(0,t.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","System should allow rate limiting by different identifiers (user_id, IP, API key)"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","System should support different rate limiting algorithms (fixed window, sliding window, token bucket)"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","System should allow configurable rate limits per API endpoint"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","System should return appropriate HTTP status codes and retry information when limits are exceeded"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","System should provide real-time rate limit status to clients"]}),"\n"]}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsxs)(n.p,{children:["\ud83d\udca1 ",(0,t.jsx)(n.strong,{children:"Tip:"})," These 5 requirements cover the essential functionality while keeping scope manageable."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"2-non-functional-requirements",children:"2) Non-functional Requirements"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"System Quality Requirements:"})}),"\n",(0,t.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,t.jsx)(n.strong,{children:"Ultra-Low Latency:"})," Rate limit checks should add < 1ms latency to API calls"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,t.jsx)(n.strong,{children:"High Availability:"})," 99.99% uptime - rate limiter failure shouldn't bring down services"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,t.jsx)(n.strong,{children:"High Throughput:"})," Handle 10M+ rate limit checks per second"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,t.jsx)(n.strong,{children:"Accuracy:"})," Rate limiting should be precise with minimal false positives/negatives"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,t.jsx)(n.strong,{children:"Scalability:"})," Horizontally scalable to support growing API traffic"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,t.jsx)(n.strong,{children:"Fault Tolerance:"})," Fail-open (allow requests) when rate limiter is unavailable"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Rationale:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Ultra-low latency:"})," Rate limiting is on the critical path of every API call"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"High availability:"})," More critical than accuracy - better to allow some requests than block all"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Fail-open:"})," Ensures service availability over perfect rate limiting"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"3-capacity-estimation",children:"3) Capacity Estimation"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Key Calculations That Influence Design:"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Throughput Requirements:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"10M rate limit checks/second at peak"}),"\n",(0,t.jsx)(n.li,{children:"Assuming 80/20 rule: 8M reads, 2M writes per second"}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.em,{children:"Impact:"})," Need in-memory caching and optimized data structures"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Memory Requirements per Algorithm:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Token Bucket: ~100 bytes per user (bucket state)\nSliding Window: ~1KB per user (request timestamps)\nFixed Window: ~50 bytes per user (counter + timestamp)\n\nFor 100M active users with token bucket:\n100M \xd7 100 bytes = 10GB memory per rate limiter node\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.em,{children:"Impact:"})," Memory-efficient algorithms and data partitioning required"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Network Overhead:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Each check: ~200 bytes request/response"}),"\n",(0,t.jsx)(n.li,{children:"10M checks/sec \xd7 200 bytes = 2GB/s network traffic"}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.em,{children:"Impact:"})," Need local caching and efficient serialization"]}),"\n"]}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsx)(n.p,{children:"These calculations drive our choice of algorithms, caching strategy, and node sizing."}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"core-entities-2-minutes",children:"Core Entities (~2 minutes)"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Primary Entities:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"RateLimit:"})," Configuration for rate limiting rules (endpoint, limit, window, algorithm)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"RateLimitBucket:"})," Current state of rate limiting for a specific identifier (counter, last_reset, tokens)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"RateLimitRequest:"})," Information needed to check rate limit (identifier, endpoint, timestamp)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"RateLimitResponse:"})," Result of rate limit check (allowed, remaining, reset_time)"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Entity Relationships:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["RateLimit defines rules (1",":N"," with RateLimitBucket)"]}),"\n",(0,t.jsx)(n.li,{children:"RateLimitBucket tracks state per identifier"}),"\n",(0,t.jsx)(n.li,{children:"RateLimitRequest triggers check against RateLimitBucket"}),"\n",(0,t.jsx)(n.li,{children:"RateLimitResponse provides decision and metadata"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Key Identifiers:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"user_id:"})," For user-specific rate limiting"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"ip_address:"})," For IP-based protection against abuse"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"api_key:"})," For client application rate limiting"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"endpoint:"})," For per-API rate limiting"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"api-or-system-interface-5-minutes",children:"API or System Interface (~5 minutes)"}),"\n",(0,t.jsx)(n.h3,{id:"protocol-choice-grpc",children:"Protocol Choice: gRPC"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Reasoning:"})," Ultra-low latency requirement makes gRPC ideal due to HTTP/2 multiplexing, binary protocol, and connection reuse. Also supports both synchronous and async calls."]}),"\n",(0,t.jsx)(n.h3,{id:"core-api-endpoints",children:"Core API Endpoints"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Rate Limit Check (Primary API):"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-protobuf",children:'service RateLimiter {\n  rpc CheckRateLimit(RateLimitRequest) returns (RateLimitResponse);\n  rpc CheckRateLimitBatch(BatchRateLimitRequest) returns (BatchRateLimitResponse);\n}\n\nmessage RateLimitRequest {\n  string identifier = 1;        // user_id, ip, api_key\n  string identifier_type = 2;   // "user", "ip", "api_key"\n  string endpoint = 3;          // "/api/v1/posts"\n  int32 tokens_requested = 4;   // default: 1\n  int64 timestamp = 5;          // current timestamp\n}\n\nmessage RateLimitResponse {\n  bool allowed = 1;             // whether request should be allowed\n  int32 remaining_tokens = 2;   // tokens left in current window\n  int64 reset_time = 3;         // when the limit resets (unix timestamp)\n  int32 retry_after_seconds = 4; // how long to wait before retry\n}\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Configuration Management:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-protobuf",children:'service RateLimiterConfig {\n  rpc CreateRateLimit(CreateRateLimitRequest) returns (RateLimit);\n  rpc UpdateRateLimit(UpdateRateLimitRequest) returns (RateLimit);\n  rpc DeleteRateLimit(DeleteRateLimitRequest) returns (Empty);\n  rpc ListRateLimits(ListRateLimitsRequest) returns (ListRateLimitsResponse);\n}\n\nmessage RateLimit {\n  string id = 1;\n  string endpoint = 2;          // "/api/v1/posts" or "*" for global\n  string identifier_type = 3;   // "user", "ip", "api_key"\n  int32 limit = 4;             // requests allowed per window\n  int32 window_seconds = 5;     // time window in seconds\n  string algorithm = 6;         // "token_bucket", "sliding_window", "fixed_window"\n  bool enabled = 7;\n}\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Monitoring & Health:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-protobuf",children:"service RateLimiterHealth {\n  rpc GetHealthStatus(Empty) returns (HealthResponse);\n  rpc GetMetrics(MetricsRequest) returns (MetricsResponse);\n  rpc ResetRateLimit(ResetRequest) returns (Empty);  // For testing/admin\n}\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Client Integration Example:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# How services would integrate\nrate_limiter = RateLimiterClient("rate-limiter.internal:9090")\n\ndef api_handler(user_id, request):\n    # Check rate limit before processing\n    check_request = RateLimitRequest(\n        identifier=user_id,\n        identifier_type="user",\n        endpoint="/api/v1/posts",\n        tokens_requested=1\n    )\n\n    response = rate_limiter.CheckRateLimit(check_request)\n\n    if not response.allowed:\n        return HTTP_429_TOO_MANY_REQUESTS, {\n            "error": "Rate limit exceeded",\n            "retry_after": response.retry_after_seconds\n        }\n\n    # Process actual API request\n    return handle_api_request(request)\n'})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"data-flow-5-minutes",children:"Data Flow (~5 minutes)"}),"\n",(0,t.jsx)(n.h3,{id:"rate-limit-check-flow",children:"Rate Limit Check Flow"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"API Request:"})," Client makes request to API service (e.g., POST /api/v1/posts)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Rate Limit Check:"})," API service calls Rate Limiter with (user_id, endpoint, timestamp)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cache Lookup:"})," Rate Limiter checks local cache for user's bucket state"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Algorithm Execution:"})," Apply rate limiting algorithm (token bucket/sliding window)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Decision:"})," Determine if request should be allowed based on current state"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"State Update:"})," Update bucket state (decrement tokens, update timestamps)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Response:"})," Return decision with remaining quota and reset time"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"API Processing:"})," API service either processes request or returns 429 error"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"configuration-update-flow",children:"Configuration Update Flow"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Config Change:"})," Admin updates rate limit via configuration API"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validation:"})," Validate new configuration (positive limits, valid algorithms)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Database Update:"})," Store new configuration in persistent storage"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cache Invalidation:"})," Invalidate relevant cached configurations"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Node Broadcast:"})," Notify all rate limiter nodes of configuration change"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Hot Reload:"})," Nodes reload configuration without restart"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"bucket-state-synchronization-flow-distributed",children:"Bucket State Synchronization Flow (Distributed)"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"State Update:"})," Local node updates bucket state"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Async Replication:"})," Broadcast state changes to other nodes (eventual consistency)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Conflict Resolution:"})," Use timestamp-based resolution for conflicts"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Periodic Sync:"})," Background job synchronizes states across nodes"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"high-level-design-10-15-minutes",children:"High Level Design (~10-15 minutes)"}),"\n",(0,t.jsx)(n.h3,{id:"design-approach",children:"Design Approach"}),"\n",(0,t.jsx)(n.p,{children:"Building the system to handle the primary use case first (single rate limit check), then expanding to support batch operations and distributed scenarios."}),"\n",(0,t.jsx)(n.h3,{id:"system-architecture",children:"System Architecture"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"[API Services] -> [gRPC] -> [Load Balancer] -> [Rate Limiter Cluster]\n                                                      |\n                                                [Rate Limiter Nodes]\n                                                      |\n                        +-----------------------------+-----------------------------+\n                        |                             |                             |\n                [Local Cache]                [Configuration Store]           [Metrics Store]\n                 (In-Memory)                   (PostgreSQL)                 (Prometheus)\n                        |                             |\n                [State Sync]                 [Admin Dashboard]\n                 (Redis/Gossip)\n"})}),"\n",(0,t.jsx)(n.h3,{id:"detailed-component-design",children:"Detailed Component Design"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Rate Limiter Node Architecture:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502               Rate Limiter Node             \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  gRPC Server (Port 9090)                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Algorithm Engine                           \u2502\n\u2502  \u251c\u2500 Token Bucket Implementation             \u2502\n\u2502  \u251c\u2500 Sliding Window Implementation           \u2502\n\u2502  \u2514\u2500 Fixed Window Implementation             \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Local Cache (In-Memory)                    \u2502\n\u2502  \u251c\u2500 Configuration Cache                     \u2502\n\u2502  \u251c\u2500 Bucket State Cache (LRU)               \u2502\n\u2502  \u2514\u2500 Hot Data (Last 5 min activity)         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  State Synchronization                      \u2502\n\u2502  \u251c\u2500 Redis Publisher/Subscriber              \u2502\n\u2502  \u2514\u2500 Background Sync Jobs                    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Metrics & Monitoring                       \u2502\n\u2502  \u251c\u2500 Request Counters                        \u2502\n\u2502  \u251c\u2500 Latency Histograms                      \u2502\n\u2502  \u2514\u2500 Health Checks                           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,t.jsx)(n.h3,{id:"algorithm-implementations",children:"Algorithm Implementations"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"1. Token Bucket Algorithm (Recommended):"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class TokenBucket:\n    def __init__(self, capacity, refill_rate):\n        self.capacity = capacity          # Maximum tokens\n        self.tokens = capacity           # Current tokens\n        self.refill_rate = refill_rate   # Tokens per second\n        self.last_refill = time.time()\n\n    def consume(self, tokens=1):\n        now = time.time()\n        # Add tokens based on time elapsed\n        time_elapsed = now - self.last_refill\n        self.tokens = min(self.capacity,\n                         self.tokens + time_elapsed * self.refill_rate)\n        self.last_refill = now\n\n        if self.tokens >= tokens:\n            self.tokens -= tokens\n            return True\n        return False\n\n    def get_retry_after(self):\n        if self.tokens >= 1:\n            return 0\n        return (1 - self.tokens) / self.refill_rate\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"2. Sliding Window Log:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class SlidingWindowLog:\n    def __init__(self, limit, window_seconds):\n        self.limit = limit\n        self.window_seconds = window_seconds\n        self.requests = []  # List of timestamps\n\n    def is_allowed(self, timestamp):\n        # Remove old requests outside window\n        cutoff = timestamp - self.window_seconds\n        self.requests = [req for req in self.requests if req > cutoff]\n\n        if len(self.requests) < self.limit:\n            self.requests.append(timestamp)\n            return True\n        return False\n"})}),"\n",(0,t.jsx)(n.h3,{id:"database-schema",children:"Database Schema"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Rate Limit Configurations:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:"CREATE TABLE rate_limits (\n    id UUID PRIMARY KEY,\n    endpoint VARCHAR(255) NOT NULL,\n    identifier_type VARCHAR(50) NOT NULL, -- 'user', 'ip', 'api_key'\n    limit_value INTEGER NOT NULL,\n    window_seconds INTEGER NOT NULL,\n    algorithm VARCHAR(50) NOT NULL, -- 'token_bucket', 'sliding_window', 'fixed_window'\n    enabled BOOLEAN DEFAULT true,\n    created_at TIMESTAMP DEFAULT NOW(),\n    updated_at TIMESTAMP DEFAULT NOW(),\n    UNIQUE(endpoint, identifier_type)\n);\n\n-- Index for fast lookups\nCREATE INDEX idx_rate_limits_endpoint_type ON rate_limits(endpoint, identifier_type);\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Rate Limit State (Redis/In-Memory):"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Redis key structure\nbucket_key = f"bucket:{identifier_type}:{identifier}:{endpoint}"\nbucket_data = {\n    "tokens": 10,              # Current tokens (token bucket)\n    "last_refill": 1640995200, # Last refill timestamp\n    "requests": [timestamps],   # Request log (sliding window)\n    "window_start": 1640995200, # Window start (fixed window)\n    "request_count": 5         # Requests in current window\n}\n'})}),"\n",(0,t.jsx)(n.h3,{id:"technology-stack",children:"Technology Stack"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Application:"})," Go/Java for high-performance gRPC services"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cache:"})," Redis for distributed state + local in-memory cache"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Database:"})," PostgreSQL for configuration storage"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Load Balancer:"})," Envoy proxy with gRPC load balancing"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Monitoring:"})," Prometheus + Grafana for metrics"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Service Mesh:"})," Istio for service-to-service communication"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"deep-dives-10-minutes",children:"Deep Dives (~10 minutes)"}),"\n",(0,t.jsx)(n.h3,{id:"1-algorithm-comparison-and-selection",children:"1. Algorithm Comparison and Selection"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Token Bucket (Recommended):"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Pros:"})," Allows burst traffic, memory efficient, smooth rate limiting"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cons:"})," Slightly more complex implementation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Use Case:"})," General purpose, handles traffic bursts well"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Memory:"})," ~100 bytes per bucket (tokens, last_refill, capacity)"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Sliding Window Log:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Pros:"})," Most accurate, prevents burst at window boundaries"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cons:"})," High memory usage, complex cleanup"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Use Case:"})," When precision is critical"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Memory:"})," ~1KB per bucket (timestamp array)"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Fixed Window Counter:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Pros:"})," Simple, memory efficient"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cons:"})," Allows 2x burst at window boundaries"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Use Case:"})," Simple rate limiting with acceptable burst"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Memory:"})," ~50 bytes per bucket (counter, window_start)"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Decision Matrix:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Requirement    | Token Bucket | Sliding Window | Fixed Window\n---------------|--------------|----------------|-------------\nMemory Usage   | Medium       | High           | Low\nAccuracy       | High         | Highest        | Medium\nBurst Handling | Excellent    | Good           | Poor\nImplementation | Medium       | Complex        | Simple\nPerformance    | Excellent    | Good           | Excellent\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Recommendation:"})," Token Bucket for general use, Sliding Window for critical APIs needing precision."]}),"\n",(0,t.jsx)(n.h3,{id:"2-distributed-state-management",children:"2. Distributed State Management"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Challenge:"})," Maintaining consistent rate limit state across multiple nodes while achieving < 1ms latency."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Solution: Multi-Tier Caching with Eventual Consistency"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Local Node Cache (L1):"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class LocalRateLimitCache:\n    def __init__(self, max_size=1000000):\n        self.cache = LRUCache(max_size)  # In-memory LRU cache\n        self.hit_ratio_target = 0.95     # 95% cache hit rate\n\n    def get_bucket(self, key):\n        bucket = self.cache.get(key)\n        if bucket is None:\n            # Cache miss - fetch from Redis\n            bucket = self.fetch_from_redis(key)\n            self.cache.set(key, bucket, ttl=300)  # 5-minute TTL\n        return bucket\n\n    def update_bucket(self, key, bucket):\n        self.cache.set(key, bucket)\n        # Async update to Redis for other nodes\n        self.async_update_redis(key, bucket)\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Redis Shared State (L2):"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class RedisStateManager:\n    def __init__(self):\n        self.redis_client = redis.Redis(host=\'redis-cluster\')\n        self.local_updates = Queue()  # Buffer for batch updates\n\n    def update_bucket_state(self, key, bucket_state):\n        # Use Redis pipelines for batched updates\n        pipeline = self.redis_client.pipeline()\n        pipeline.hset(key, "tokens", bucket_state.tokens)\n        pipeline.hset(key, "last_refill", bucket_state.last_refill)\n        pipeline.expire(key, 3600)  # 1-hour TTL\n        pipeline.execute()\n\n    def sync_states_batch(self):\n        # Background job to sync local changes to Redis\n        while not self.local_updates.empty():\n            updates = []\n            for _ in range(min(100, self.local_updates.size())):\n                updates.append(self.local_updates.get())\n            self.batch_update_redis(updates)\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Conflict Resolution Strategy:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"def resolve_bucket_conflict(local_bucket, redis_bucket):\n    # Use latest timestamp as source of truth\n    if local_bucket.last_refill > redis_bucket.last_refill:\n        return local_bucket\n    else:\n        # Merge states: take more restrictive values\n        return BucketState(\n            tokens=min(local_bucket.tokens, redis_bucket.tokens),\n            last_refill=redis_bucket.last_refill\n        )\n"})}),"\n",(0,t.jsx)(n.h3,{id:"3-high-availability-and-fault-tolerance",children:"3. High Availability and Fault Tolerance"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Fail-Open Strategy:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class RateLimiterService:\n    def __init__(self):\n        self.circuit_breaker = CircuitBreaker(\n            failure_threshold=5,\n            recovery_timeout=30,\n            expected_exception=RedisConnectionError\n        )\n\n    def check_rate_limit(self, request):\n        try:\n            with self.circuit_breaker:\n                return self._perform_rate_limit_check(request)\n        except CircuitBreakerOpenException:\n            # Circuit breaker open - fail open\n            self.metrics.increment("rate_limiter.fail_open")\n            return RateLimitResponse(\n                allowed=True,\n                remaining_tokens=1,\n                reset_time=time.time() + 60\n            )\n        except Exception as e:\n            # Unexpected error - log and fail open\n            self.logger.error(f"Rate limiter error: {e}")\n            return RateLimitResponse(allowed=True, remaining_tokens=1)\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Health Monitoring:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class HealthChecker:\n    def check_health(self):\n        health_status = {\n            "redis_connection": self.check_redis_connectivity(),\n            "local_cache_hit_ratio": self.get_cache_hit_ratio(),\n            "average_latency_ms": self.get_avg_latency(),\n            "error_rate": self.get_error_rate()\n        }\n\n        # Overall health based on critical metrics\n        if (health_status["redis_connection"] and\n            health_status["local_cache_hit_ratio"] > 0.90 and\n            health_status["average_latency_ms"] < 2.0 and\n            health_status["error_rate"] < 0.01):\n            return HealthStatus.HEALTHY\n        else:\n            return HealthStatus.DEGRADED\n'})}),"\n",(0,t.jsx)(n.h3,{id:"4-performance-optimizations",children:"4. Performance Optimizations"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Batched Rate Limit Checks:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def check_rate_limit_batch(self, requests):\n    """Check multiple rate limits in a single call"""\n    results = []\n\n    # Group requests by identifier for cache efficiency\n    grouped_requests = self.group_by_identifier(requests)\n\n    for identifier, reqs in grouped_requests.items():\n        bucket = self.get_bucket(identifier)\n        for req in reqs:\n            result = bucket.consume(req.tokens_requested)\n            results.append(result)\n\n    return results\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Connection Pooling and Multiplexing:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# gRPC connection pooling\nclass RateLimiterClient:\n    def __init__(self, server_addresses):\n        self.channels = [\n            grpc.insecure_channel(addr, options=[\n                ('grpc.keepalive_time_ms', 10000),\n                ('grpc.max_receive_message_length', 4 * 1024 * 1024),\n                ('grpc.max_send_message_length', 4 * 1024 * 1024),\n            ]) for addr in server_addresses\n        ]\n        self.current_channel = 0\n\n    def get_next_channel(self):\n        # Round-robin across channels\n        channel = self.channels[self.current_channel]\n        self.current_channel = (self.current_channel + 1) % len(self.channels)\n        return channel\n"})}),"\n",(0,t.jsx)(n.h3,{id:"5-monitoring-and-observability",children:"5. Monitoring and Observability"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Key Metrics:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class RateLimiterMetrics:\n    def __init__(self):\n        # Business metrics\n        self.requests_allowed = Counter('rate_limiter_requests_allowed_total')\n        self.requests_blocked = Counter('rate_limiter_requests_blocked_total')\n        self.false_positives = Counter('rate_limiter_false_positives_total')\n\n        # Performance metrics\n        self.check_latency = Histogram('rate_limiter_check_latency_seconds')\n        self.cache_hit_ratio = Gauge('rate_limiter_cache_hit_ratio')\n        self.active_buckets = Gauge('rate_limiter_active_buckets')\n\n        # System metrics\n        self.redis_connection_errors = Counter('rate_limiter_redis_errors_total')\n        self.memory_usage = Gauge('rate_limiter_memory_usage_bytes')\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Alerting Rules:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'# Prometheus alerting rules\ngroups:\n  - name: rate_limiter\n    rules:\n      - alert: RateLimiterHighLatency\n        expr: histogram_quantile(0.95, rate_limiter_check_latency_seconds) > 0.002\n        for: 5m\n        labels:\n          severity: warning\n        annotations:\n          summary: "Rate limiter latency is high"\n\n      - alert: RateLimiterCacheMissRate\n        expr: rate_limiter_cache_hit_ratio < 0.90\n        for: 10m\n        labels:\n          severity: warning\n        annotations:\n          summary: "Rate limiter cache hit rate is low"\n\n      - alert: RateLimiterRedisDown\n        expr: up{job="rate-limiter-redis"} == 0\n        for: 1m\n        labels:\n          severity: critical\n        annotations:\n          summary: "Rate limiter Redis is down"\n'})}),"\n",(0,t.jsx)(n.h3,{id:"6-configuration-management",children:"6. Configuration Management"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Dynamic Configuration Updates:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class ConfigurationManager:\n    def __init__(self):\n        self.config_cache = {}\n        self.config_version = 0\n        self.subscribers = []\n\n    def update_rate_limit_config(self, config):\n        # Validate configuration\n        self.validate_config(config)\n\n        # Store in database\n        self.db.save_rate_limit_config(config)\n\n        # Update local cache\n        self.config_cache[config.key] = config\n        self.config_version += 1\n\n        # Notify all nodes\n        self.broadcast_config_update(config)\n\n    def validate_config(self, config):\n        if config.limit <= 0:\n            raise ValueError("Rate limit must be positive")\n        if config.window_seconds <= 0:\n            raise ValueError("Window must be positive")\n        if config.algorithm not in SUPPORTED_ALGORITHMS:\n            raise ValueError(f"Unsupported algorithm: {config.algorithm}")\n'})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(n.p,{children:"This Rate Limiter design successfully handles the core requirements:"}),"\n",(0,t.jsxs)(n.p,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"Functional Requirements Met:"})]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Multi-identifier support (user, IP, API key)"}),"\n",(0,t.jsx)(n.li,{children:"Multiple algorithm implementations (token bucket, sliding window, fixed window)"}),"\n",(0,t.jsx)(n.li,{children:"Configurable per-endpoint limits"}),"\n",(0,t.jsx)(n.li,{children:"Proper HTTP responses with retry information"}),"\n",(0,t.jsx)(n.li,{children:"Real-time rate limit status"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"Non-functional Requirements Addressed:"})]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Ultra-low latency:"})," < 1ms through multi-tier caching and in-memory algorithms"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"High availability:"})," 99.99% uptime with fail-open strategy and circuit breakers"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"High throughput:"})," 10M+ checks/second via horizontal scaling and connection pooling"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Accuracy:"})," Token bucket algorithm provides precise rate limiting with burst handling"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Fault tolerance:"})," Graceful degradation when dependencies fail"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"Production-Ready Features:"})]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Distributed state management with eventual consistency"}),"\n",(0,t.jsx)(n.li,{children:"Comprehensive monitoring and alerting"}),"\n",(0,t.jsx)(n.li,{children:"Dynamic configuration updates without restarts"}),"\n",(0,t.jsx)(n.li,{children:"Batched operations for efficiency"}),"\n",(0,t.jsx)(n.li,{children:"Circuit breaker pattern for reliability"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"The design scales from thousands to millions of requests per second while maintaining sub-millisecond response times. The fail-open approach ensures that rate limiter issues don't cause cascading failures across dependent services, making it suitable for critical production environments."})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}}}]);