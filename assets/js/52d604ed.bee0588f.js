"use strict";(self.webpackChunkdev_enigma=self.webpackChunkdev_enigma||[]).push([[86631],{2202:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>t,default:()=>h,frontMatter:()=>a,metadata:()=>l,toc:()=>o});var r=s(74848),i=s(28453);const a={},t="Data Storage Systems - System Design Guide",l={id:"Backend System Design/Data Storage Systems - System Design Guide",title:"Data Storage Systems - System Design Guide",description:"A comprehensive guide to data-related storage and processing systems from a system design interview perspective.",source:"@site/docs/Backend System Design/Data Storage Systems - System Design Guide.mdx",sourceDirName:"Backend System Design",slug:"/Backend System Design/Data Storage Systems - System Design Guide",permalink:"/docs/Backend System Design/Data Storage Systems - System Design Guide",draft:!1,unlisted:!1,editUrl:"https://github.com/carefree-ladka/docs/Backend System Design/Data Storage Systems - System Design Guide.mdx",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Complete System Design Study Guide",permalink:"/docs/Backend System Design/Complete System Design Study Guide"},next:{title:"Database Systems Comparison",permalink:"/docs/Backend System Design/Database Systems Comparison"}},c={},o=[{value:"Table of Contents",id:"table-of-contents",level:2},{value:"1. Core Concepts Overview",id:"core-concepts",level:2},{value:"ETL (Extract \u2192 Transform \u2192 Load)",id:"etl-extract--transform--load",level:3},{value:"ELT (Extract \u2192 Load \u2192 Transform)",id:"elt-extract--load--transform",level:3},{value:"ETL vs ELT Comparison",id:"etl-vs-elt-comparison",level:3},{value:"2. Storage Systems",id:"storage-systems",level:2},{value:"Data Warehouse",id:"data-warehouse",level:3},{value:"Data Lake",id:"data-lake",level:3},{value:"Lakehouse Architecture",id:"lakehouse-architecture",level:3},{value:"3. Processing Engines",id:"processing-engines",level:2},{value:"Apache Spark",id:"apache-spark",level:3},{value:"Snowflake",id:"snowflake",level:3},{value:"4. System Design Considerations",id:"design-considerations",level:2},{value:"Data Flow Architecture",id:"data-flow-architecture",level:3},{value:"Key Design Questions",id:"key-design-questions",level:3},{value:"1. Data Characteristics",id:"1-data-characteristics",level:4},{value:"2. Access Patterns",id:"2-access-patterns",level:4},{value:"3. Processing Requirements",id:"3-processing-requirements",level:4},{value:"4. Scalability Needs",id:"4-scalability-needs",level:4},{value:"Storage Selection Matrix",id:"storage-selection-matrix",level:3},{value:"Scaling Strategies",id:"scaling-strategies",level:3},{value:"Ingestion Scaling",id:"ingestion-scaling",level:4},{value:"Storage Scaling",id:"storage-scaling",level:4},{value:"Query Performance Optimization",id:"query-performance-optimization",level:4},{value:"Concurrency Handling",id:"concurrency-handling",level:4},{value:"5. Modern Data Architecture Patterns",id:"architecture-patterns",level:2},{value:"Lambda Architecture",id:"lambda-architecture",level:3},{value:"Kappa Architecture",id:"kappa-architecture",level:3},{value:"Medallion Architecture (Lakehouse)",id:"medallion-architecture-lakehouse",level:3},{value:"6. Interview Checklist",id:"interview-checklist",level:2},{value:"Step 1: Clarify Requirements (5-10 minutes)",id:"step-1-clarify-requirements-5-10-minutes",level:3},{value:"Step 2: Design Ingestion (5-10 minutes)",id:"step-2-design-ingestion-5-10-minutes",level:3},{value:"Step 3: Choose Processing Approach (5 minutes)",id:"step-3-choose-processing-approach-5-minutes",level:3},{value:"Step 4: Select Storage (10 minutes)",id:"step-4-select-storage-10-minutes",level:3},{value:"Step 5: Plan Consumption (5 minutes)",id:"step-5-plan-consumption-5-minutes",level:3},{value:"Step 6: Address Non-Functional Requirements (5-10 minutes)",id:"step-6-address-non-functional-requirements-5-10-minutes",level:3},{value:"7. Common Interview Scenarios",id:"interview-scenarios",level:2},{value:"Scenario 1: Real-time Analytics Dashboard",id:"scenario-1-real-time-analytics-dashboard",level:3},{value:"Scenario 2: Data Lake for Machine Learning",id:"scenario-2-data-lake-for-machine-learning",level:3},{value:"Scenario 3: Enterprise Data Warehouse",id:"scenario-3-enterprise-data-warehouse",level:3}];function d(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",input:"input",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"data-storage-systems---system-design-guide",children:"Data Storage Systems - System Design Guide"})}),"\n",(0,r.jsx)(n.p,{children:"A comprehensive guide to data-related storage and processing systems from a system design interview perspective."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"table-of-contents",children:"Table of Contents"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"#core-concepts",children:"Core Concepts Overview"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"#storage-systems",children:"Storage Systems"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"#processing-engines",children:"Processing Engines"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"#design-considerations",children:"System Design Considerations"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"#architecture-patterns",children:"Modern Data Architecture Patterns"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"#interview-checklist",children:"Interview Checklist"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"#interview-scenarios",children:"Common Interview Scenarios"})}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"core-concepts",children:"1. Core Concepts Overview"}),"\n",(0,r.jsx)(n.h3,{id:"etl-extract--transform--load",children:"ETL (Extract \u2192 Transform \u2192 Load)"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Definition"}),": Data pipeline pattern where data is extracted from sources, transformed (cleaned, aggregated, enriched), and then loaded into the target system."]}),"\n",(0,r.jsx)(n.mermaid,{value:'graph LR\n    subgraph "Source Systems"\n        DB1[(MySQL DB)]\n        DB2[(PostgreSQL)]\n        Logs[Log Files]\n    end\n\n    subgraph "ETL Process"\n        E[Extract]\n        T[Transform<br/>\u2022 Clean<br/>\u2022 Validate<br/>\u2022 Aggregate<br/>\u2022 Enrich]\n        L[Load]\n    end\n\n    subgraph "Target Systems"\n        DW[(Data Warehouse<br/>Snowflake/Redshift)]\n    end\n\n    DB1 --\x3e E\n    DB2 --\x3e E\n    Logs --\x3e E\n    E --\x3e T\n    T --\x3e L\n    L --\x3e DW'}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Purpose"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Complex business logic transformations before storage"}),"\n",(0,r.jsx)(n.li,{children:"Data quality enforcement at ingestion"}),"\n",(0,r.jsx)(n.li,{children:"Schema validation and normalization"}),"\n",(0,r.jsx)(n.li,{children:"Data enrichment and integration from multiple sources"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"When to Use"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Transformations are computationally intensive"}),"\n",(0,r.jsx)(n.li,{children:"Need to reduce data volume before loading"}),"\n",(0,r.jsx)(n.li,{children:"Target system has limited compute capacity"}),"\n",(0,r.jsx)(n.li,{children:"Compliance requires data cleansing before storage"}),"\n",(0,r.jsx)(n.li,{children:"Legacy systems with limited query capabilities"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Typical Tools"}),": Talend, Informatica, AWS Glue, Apache Airflow + Spark, Azure Data Factory"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Example Flow"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"MySQL DB \u2192 Extract \u2192 Spark Transformation \u2192 Load \u2192 Snowflake\nLogs (S3) \u2192 Extract \u2192 Filter/Aggregate \u2192 Load \u2192 Redshift\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Trade-offs"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u2705 Reduced storage costs (only transformed data stored)"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Better performance for downstream queries"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Data quality guaranteed before loading"}),"\n",(0,r.jsx)(n.li,{children:"\u274c Longer pipeline latency"}),"\n",(0,r.jsx)(n.li,{children:"\u274c Less flexibility for reprocessing"}),"\n",(0,r.jsx)(n.li,{children:"\u274c Schema changes require pipeline updates"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"elt-extract--load--transform",children:"ELT (Extract \u2192 Load \u2192 Transform)"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Definition"}),": Modern pattern where raw data is loaded first, then transformations run inside the data warehouse/lake."]}),"\n",(0,r.jsx)(n.mermaid,{value:'graph LR\n    subgraph "Source Systems"\n        API[API Data]\n        Stream[Kafka Stream]\n        DB[(Databases)]\n    end\n\n    subgraph "ELT Process"\n        E[Extract]\n        L[Load<br/>Raw Data]\n    end\n\n    subgraph "Modern Warehouse"\n        Store[(Store Raw)]\n        T[Transform<br/>\u2022 SQL<br/>\u2022 dbt<br/>\u2022 In-warehouse]\n    end\n\n    subgraph "Outputs"\n        Views[Views/Tables]\n        Reports[Reports]\n    end\n\n    API --\x3e E\n    Stream --\x3e E\n    DB --\x3e E\n    E --\x3e L\n    L --\x3e Store\n    Store --\x3e T\n    T --\x3e Views\n    T --\x3e Reports'}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Purpose"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Leverage powerful warehouse compute engines"}),"\n",(0,r.jsx)(n.li,{children:"Preserve raw data for flexibility"}),"\n",(0,r.jsx)(n.li,{children:"Enable faster ingestion"}),"\n",(0,r.jsx)(n.li,{children:"Support schema evolution"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"When to Use"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Using modern cloud warehouses (Snowflake, BigQuery)"}),"\n",(0,r.jsx)(n.li,{children:"Need to preserve raw data"}),"\n",(0,r.jsx)(n.li,{children:"Transformations might change frequently"}),"\n",(0,r.jsx)(n.li,{children:"Multiple teams need different views of same data"}),"\n",(0,r.jsx)(n.li,{children:"Real-time or near-real-time requirements"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Advantages"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Schema-on-read flexibility"}),"\n",(0,r.jsx)(n.li,{children:"Raw data always available for reprocessing"}),"\n",(0,r.jsx)(n.li,{children:"Faster initial ingestion"}),"\n",(0,r.jsx)(n.li,{children:"Easier debugging (raw data visible)"}),"\n",(0,r.jsx)(n.li,{children:"Better support for iterative analytics"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Example Flow"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"API Data \u2192 Load to S3 \u2192 Load to Snowflake \u2192 Transform with dbt\nKafka Stream \u2192 Load to BigQuery \u2192 Transform with SQL\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Trade-offs"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u2705 Raw data preservation"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Faster time-to-insights"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 More flexible reprocessing"}),"\n",(0,r.jsx)(n.li,{children:"\u274c Higher storage costs"}),"\n",(0,r.jsx)(n.li,{children:"\u274c Requires powerful warehouse compute"}),"\n",(0,r.jsx)(n.li,{children:"\u274c Data quality issues propagate downstream"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"etl-vs-elt-comparison",children:"ETL vs ELT Comparison"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Aspect"}),(0,r.jsx)(n.th,{children:"ETL"}),(0,r.jsx)(n.th,{children:"ELT"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Transform Location"})}),(0,r.jsx)(n.td,{children:"Before load (external)"}),(0,r.jsx)(n.td,{children:"After load (in-warehouse)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Compute"})}),(0,r.jsx)(n.td,{children:"Separate processing cluster"}),(0,r.jsx)(n.td,{children:"Warehouse compute"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Data Volume"})}),(0,r.jsx)(n.td,{children:"Reduced before storage"}),(0,r.jsx)(n.td,{children:"Full raw data stored"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Schema"})}),(0,r.jsx)(n.td,{children:"Fixed upfront"}),(0,r.jsx)(n.td,{children:"Flexible, schema-on-read"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Latency"})}),(0,r.jsx)(n.td,{children:"Higher (transform first)"}),(0,r.jsx)(n.td,{children:"Lower (load raw quickly)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Cost"})}),(0,r.jsx)(n.td,{children:"Lower storage, higher compute"}),(0,r.jsx)(n.td,{children:"Higher storage, leverage warehouse"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Use Case"})}),(0,r.jsx)(n.td,{children:"Legacy systems, complex pre-processing"}),(0,r.jsx)(n.td,{children:"Modern cloud warehouses"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Best For"})}),(0,r.jsx)(n.td,{children:"Teradata, Oracle"}),(0,r.jsx)(n.td,{children:"Snowflake, BigQuery, Databricks"})]})]})]}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    subgraph "ETL - Transform Before Load"\n        ETL_Source[Data Sources] --\x3e ETL_Extract[Extract]\n        ETL_Extract --\x3e ETL_Transform[Transform<br/>Heavy Processing]\n        ETL_Transform --\x3e ETL_Load[Load]\n        ETL_Load --\x3e ETL_Target[(Reduced Data<br/>in Warehouse)]\n    end\n\n    subgraph "ELT - Transform After Load"\n        ELT_Source[Data Sources] --\x3e ELT_Extract[Extract]\n        ELT_Extract --\x3e ELT_Load[Load Raw]\n        ELT_Load --\x3e ELT_Store[(Full Raw Data<br/>in Warehouse)]\n        ELT_Store --\x3e ELT_Transform[Transform<br/>In-Warehouse]\n        ELT_Transform --\x3e ELT_Views[Transformed Views]\n    end'}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"storage-systems",children:"2. Storage Systems"}),"\n",(0,r.jsx)(n.h3,{id:"data-warehouse",children:"Data Warehouse"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Definition"}),": Structured, schema-on-write system optimized for analytics (OLAP) workloads."]}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    subgraph "Data Warehouse Architecture"\n        subgraph "Data Sources"\n            OLTP[(OLTP Databases)]\n            Logs[Application Logs]\n            External[External APIs]\n        end\n\n        subgraph "ETL/ELT Layer"\n            Pipeline[Data Pipeline]\n        end\n\n        subgraph "Warehouse Storage"\n            Fact[Fact Tables<br/>Sales, Events]\n            Dim1[Dimension: Users]\n            Dim2[Dimension: Products]\n            Dim3[Dimension: Time]\n        end\n\n        subgraph "Consumption"\n            BI[BI Tools<br/>Tableau, Looker]\n            Reports[Reports]\n            Dashboards[Dashboards]\n        end\n    end\n\n    OLTP --\x3e Pipeline\n    Logs --\x3e Pipeline\n    External --\x3e Pipeline\n\n    Pipeline --\x3e Fact\n    Pipeline --\x3e Dim1\n    Pipeline --\x3e Dim2\n    Pipeline --\x3e Dim3\n\n    Fact --\x3e BI\n    Dim1 --\x3e BI\n    Dim2 --\x3e BI\n    Dim3 --\x3e BI\n\n    BI --\x3e Reports\n    BI --\x3e Dashboards'}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Purpose"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Centralized repository for structured business data"}),"\n",(0,r.jsx)(n.li,{children:"Optimized for complex analytical queries"}),"\n",(0,r.jsx)(n.li,{children:"Historical data analysis"}),"\n",(0,r.jsx)(n.li,{children:"Business intelligence and reporting"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Architecture Characteristics"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Columnar storage"}),": Optimized for aggregate queries"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Append-only"}),": Immutable historical records"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Optimized joins"}),": Efficient cross-table analytics"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Materialized views"}),": Pre-computed aggregations"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Query optimization"}),": Cost-based optimizers"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Data Modeling"}),":"]}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    subgraph "Star Schema Example"\n        FactSales[Fact Table: Sales<br/>---------<br/>sale_id<br/>user_id FK<br/>product_id FK<br/>date_id FK<br/>amount<br/>quantity]\n\n        DimUser[Dimension: Users<br/>---------<br/>user_id PK<br/>name<br/>email<br/>country<br/>segment]\n\n        DimProduct[Dimension: Products<br/>---------<br/>product_id PK<br/>name<br/>category<br/>brand<br/>price]\n\n        DimDate[Dimension: Date<br/>---------<br/>date_id PK<br/>date<br/>month<br/>quarter<br/>year]\n\n        FactSales --\x3e DimUser\n        FactSales --\x3e DimProduct\n        FactSales --\x3e DimDate\n    end'}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Star Schema"}),": Fact table + dimension tables"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Snowflake Schema"}),": Normalized dimension tables"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Fact Tables"}),": Metrics, measurements (sales, clicks)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Dimension Tables"}),": Descriptive attributes (users, products, time)"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Examples"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Snowflake (cloud-native)"}),"\n",(0,r.jsx)(n.li,{children:"Amazon Redshift (AWS)"}),"\n",(0,r.jsx)(n.li,{children:"Google BigQuery (GCP)"}),"\n",(0,r.jsx)(n.li,{children:"Azure Synapse Analytics"}),"\n",(0,r.jsx)(n.li,{children:"Teradata (on-premise/legacy)"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"When to Use"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Structured business analytics"}),"\n",(0,r.jsx)(n.li,{children:"BI dashboards and reports"}),"\n",(0,r.jsx)(n.li,{children:"SQL-heavy workloads"}),"\n",(0,r.jsx)(n.li,{children:"Historical trend analysis"}),"\n",(0,r.jsx)(n.li,{children:"Regulatory reporting"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Trade-offs"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u2705 Excellent query performance for analytics"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 SQL interface familiar to analysts"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Strong consistency guarantees"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Optimized for aggregations and joins"}),"\n",(0,r.jsx)(n.li,{children:"\u274c Schema changes are costly"}),"\n",(0,r.jsx)(n.li,{children:"\u274c Not ideal for unstructured data"}),"\n",(0,r.jsx)(n.li,{children:"\u274c Higher cost per GB than data lakes"}),"\n",(0,r.jsx)(n.li,{children:"\u274c Not designed for real-time updates"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"data-lake",children:"Data Lake"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Definition"}),": Repository that stores raw data in native format - structured, semi-structured, and unstructured."]}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    subgraph "Data Lake Architecture"\n        subgraph "Ingestion"\n            Batch[Batch Ingestion]\n            Stream[Stream Ingestion]\n            API[API Ingestion]\n        end\n\n        subgraph "Storage - S3/ADLS/GCS"\n            Raw[Raw Zone<br/>JSON, CSV, Logs]\n            Processed[Processed Zone<br/>Parquet, Avro]\n            Curated[Curated Zone<br/>Optimized Tables]\n        end\n\n        subgraph "Processing"\n            Spark[Spark Jobs]\n            Glue[AWS Glue]\n            Dataflow[Cloud Dataflow]\n        end\n\n        subgraph "Consumption"\n            Athena[Athena/Presto]\n            ML[ML Platforms]\n            Analytics[Analytics Tools]\n        end\n\n        subgraph "Governance"\n            Catalog[Data Catalog]\n            Lineage[Data Lineage]\n            Quality[Quality Checks]\n        end\n    end\n\n    Batch --\x3e Raw\n    Stream --\x3e Raw\n    API --\x3e Raw\n\n    Raw --\x3e Spark\n    Spark --\x3e Processed\n    Processed --\x3e Glue\n    Glue --\x3e Curated\n\n    Curated --\x3e Athena\n    Curated --\x3e ML\n    Curated --\x3e Analytics\n\n    Catalog -.-> Raw\n    Catalog -.-> Processed\n    Catalog -.-> Curated'}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Purpose"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Store massive volumes of raw data cheaply"}),"\n",(0,r.jsx)(n.li,{children:"Support diverse data types and formats"}),"\n",(0,r.jsx)(n.li,{children:"Enable exploratory analytics and ML"}),"\n",(0,r.jsx)(n.li,{children:"Serve as single source of truth"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Schema Approach"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Schema-on-read"}),": Structure applied at query time"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Format flexibility"}),": JSON, CSV, Parquet, Avro, logs, images, videos"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"No upfront modeling"}),": Store first, structure later"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Examples"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"AWS S3 + Athena/Glue"}),"\n",(0,r.jsx)(n.li,{children:"Azure Data Lake Storage (ADLS)"}),"\n",(0,r.jsx)(n.li,{children:"Google Cloud Storage + BigQuery"}),"\n",(0,r.jsx)(n.li,{children:"Hadoop HDFS (on-premise)"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Common Formats"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Parquet"}),": Columnar, compressed, great for analytics"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Avro"}),": Row-based, schema evolution support"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ORC"}),": Optimized columnar format for Hive"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"JSON"}),": Flexible but inefficient for large scale"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"When to Use"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Storing raw logs, events, clickstreams"}),"\n",(0,r.jsx)(n.li,{children:"IoT sensor data"}),"\n",(0,r.jsx)(n.li,{children:"Machine learning training data"}),"\n",(0,r.jsx)(n.li,{children:"Unstructured data (images, videos, documents)"}),"\n",(0,r.jsx)(n.li,{children:"Data science exploration"}),"\n",(0,r.jsx)(n.li,{children:"Cost-effective long-term storage"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Challenges"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Data governance and cataloging"}),"\n",(0,r.jsx)(n.li,{children:"Data quality consistency"}),"\n",(0,r.jsx)(n.li,{children:"Query performance can be poor"}),"\n",(0,r.jsx)(n.li,{children:"Difficult to maintain ACID properties"}),"\n",(0,r.jsx)(n.li,{children:'"Data swamp" risk without proper management'}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Trade-offs"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u2705 Very low storage cost"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Handles any data type"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Massive scalability"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Great for ML and data science"}),"\n",(0,r.jsx)(n.li,{children:"\u274c No ACID guarantees (traditionally)"}),"\n",(0,r.jsx)(n.li,{children:"\u274c Poor query performance without optimization"}),"\n",(0,r.jsx)(n.li,{children:"\u274c Governance challenges"}),"\n",(0,r.jsx)(n.li,{children:"\u274c Requires data catalog tools"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"lakehouse-architecture",children:"Lakehouse Architecture"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Definition"}),": Hybrid approach combining data lake flexibility with data warehouse performance and reliability."]}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    subgraph "Lakehouse Architecture - Delta Lake / Iceberg / Hudi"\n        subgraph "Data Sources"\n            Sources[Batch & Streaming<br/>Sources]\n        end\n\n        subgraph "Object Storage - S3/ADLS"\n            Bronze[Bronze Layer<br/>Raw Data<br/>All formats]\n            Silver[Silver Layer<br/>Cleaned, Validated<br/>Delta/Iceberg format]\n            Gold[Gold Layer<br/>Business Aggregates<br/>Star Schema]\n        end\n\n        subgraph "Compute Engines"\n            Spark[Spark]\n            Presto[Presto/Trino]\n            Flink[Flink]\n        end\n\n        subgraph "Features"\n            ACID[ACID Transactions]\n            TimeTravel[Time Travel]\n            Schema[Schema Evolution]\n            Upsert[Upserts/Deletes]\n        end\n\n        subgraph "Consumption"\n            BI[BI Tools]\n            ML[ML Workloads]\n            Apps[Applications]\n        end\n    end\n\n    Sources --\x3e Bronze\n    Bronze --\x3e Silver\n    Silver --\x3e Gold\n\n    Spark --\x3e Silver\n    Spark --\x3e Gold\n    Presto --\x3e Gold\n    Flink --\x3e Silver\n\n    ACID -.-> Silver\n    TimeTravel -.-> Silver\n    Schema -.-> Silver\n    Upsert -.-> Silver\n\n    Gold --\x3e BI\n    Silver --\x3e ML\n    Gold --\x3e Apps'}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Purpose"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Single platform for all data workloads"}),"\n",(0,r.jsx)(n.li,{children:"ACID transactions on data lake storage"}),"\n",(0,r.jsx)(n.li,{children:"Schema enforcement with flexibility"}),"\n",(0,r.jsx)(n.li,{children:"Support both BI and ML workloads"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Key Technologies"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Delta Lake"})," (Databricks)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Apache Iceberg"})," (Netflix/Apple)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Apache Hudi"})," (Uber)"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Features"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"ACID transactions on object storage"}),"\n",(0,r.jsx)(n.li,{children:"Time travel and versioning"}),"\n",(0,r.jsx)(n.li,{children:"Schema enforcement and evolution"}),"\n",(0,r.jsx)(n.li,{children:"Unified batch and streaming"}),"\n",(0,r.jsx)(n.li,{children:"Z-ordering and data skipping"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Medallion Architecture Layers"}),":"]}),"\n",(0,r.jsx)(n.mermaid,{value:'graph LR\n    subgraph "Bronze - Raw"\n        B[Raw Data<br/>As-is from source<br/>JSON, CSV, Logs]\n    end\n\n    subgraph "Silver - Refined"\n        S[Cleaned Data<br/>Validated<br/>Deduplicated<br/>Type-safe]\n    end\n\n    subgraph "Gold - Curated"\n        G[Business Aggregates<br/>Feature Tables<br/>Dimension Tables<br/>Fact Tables]\n    end\n\n    B --\x3e S\n    S --\x3e G\n\n    style B fill:#cd7f32\n    style S fill:#c0c0c0\n    style G fill:#ffd700'}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"When to Use"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Need both data lake economics and warehouse performance"}),"\n",(0,r.jsx)(n.li,{children:"Supporting diverse teams (data scientists + analysts)"}),"\n",(0,r.jsx)(n.li,{children:"Want to eliminate data silos"}),"\n",(0,r.jsx)(n.li,{children:"Need strong consistency with S3-like costs"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Trade-offs"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u2705 Best of both worlds (lake + warehouse)"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Lower cost than pure warehouse"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 ACID guarantees on cheap storage"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Eliminates data duplication"}),"\n",(0,r.jsx)(n.li,{children:"\u274c More complex to set up"}),"\n",(0,r.jsx)(n.li,{children:"\u274c Newer technology (less mature)"}),"\n",(0,r.jsx)(n.li,{children:"\u274c Requires learning new concepts"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"processing-engines",children:"3. Processing Engines"}),"\n",(0,r.jsx)(n.h3,{id:"apache-spark",children:"Apache Spark"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Definition"}),": Distributed compute engine for large-scale data processing (batch and streaming)."]}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    subgraph "Spark Architecture"\n        subgraph "Driver Program"\n            Driver[Spark Driver<br/>SparkContext<br/>DAG Scheduler<br/>Task Scheduler]\n        end\n\n        subgraph "Cluster Manager"\n            CM[YARN / Kubernetes<br/>Mesos / Standalone]\n        end\n\n        subgraph "Worker Node 1"\n            E1[Executor 1]\n            T1[Task 1]\n            T2[Task 2]\n            Cache1[Block Cache]\n        end\n\n        subgraph "Worker Node 2"\n            E2[Executor 2]\n            T3[Task 3]\n            T4[Task 4]\n            Cache2[Block Cache]\n        end\n\n        subgraph "Worker Node 3"\n            E3[Executor 3]\n            T5[Task 5]\n            T6[Task 6]\n            Cache3[Block Cache]\n        end\n\n        subgraph "Storage"\n            HDFS[(HDFS)]\n            S3[(S3)]\n        end\n    end\n\n    Driver --\x3e CM\n    CM --\x3e E1\n    CM --\x3e E2\n    CM --\x3e E3\n\n    E1 --\x3e T1\n    E1 --\x3e T2\n    E2 --\x3e T3\n    E2 --\x3e T4\n    E3 --\x3e T5\n    E3 --\x3e T6\n\n    T1 --\x3e Cache1\n    T3 --\x3e Cache2\n    T5 --\x3e Cache3\n\n    HDFS -.-> T1\n    HDFS -.-> T3\n    HDFS -.-> T5\n    S3 -.-> T2\n    S3 -.-> T4\n    S3 -.-> T6'}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Purpose"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Process massive datasets in parallel"}),"\n",(0,r.jsx)(n.li,{children:"Complex transformations and aggregations"}),"\n",(0,r.jsx)(n.li,{children:"Machine learning pipelines"}),"\n",(0,r.jsx)(n.li,{children:"Unified batch and streaming"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Architecture Components"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Driver"}),": Coordinates work, builds execution plan"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Executors"}),": Perform computations on worker nodes"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Cluster Manager"}),": Resource allocation (YARN, Kubernetes, Mesos)"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"APIs"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Spark SQL"})," (DataFrames) - Structured data processing"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"RDD"})," (Resilient Distributed Datasets) - Low-level API"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Structured Streaming"})," - Stream processing"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"MLlib"})," - Machine learning library"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"GraphX"})," - Graph processing"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Processing Model"}),":"]}),"\n",(0,r.jsx)(n.mermaid,{value:'graph LR\n    subgraph "Spark Processing Flow"\n        Read[Read Data<br/>HDFS/S3/Kafka]\n        Transform1[Transformation 1<br/>Filter, Map]\n        Transform2[Transformation 2<br/>Join, GroupBy]\n        Transform3[Transformation 3<br/>Aggregate]\n        Action[Action<br/>Write, Count]\n    end\n\n    Read --\x3e Transform1\n    Transform1 --\x3e Transform2\n    Transform2 --\x3e Transform3\n    Transform3 --\x3e Action\n\n    Note1[Lazy Evaluation<br/>No execution until Action]\n\n    Transform1 -.-> Note1\n    Transform2 -.-> Note1\n    Transform3 -.-> Note1'}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"When to Use"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"ETL pipelines with complex logic"}),"\n",(0,r.jsx)(n.li,{children:"Large-scale data transformations (TB/PB scale)"}),"\n",(0,r.jsx)(n.li,{children:"Machine learning at scale"}),"\n",(0,r.jsx)(n.li,{children:"Batch processing of historical data"}),"\n",(0,r.jsx)(n.li,{children:"Stream processing with Structured Streaming (second-level latency acceptable)"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Common Deployment Pattern"}),":"]}),"\n",(0,r.jsx)(n.mermaid,{value:'graph LR\n    subgraph "Batch Processing"\n        Source1[(HDFS/S3<br/>Historical Data)]\n        Spark1[Spark Batch Job<br/>Daily/Hourly ETL]\n        Target1[(Parquet Files<br/>Data Lake)]\n    end\n\n    subgraph "Stream Processing"\n        Source2[Kafka<br/>Real-time Events]\n        Spark2[Spark Streaming<br/>Micro-batches]\n        Target2[(Delta Lake<br/>Near Real-time)]\n    end\n\n    Source1 --\x3e Spark1\n    Spark1 --\x3e Target1\n\n    Source2 --\x3e Spark2\n    Spark2 --\x3e Target2'}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Trade-offs"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u2705 Massive scalability (handles PB-scale data)"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Rich transformation APIs (SQL, DataFrame, RDD)"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Supports multiple languages (Scala, Python, Java, R)"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Unified batch and streaming"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 In-memory processing for speed"}),"\n",(0,r.jsx)(n.li,{children:"\u274c High operational complexity"}),"\n",(0,r.jsx)(n.li,{children:"\u274c Expensive for small workloads"}),"\n",(0,r.jsx)(n.li,{children:"\u274c Requires cluster management expertise"}),"\n",(0,r.jsx)(n.li,{children:"\u274c Higher latency than true streaming (Flink)"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"snowflake",children:"Snowflake"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Definition"}),": Cloud-native data warehouse with separated compute and storage architecture."]}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    subgraph "Snowflake Architecture"\n        subgraph "Cloud Services Layer"\n            Auth[Authentication]\n            Meta[Metadata Management]\n            Query[Query Optimizer]\n            Security[Security & Governance]\n        end\n\n        subgraph "Compute Layer - Virtual Warehouses"\n            VW1[Virtual Warehouse 1<br/>ETL Workloads<br/>X-Large]\n            VW2[Virtual Warehouse 2<br/>BI Queries<br/>Medium]\n            VW3[Virtual Warehouse 3<br/>Data Science<br/>Large]\n        end\n\n        subgraph "Storage Layer"\n            Store[(Centralized Storage<br/>Columnar Format<br/>Compressed<br/>Encrypted)]\n        end\n\n        subgraph "Data Sharing"\n            Share[Data Sharing<br/>Between Accounts<br/>Zero-copy]\n        end\n    end\n\n    Auth --\x3e VW1\n    Auth --\x3e VW2\n    Auth --\x3e VW3\n\n    Meta --\x3e Store\n    Query --\x3e VW1\n    Query --\x3e VW2\n    Query --\x3e VW3\n\n    VW1 --\x3e Store\n    VW2 --\x3e Store\n    VW3 --\x3e Store\n\n    Store --\x3e Share'}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Architecture Innovation"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Storage Layer"}),": Centralized, automatic replication, micro-partitions"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Compute Layer"}),": Multiple independent virtual warehouses"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Cloud Services"}),": Metadata, optimization, security, transaction management"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Key Features"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Separation of Compute and Storage"})}),"\n"]}),"\n",(0,r.jsx)(n.mermaid,{value:'graph LR\n    subgraph "Traditional Warehouse"\n        T_Compute[Compute]\n        T_Storage[(Storage)]\n        T_Compute --- T_Storage\n    end\n\n    subgraph "Snowflake Model"\n        S_Compute1[Compute 1]\n        S_Compute2[Compute 2]\n        S_Compute3[Compute 3]\n        S_Storage[(Centralized<br/>Storage)]\n\n        S_Compute1 --\x3e S_Storage\n        S_Compute2 --\x3e S_Storage\n        S_Compute3 --\x3e S_Storage\n    end\n\n    style T_Compute fill:#ffcccc\n    style T_Storage fill:#ffcccc\n    style S_Compute1 fill:#ccffcc\n    style S_Compute2 fill:#ccffcc\n    style S_Compute3 fill:#ccffcc\n    style S_Storage fill:#ccffcc'}),"\n",(0,r.jsxs)(n.ol,{start:"2",children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Zero-Copy Cloning"}),": Instant data copies without storage duplication"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Time Travel"}),": Query historical data (up to 90 days)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Multi-cluster Warehouses"}),": Auto-scaling for high concurrency"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Data Sharing"}),": Share live data between accounts without copying"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Secure Views"}),": Row/column-level security"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"When to Use"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Business intelligence and analytics"}),"\n",(0,r.jsx)(n.li,{children:"Data warehousing with variable workloads"}),"\n",(0,r.jsx)(n.li,{children:"Need to scale read and write independently"}),"\n",(0,r.jsx)(n.li,{children:"Multiple teams with different compute needs"}),"\n",(0,r.jsx)(n.li,{children:"Cross-organization data sharing"}),"\n",(0,r.jsx)(n.li,{children:"Ad-hoc analytics with varying concurrency"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Design Patterns"}),":"]}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    subgraph "Snowflake Usage Patterns"\n        subgraph "Ingestion"\n            ETL_WH[ETL Virtual Warehouse<br/>X-Large<br/>Runs 2 hours/day]\n        end\n\n        subgraph "Analytics"\n            BI_WH[BI Virtual Warehouse<br/>Medium<br/>Multi-cluster<br/>Auto-scale 1-5]\n        end\n\n        subgraph "Data Science"\n            DS_WH[Data Science WH<br/>Large<br/>On-demand]\n        end\n\n        subgraph "Shared Storage"\n            Tables[(Tables<br/>Clustered by date<br/>Materialized Views)]\n        end\n    end\n\n    ETL_WH --\x3e Tables\n    Tables --\x3e BI_WH\n    Tables --\x3e DS_WH'}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Create separate warehouses for ETL, BI, Data Science"}),"\n",(0,r.jsx)(n.li,{children:"Use clustering keys for large tables (>1TB)"}),"\n",(0,r.jsx)(n.li,{children:"Materialize views for repeated aggregations"}),"\n",(0,r.jsx)(n.li,{children:"Partition by time for time-series data"}),"\n",(0,r.jsx)(n.li,{children:"Use multi-cluster warehouses for high concurrency"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Trade-offs"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u2705 Scales compute independently from storage"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 No infrastructure management (fully managed)"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Excellent concurrency handling"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Pay only for compute used (per-second billing)"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Automatic optimization and tuning"}),"\n",(0,r.jsx)(n.li,{children:"\u274c Can be expensive at scale (compute costs add up)"}),"\n",(0,r.jsx)(n.li,{children:"\u274c Vendor lock-in"}),"\n",(0,r.jsx)(n.li,{children:"\u274c Not ideal for real-time updates (optimized for batch/micro-batch)"}),"\n",(0,r.jsx)(n.li,{children:"\u274c Limited support for unstructured data"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"design-considerations",children:"4. System Design Considerations"}),"\n",(0,r.jsx)(n.h3,{id:"data-flow-architecture",children:"Data Flow Architecture"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Typical Modern Data Stack"}),":"]}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    subgraph "Data Sources Layer"\n        OLTP[(OLTP Databases<br/>MySQL, Postgres)]\n        Logs[Application Logs<br/>CloudWatch, Datadog]\n        APIs[External APIs<br/>REST, GraphQL]\n        IoT[IoT Devices<br/>Sensors, Events]\n    end\n\n    subgraph "Ingestion Layer"\n        CDC[Change Data Capture<br/>Debezium, Maxwell]\n        MQ[Message Queue<br/>Kafka, Kinesis, Pub/Sub]\n        Gateway[API Gateways<br/>Kong, Apigee]\n    end\n\n    subgraph "Processing Layer"\n        Stream[Stream Processing<br/>Flink, Spark Streaming]\n        Batch[Batch Processing<br/>Spark, Airflow, dbt]\n    end\n\n    subgraph "Storage Layer"\n        Lake[Data Lake S3/ADLS<br/>Bronze: Raw Data]\n        Lakehouse[Lakehouse Delta/Iceberg<br/>Silver: Curated]\n        Warehouse[Data Warehouse<br/>Gold: Analytics Ready]\n    end\n\n    subgraph "Consumption Layer"\n        BI[BI Tools<br/>Tableau, Looker, Power BI]\n        ML[ML Platforms<br/>SageMaker, Databricks]\n        API_Layer[API Layer<br/>REST, GraphQL]\n        Apps[Applications<br/>Dashboards, Reports]\n    end\n\n    OLTP --\x3e CDC\n    Logs --\x3e MQ\n    APIs --\x3e Gateway\n    IoT --\x3e MQ\n\n    CDC --\x3e MQ\n    Gateway --\x3e MQ\n\n    MQ --\x3e Stream\n    MQ --\x3e Batch\n\n    Stream --\x3e Lake\n    Batch --\x3e Lake\n\n    Lake --\x3e Lakehouse\n    Lakehouse --\x3e Warehouse\n\n    Warehouse --\x3e BI\n    Lakehouse --\x3e ML\n    Warehouse --\x3e API_Layer\n    API_Layer --\x3e Apps\n\n    style Lake fill:#cd7f32\n    style Lakehouse fill:#c0c0c0\n    style Warehouse fill:#ffd700'}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"key-design-questions",children:"Key Design Questions"}),"\n",(0,r.jsx)(n.p,{children:"When designing a data analytics system, always ask these questions:"}),"\n",(0,r.jsx)(n.h4,{id:"1-data-characteristics",children:"1. Data Characteristics"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Volume"}),": How much data per day/hour/minute?"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Velocity"}),": Real-time, near real-time, micro-batch, or batch?"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Variety"}),": Structured, semi-structured, or unstructured?"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Veracity"}),": What are data quality and consistency needs?"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"2-access-patterns",children:"2. Access Patterns"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Read-heavy or write-heavy?"}),"\n",(0,r.jsx)(n.li,{children:"Query latency requirements? (milliseconds vs seconds vs minutes)"}),"\n",(0,r.jsx)(n.li,{children:"Number of concurrent users?"}),"\n",(0,r.jsx)(n.li,{children:"Ad-hoc vs predefined queries?"}),"\n",(0,r.jsx)(n.li,{children:"Point queries vs analytical aggregations?"}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"3-processing-requirements",children:"3. Processing Requirements"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Transformation complexity? (simple filters vs complex joins)"}),"\n",(0,r.jsx)(n.li,{children:"Need for joins across multiple sources?"}),"\n",(0,r.jsx)(n.li,{children:"Stateful vs stateless processing?"}),"\n",(0,r.jsx)(n.li,{children:"Reprocessing/backfill requirements?"}),"\n",(0,r.jsx)(n.li,{children:"Data lineage and audit needs?"}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"4-scalability-needs",children:"4. Scalability Needs"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Expected data growth rate?"}),"\n",(0,r.jsx)(n.li,{children:"Geographic distribution requirements?"}),"\n",(0,r.jsx)(n.li,{children:"Peak vs average load patterns?"}),"\n",(0,r.jsx)(n.li,{children:"Data retention and archival policies?"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"storage-selection-matrix",children:"Storage Selection Matrix"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Scenario"}),(0,r.jsx)(n.th,{children:"Best Choice"}),(0,r.jsx)(n.th,{children:"Reason"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Raw logs & ML data"}),(0,r.jsx)(n.td,{children:"Data Lake (S3/ADLS)"}),(0,r.jsx)(n.td,{children:"Schema-on-read, cost-effective, supports any format"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Business reports & BI"}),(0,r.jsx)(n.td,{children:"Data Warehouse (Snowflake/BigQuery)"}),(0,r.jsx)(n.td,{children:"Optimized for OLAP, SQL interface"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Complex ETL pipelines"}),(0,r.jsx)(n.td,{children:"Spark / AWS Glue"}),(0,r.jsx)(n.td,{children:"Distributed compute, handles TB/PB scale"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Unified raw + analytics"}),(0,r.jsx)(n.td,{children:"Lakehouse (Delta/Iceberg)"}),(0,r.jsx)(n.td,{children:"Combines lake economics with warehouse performance"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Real-time analytics"}),(0,r.jsx)(n.td,{children:"Kafka + Flink + Warehouse"}),(0,r.jsx)(n.td,{children:"Low latency streaming pipeline"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"High concurrency BI"}),(0,r.jsx)(n.td,{children:"Snowflake / BigQuery"}),(0,r.jsx)(n.td,{children:"Scales compute separately, multi-cluster support"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Operational analytics"}),(0,r.jsx)(n.td,{children:"Druid / ClickHouse"}),(0,r.jsx)(n.td,{children:"Fast aggregations, sub-second queries"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Time-series data"}),(0,r.jsx)(n.td,{children:"InfluxDB / TimescaleDB / Druid"}),(0,r.jsx)(n.td,{children:"Optimized for time-based queries"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Search & text analytics"}),(0,r.jsx)(n.td,{children:"Elasticsearch"}),(0,r.jsx)(n.td,{children:"Full-text search, log analytics"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Graph relationships"}),(0,r.jsx)(n.td,{children:"Neo4j / Amazon Neptune"}),(0,r.jsx)(n.td,{children:"Graph traversal, relationship queries"})]})]})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"scaling-strategies",children:"Scaling Strategies"}),"\n",(0,r.jsx)(n.h4,{id:"ingestion-scaling",children:"Ingestion Scaling"}),"\n",(0,r.jsx)(n.mermaid,{value:'graph LR\n    subgraph "High-Throughput Ingestion"\n        Sources[Multiple Sources<br/>Millions events/sec]\n        LB[Load Balancer]\n\n        subgraph "Message Queue Partitions"\n            P1[Partition 1]\n            P2[Partition 2]\n            P3[Partition 3]\n            P4[Partition N]\n        end\n\n        subgraph "Consumer Groups"\n            C1[Consumer 1]\n            C2[Consumer 2]\n            C3[Consumer 3]\n            C4[Consumer N]\n        end\n\n        Store[(Storage)]\n    end\n\n    Sources --\x3e LB\n    LB --\x3e P1\n    LB --\x3e P2\n    LB --\x3e P3\n    LB --\x3e P4\n\n    P1 --\x3e C1\n    P2 --\x3e C2\n    P3 --\x3e C3\n    P4 --\x3e C4\n\n    C1 --\x3e Store\n    C2 --\x3e Store\n    C3 --\x3e Store\n    C4 --\x3e Store'}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Strategies"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Message queues (Kafka, Kinesis) for buffering"}),"\n",(0,r.jsx)(n.li,{children:"Partition by key for parallel processing"}),"\n",(0,r.jsx)(n.li,{children:"Rate limiting at source"}),"\n",(0,r.jsx)(n.li,{children:"Back-pressure mechanisms"}),"\n",(0,r.jsx)(n.li,{children:"Dead letter queues for failed messages"}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"storage-scaling",children:"Storage Scaling"}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    subgraph "Storage Tiers"\n        Hot[Hot Tier<br/>Last 7 days<br/>SSD/Premium<br/>$$]\n        Warm[Warm Tier<br/>8-90 days<br/>Standard Storage<br/>$$]\n        Cold[Cold Tier<br/>90+ days<br/>Archive Storage<br/>$]\n\n        Query[Query Engine]\n    end\n\n    Hot --\x3e Query\n    Warm --\x3e Query\n    Cold --\x3e Query\n\n    Hot -.->|Age out| Warm\n    Warm -.->|Age out| Cold'}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Strategies"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Separate compute from storage (Snowflake model)"}),"\n",(0,r.jsx)(n.li,{children:"Horizontal partitioning (sharding by key)"}),"\n",(0,r.jsx)(n.li,{children:"Time-based partitioning (year/month/day)"}),"\n",(0,r.jsx)(n.li,{children:"Tiered storage (hot/warm/cold based on access patterns)"}),"\n",(0,r.jsx)(n.li,{children:"Compression and columnar formats"}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"query-performance-optimization",children:"Query Performance Optimization"}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    subgraph "Query Optimization Strategies"\n        Query[User Query]\n\n        subgraph "Optimization Layer"\n            Cache[Result Cache<br/>Recent queries]\n            MV[Materialized Views<br/>Pre-computed aggregations]\n            Index[Indexes & Statistics<br/>Query planning]\n            Partition[Partition Pruning<br/>Scan only relevant data]\n        end\n\n        subgraph "Execution"\n            Parallel[Parallel Execution<br/>Multiple nodes]\n            Compute[Compute Cluster]\n        end\n\n        Storage[(Storage<br/>Partitioned Tables)]\n    end\n\n    Query --\x3e Cache\n    Cache --\x3e|Miss| MV\n    MV --\x3e|Miss| Index\n    Index --\x3e Partition\n    Partition --\x3e Parallel\n    Parallel --\x3e Compute\n    Compute --\x3e Storage'}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Techniques"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Materialized views"})," for repeated aggregations"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Clustering keys / Sort keys"})," for common filters"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Data skipping"})," with statistics (min/max, bloom filters)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Result caching"})," for identical queries"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Query federation"})," across multiple sources"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Partition pruning"})," to scan only relevant data"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"concurrency-handling",children:"Concurrency Handling"}),"\n",(0,r.jsx)(n.mermaid,{value:'graph LR\n    subgraph "High Concurrency Architecture"\n        Users[100s of Concurrent Users]\n\n        subgraph "Load Distribution"\n            Router[Query Router]\n\n            subgraph "Auto-Scaling Compute"\n                C1[Cluster 1]\n                C2[Cluster 2]\n                C3[Cluster 3]\n                C4[Cluster N]\n            end\n        end\n\n        subgraph "Optimization"\n            Queue[Query Queue<br/>Priority-based]\n            Pool[Connection Pool]\n        end\n\n        Store[(Shared Storage)]\n    end\n\n    Users --\x3e Router\n    Router --\x3e Queue\n    Queue --\x3e Pool\n\n    Pool --\x3e C1\n    Pool --\x3e C2\n    Pool --\x3e C3\n    Pool --\x3e C4\n\n    C1 --\x3e Store\n    C2 --\x3e Store\n    C3 --\x3e Store\n    C4 --\x3e Store'}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Strategies"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Multi-cluster compute (Snowflake multi-cluster warehouses)"}),"\n",(0,r.jsx)(n.li,{children:"Read replicas for read-heavy workloads"}),"\n",(0,r.jsx)(n.li,{children:"Connection pooling to manage connections efficiently"}),"\n",(0,r.jsx)(n.li,{children:"Query queuing and prioritization"}),"\n",(0,r.jsx)(n.li,{children:"Workload management (separate ETL from BI queries)"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"architecture-patterns",children:"5. Modern Data Architecture Patterns"}),"\n",(0,r.jsx)(n.h3,{id:"lambda-architecture",children:"Lambda Architecture"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Concept"}),": Separate batch and speed layers for comprehensive data processing."]}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    subgraph "Lambda Architecture"\n        subgraph "Data Sources"\n            Source[Data Sources<br/>Events, Logs, Transactions]\n        end\n\n        subgraph "Batch Layer"\n            BatchStore[(Master Dataset<br/>HDFS/S3<br/>Immutable, append-only)]\n            BatchProcess[Batch Processing<br/>Spark, MapReduce<br/>Complete, accurate views]\n            BatchView[(Batch Views<br/>Pre-computed aggregations)]\n        end\n\n        subgraph "Speed Layer"\n            StreamQueue[Message Queue<br/>Kafka]\n            StreamProcess[Stream Processing<br/>Flink, Storm<br/>Low latency, approximate]\n            RealtimeView[(Realtime Views<br/>Recent data only)]\n        end\n\n        subgraph "Serving Layer"\n            Merge[Query Merge Logic<br/>Combines batch + realtime]\n            API[API / Application]\n        end\n    end\n\n    Source --\x3e StreamQueue\n    Source --\x3e BatchStore\n\n    StreamQueue --\x3e StreamProcess\n    StreamProcess --\x3e RealtimeView\n\n    BatchStore --\x3e BatchProcess\n    BatchProcess --\x3e BatchView\n\n    BatchView --\x3e Merge\n    RealtimeView --\x3e Merge\n\n    Merge --\x3e API'}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Layers"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Batch Layer"}),": Historical data processed with Spark/MapReduce on HDFS/S3"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Complete and accurate"}),"\n",(0,r.jsx)(n.li,{children:"High latency (hours/days)"}),"\n",(0,r.jsx)(n.li,{children:"Immutable dataset"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Speed Layer"}),": Real-time data processed with Flink/Storm from Kafka"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Low latency (seconds/minutes)"}),"\n",(0,r.jsx)(n.li,{children:"Approximate results"}),"\n",(0,r.jsx)(n.li,{children:"Compensates for batch layer lag"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Serving Layer"}),": Merged views from both layers (Druid, Cassandra, HBase)"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Queries combine batch and real-time data"}),"\n",(0,r.jsx)(n.li,{children:"Application-facing API"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Purpose"}),": Handle both batch and real-time processing with different latency/accuracy trade-offs"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Use Cases"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Real-time dashboards with historical context"}),"\n",(0,r.jsx)(n.li,{children:"Fraud detection with learning from history"}),"\n",(0,r.jsx)(n.li,{children:"Recommendation systems"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Challenges"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Complexity of maintaining two separate code paths"}),"\n",(0,r.jsx)(n.li,{children:"Data consistency between layers"}),"\n",(0,r.jsx)(n.li,{children:"Operational overhead"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"kappa-architecture",children:"Kappa Architecture"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Concept"}),": Simplified approach treating everything as a stream."]}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    subgraph "Kappa Architecture"\n        subgraph "Data Sources"\n            Source[Data Sources<br/>All data as events]\n        end\n\n        subgraph "Stream Storage"\n            Kafka[Kafka / Kinesis<br/>Replayable Event Log<br/>Long retention]\n        end\n\n        subgraph "Stream Processing"\n            Process1[Stream Processing Layer<br/>Flink / Spark Streaming<br/>Real-time views]\n            Process2[Batch Reprocessing<br/>Same code, replay from offset<br/>Historical corrections]\n        end\n\n        subgraph "Serving Layer"\n            DB[(Serving Database<br/>Cassandra, Elasticsearch<br/>Single source of truth)]\n            API[API Layer]\n        end\n    end\n\n    Source --\x3e Kafka\n    Kafka --\x3e Process1\n    Kafka -.->|Replay for backfill| Process2\n\n    Process1 --\x3e DB\n    Process2 -.->|Corrections| DB\n\n    DB --\x3e API'}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simplified Approach"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Single stream processing pipeline for all data"}),"\n",(0,r.jsx)(n.li,{children:"Replayable message queue (Kafka with long retention)"}),"\n",(0,r.jsx)(n.li,{children:"Same code handles both real-time and batch"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Components"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Stream Storage"}),": Kafka with days/weeks of retention"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Processing"}),": Flink, Spark Streaming, Kafka Streams"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Serving"}),": Cassandra, Elasticsearch, Druid"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Advantages"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Single codebase (no duplicate logic)"}),"\n",(0,r.jsx)(n.li,{children:"Easier to maintain and debug"}),"\n",(0,r.jsx)(n.li,{children:"Flexibility to reprocess by replaying stream"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"When to Use"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"All data can be modeled as events/streams"}),"\n",(0,r.jsx)(n.li,{children:"Team has strong stream processing expertise"}),"\n",(0,r.jsx)(n.li,{children:"Want to avoid complexity of Lambda"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Limitations"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Requires replayable message queue"}),"\n",(0,r.jsx)(n.li,{children:"Historical reprocessing can be slow"}),"\n",(0,r.jsx)(n.li,{children:"May not suit all use cases"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"medallion-architecture-lakehouse",children:"Medallion Architecture (Lakehouse)"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Concept"}),": Progressive data refinement through layers (Bronze \u2192 Silver \u2192 Gold)."]}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    subgraph "Medallion Architecture - Databricks/Delta Lake"\n        subgraph "Data Sources"\n            Sources[Multiple Sources<br/>APIs, Databases, Logs, IoT]\n        end\n\n        subgraph "Bronze Layer - Raw Zone"\n            Bronze[(Bronze Tables<br/>Raw, unprocessed data<br/>Exact copy from source<br/>All history preserved<br/>Delta/Parquet format)]\n            BronzeNote[Purpose: Data lineage<br/>Audit trail<br/>Reprocessing capability]\n        end\n\n        subgraph "Silver Layer - Refined Zone"\n            Silver[(Silver Tables<br/>Cleaned, validated<br/>Deduplicated<br/>Type-safe<br/>Conformed dimensions)]\n            SilverNote[Purpose: Standardized<br/>Quality assured<br/>Business-ready]\n        end\n\n        subgraph "Gold Layer - Curated Zone"\n            Gold[(Gold Tables<br/>Business-level aggregates<br/>Star schema / Data marts<br/>Feature tables<br/>ML-ready datasets)]\n            GoldNote[Purpose: Optimized for<br/>specific use cases<br/>Low latency queries]\n        end\n\n        subgraph "Consumption"\n            BI[BI & Analytics<br/>Dashboards, Reports]\n            ML[ML & Data Science<br/>Model training]\n            Apps[Applications<br/>Production systems]\n        end\n    end\n\n    Sources --\x3e Bronze\n    Bronze --\x3e Silver\n    Silver --\x3e Gold\n\n    Gold --\x3e BI\n    Silver --\x3e ML\n    Gold --\x3e Apps\n\n    BronzeNote -.-> Bronze\n    SilverNote -.-> Silver\n    GoldNote -.-> Gold\n\n    style Bronze fill:#cd7f32\n    style Silver fill:#c0c0c0\n    style Gold fill:#ffd700'}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Layer Characteristics"}),":"]}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Layer"}),(0,r.jsx)(n.th,{children:"Data Quality"}),(0,r.jsx)(n.th,{children:"Schema"}),(0,r.jsx)(n.th,{children:"Purpose"}),(0,r.jsx)(n.th,{children:"Example"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Bronze"})}),(0,r.jsx)(n.td,{children:"Raw, as-is"}),(0,r.jsx)(n.td,{children:"Flexible"}),(0,r.jsx)(n.td,{children:"Ingestion, lineage"}),(0,r.jsx)(n.td,{children:"Raw JSON logs, CDC events"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Silver"})}),(0,r.jsx)(n.td,{children:"Cleaned, validated"}),(0,r.jsx)(n.td,{children:"Enforced"}),(0,r.jsx)(n.td,{children:"Standardized business data"}),(0,r.jsx)(n.td,{children:"Deduplicated users, validated transactions"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Gold"})}),(0,r.jsx)(n.td,{children:"Aggregated, enriched"}),(0,r.jsx)(n.td,{children:"Optimized"}),(0,r.jsx)(n.td,{children:"Analytics, ML features"}),(0,r.jsx)(n.td,{children:"Daily sales by region, user behavior features"})]})]})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Benefits"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Clear data lineage and quality progression"}),"\n",(0,r.jsx)(n.li,{children:"Different SLAs per layer (Bronze: append-only, Silver: updates allowed, Gold: optimized)"}),"\n",(0,r.jsx)(n.li,{children:"Supports both exploratory and production workloads"}),"\n",(0,r.jsx)(n.li,{children:"Incremental quality improvement"}),"\n",(0,r.jsx)(n.li,{children:"Easy debugging (can trace back through layers)"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Implementation"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Bronze: Raw Kafka events \u2192 Delta table (append-only)\nSilver: Deduplicate, validate, join dimensions \u2192 Delta table (upserts)\nGold: Aggregate by time/dimension, create features \u2192 Delta table (optimized)\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"interview-checklist",children:"6. Interview Checklist"}),"\n",(0,r.jsx)(n.p,{children:"When designing a data analytics platform in an interview:"}),"\n",(0,r.jsx)(n.h3,{id:"step-1-clarify-requirements-5-10-minutes",children:"Step 1: Clarify Requirements (5-10 minutes)"}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TD\n    Start[Start Interview] --\x3e Questions{Ask Clarifying Questions}\n\n    Questions --\x3e Q1[Data Characteristics<br/>\u2022 Volume?<br/>\u2022 Velocity?<br/>\u2022 Variety?]\n    Questions --\x3e Q2[Access Patterns<br/>\u2022 Query types?<br/>\u2022 Latency?<br/>\u2022 Concurrency?]\n    Questions --\x3e Q3[Processing Needs<br/>\u2022 Transformations?<br/>\u2022 Batch vs Stream?<br/>\u2022 Reprocessing?]\n    Questions --\x3e Q4[Scale & Growth<br/>\u2022 Current scale?<br/>\u2022 Growth rate?<br/>\u2022 Geographic?]\n\n    Q1 --\x3e Design[Design Solution]\n    Q2 --\x3e Design\n    Q3 --\x3e Design\n    Q4 --\x3e Design"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Questions to Ask"}),":"]}),"\n",(0,r.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","What are the primary data sources? (databases, APIs, logs, IoT)"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","What's the data volume? (MB/GB/TB per day)"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","What's the data velocity? (real-time, batch, micro-batch)"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","What are the query latency requirements? (milliseconds, seconds, minutes)"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","How many concurrent users? (10s, 100s, 1000s)"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","What are the retention needs? (days, months, years)"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Are there compliance/regulatory requirements? (GDPR, HIPAA)"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","What's the budget? (cost constraints)"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"step-2-design-ingestion-5-10-minutes",children:"Step 2: Design Ingestion (5-10 minutes)"}),"\n",(0,r.jsx)(n.mermaid,{value:'graph LR\n    subgraph "Ingestion Design Decision Tree"\n        Start{Data Source Type}\n\n        Start --\x3e|Databases| CDC[Change Data Capture<br/>Debezium, Maxwell]\n        Start --\x3e|Applications| Queue[Message Queue<br/>Kafka, Kinesis]\n        Start --\x3e|APIs| Batch[Batch Ingestion<br/>Airflow, Fivetran]\n        Start --\x3e|Files| Object[Object Storage<br/>S3, ADLS]\n\n        CDC --\x3e Validate[Schema Validation<br/>Error Handling]\n        Queue --\x3e Validate\n        Batch --\x3e Validate\n        Object --\x3e Validate\n    end'}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Decisions to Make"}),":"]}),"\n",(0,r.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Batch vs streaming ingestion?"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Need for message queue? (Kafka/Kinesis)"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","CDC for databases? (Debezium)"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Schema validation strategy?"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Error handling and dead letter queues?"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Data partitioning strategy?"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"step-3-choose-processing-approach-5-minutes",children:"Step 3: Choose Processing Approach (5 minutes)"}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TD\n    Start{Processing Requirements}\n\n    Start --\x3e|Complex transformations<br/>Large scale| Spark[Apache Spark<br/>Distributed processing]\n    Start --\x3e|Real-time millisecond<br/>latency| Flink[Apache Flink<br/>True streaming]\n    Start --\x3e|SQL transformations<br/>In-warehouse| dbt[dbt / SQL<br/>ELT approach]\n    Start --\x3e|Simple workflows| Airflow[Airflow<br/>Orchestration only]"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Decisions to Make"}),":"]}),"\n",(0,r.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","ETL vs ELT approach?"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Processing engine? (Spark, Flink, dbt)"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Transformation complexity?"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Orchestration tool? (Airflow, Dagster, Prefect)"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Incremental vs full refresh?"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"step-4-select-storage-10-minutes",children:"Step 4: Select Storage (10 minutes)"}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TD\n    Start{Storage Requirements}\n\n    Start --\x3e|Structured analytics<br/>SQL queries| Warehouse[Data Warehouse<br/>Snowflake, BigQuery, Redshift]\n\n    Start --\x3e|Raw data<br/>ML workloads| Lake[Data Lake<br/>S3 + Athena/Glue]\n\n    Start --\x3e|Both analytics + ML<br/>ACID needed| Lakehouse[Lakehouse<br/>Delta, Iceberg, Hudi]\n\n    Start --\x3e|Real-time OLAP| OLAP[OLAP Database<br/>Druid, ClickHouse]\n\n    Start --\x3e|Time-series| TimeSeries[Time-Series DB<br/>InfluxDB, TimescaleDB]"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Decisions to Make"}),":"]}),"\n",(0,r.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Data lake for raw data?"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Warehouse for analytics?"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Lakehouse for unified approach?"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Partitioning strategy? (time-based, key-based)"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Data retention and archival?"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","File format? (Parquet, Avro, ORC)"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"step-5-plan-consumption-5-minutes",children:"Step 5: Plan Consumption (5 minutes)"}),"\n",(0,r.jsx)(n.mermaid,{value:'graph LR\n    subgraph "Consumption Layer"\n        Storage[(Storage Layer)]\n\n        Storage --\x3e BI[BI Tools<br/>Tableau, Looker]\n        Storage --\x3e API[API Layer<br/>REST/GraphQL]\n        Storage --\x3e ML[ML Platforms<br/>Feature serving]\n        Storage --\x3e Export[Data Export<br/>Reports, files]\n    end'}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Decisions to Make"}),":"]}),"\n",(0,r.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","BI tool integration? (Tableau, Looker, Power BI)"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","API layer needed? (REST, GraphQL)"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","ML model serving? (Feature store)"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Data access controls? (RBAC, row-level security)"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Caching strategy?"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"step-6-address-non-functional-requirements-5-10-minutes",children:"Step 6: Address Non-Functional Requirements (5-10 minutes)"}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    subgraph "Non-Functional Requirements"\n        NFR[System Design]\n\n        NFR --\x3e Scale[Scalability<br/>\u2022 Horizontal scaling<br/>\u2022 Auto-scaling<br/>\u2022 Load balancing]\n\n        NFR --\x3e Monitor[Monitoring<br/>\u2022 Data quality checks<br/>\u2022 Pipeline health<br/>\u2022 Cost tracking]\n\n        NFR --\x3e Reliability[Reliability<br/>\u2022 Fault tolerance<br/>\u2022 Disaster recovery<br/>\u2022 Backups]\n\n        NFR --\x3e Security[Security<br/>\u2022 Encryption<br/>\u2022 Access control<br/>\u2022 Audit logs]\n\n        NFR --\x3e Cost[Cost Optimization<br/>\u2022 Storage tiers<br/>\u2022 Compute efficiency<br/>\u2022 Resource scheduling]\n    end'}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Considerations"}),":"]}),"\n",(0,r.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Scalability strategy? (horizontal, vertical)"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Monitoring and alerting? (data quality, pipeline health)"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Data quality checks? (validation, anomaly detection)"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Disaster recovery plan? (backups, replication)"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Security and compliance? (encryption, access control)"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Cost optimization? (tiered storage, auto-scaling)"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"interview-scenarios",children:"7. Common Interview Scenarios"}),"\n",(0,r.jsx)(n.h3,{id:"scenario-1-real-time-analytics-dashboard",children:"Scenario 1: Real-time Analytics Dashboard"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Problem Statement"}),":\n",(0,r.jsx)(n.em,{children:'"Design a system to display user activity metrics on a website in near real-time. The dashboard should show active users, page views, clicks, and conversions updated every few seconds."'})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Requirements Clarification"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Traffic: 100K requests/second"}),"\n",(0,r.jsx)(n.li,{children:"Latency: Display updates within 5-10 seconds"}),"\n",(0,r.jsx)(n.li,{children:"Metrics: Active users (1-min window), page views, clicks, conversions"}),"\n",(0,r.jsx)(n.li,{children:"Retention: Hot data (7 days), historical data (1 year)"}),"\n",(0,r.jsx)(n.li,{children:"Users: 100 concurrent dashboard viewers"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Design"}),":"]}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    subgraph "Real-time Analytics System"\n        subgraph "Data Sources"\n            Web[Web Application<br/>React/Angular]\n            Mobile[Mobile Apps<br/>iOS/Android]\n        end\n\n        subgraph "Ingestion"\n            LB[Load Balancer]\n            Gateway[API Gateway<br/>Event tracking endpoint]\n            Kafka[Kafka Topics<br/>\u2022 user-events<br/>\u2022 page-views<br/>\u2022 clicks]\n        end\n\n        subgraph "Stream Processing"\n            Flink[Apache Flink<br/>Windowed Aggregations<br/>5-second tumbling windows]\n            State[State Backend<br/>RocksDB]\n        end\n\n        subgraph "Hot Storage"\n            Redis[(Redis<br/>Last 5 minutes<br/>Sub-millisecond reads)]\n        end\n\n        subgraph "Historical Storage"\n            Kafka2[Kafka<br/>Raw events]\n            Spark[Spark Streaming<br/>Micro-batch processing]\n            Snowflake[(Snowflake<br/>Historical analytics)]\n        end\n\n        subgraph "API & UI"\n            API[Dashboard API<br/>WebSocket/SSE]\n            Dashboard[React Dashboard<br/>Real-time charts]\n        end\n    end\n\n    Web --\x3e LB\n    Mobile --\x3e LB\n    LB --\x3e Gateway\n    Gateway --\x3e Kafka\n\n    Kafka --\x3e Flink\n    Flink --\x3e State\n    Flink --\x3e Redis\n\n    Kafka --\x3e Kafka2\n    Kafka2 --\x3e Spark\n    Spark --\x3e Snowflake\n\n    Redis --\x3e API\n    Snowflake --\x3e API\n    API --\x3e Dashboard'}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Key Decisions"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Ingestion Layer"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Kafka for high-throughput event ingestion (handles 100K events/sec easily)"}),"\n",(0,r.jsx)(n.li,{children:"Partitioned by user_id for parallel processing"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Stream Processing"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Flink for stateful stream processing with exactly-once guarantees"}),"\n",(0,r.jsx)(n.li,{children:"5-second tumbling windows for aggregations"}),"\n",(0,r.jsx)(n.li,{children:"Maintains state for active user tracking (1-minute window)"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Storage Strategy"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Redis"}),": Hot data (last 5 minutes) for dashboard queries"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Snowflake"}),": Historical data for trend analysis"]}),"\n",(0,r.jsx)(n.li,{children:"Dual write pattern: real-time to Redis, batch to Snowflake"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"API Layer"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"WebSocket/Server-Sent Events for pushing updates to dashboard"}),"\n",(0,r.jsx)(n.li,{children:"Queries Redis for real-time metrics"}),"\n",(0,r.jsx)(n.li,{children:"Queries Snowflake for historical context"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Scalability"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Kafka partitions: 10 partitions for parallelism"}),"\n",(0,r.jsx)(n.li,{children:"Flink parallelism: 5 task managers"}),"\n",(0,r.jsx)(n.li,{children:"Redis cluster: 3 nodes with replication"}),"\n",(0,r.jsx)(n.li,{children:"Auto-scaling for API layer based on WebSocket connections"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"scenario-2-data-lake-for-machine-learning",children:"Scenario 2: Data Lake for Machine Learning"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Problem Statement"}),":\n",(0,r.jsx)(n.em,{children:'"Design a data lake to store and process diverse data sources for training machine learning models. Data includes user behavior logs, transaction records, product catalog, and customer support tickets."'})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Requirements"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Data sources: Logs (JSON), Databases (MySQL), Files (CSV), Text (tickets)"}),"\n",(0,r.jsx)(n.li,{children:"Volume: 10 TB/day"}),"\n",(0,r.jsx)(n.li,{children:"ML use case: Customer churn prediction, product recommendations"}),"\n",(0,r.jsx)(n.li,{children:"Data scientists: 50 users running ad-hoc queries"}),"\n",(0,r.jsx)(n.li,{children:"Training frequency: Daily batch jobs"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Design"}),":"]}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    subgraph "ML Data Lake Architecture"\n        subgraph "Data Sources"\n            Logs[Application Logs<br/>JSON, 5 TB/day]\n            DB[(MySQL<br/>Transactions)]\n            Files[CSV Files<br/>Product catalog]\n            Tickets[Support Tickets<br/>Unstructured text]\n        end\n\n        subgraph "Ingestion Layer"\n            Logstash[Logstash/Fluentd<br/>Log aggregation]\n            CDC[Debezium CDC<br/>Database changes]\n            S3Upload[S3 Upload<br/>File ingestion]\n        end\n\n        subgraph "Raw Storage - Bronze"\n            S3Bronze[S3 Bucket: raw-data<br/>\u2022 Partitioned by date<br/>\u2022 Original format<br/>\u2022 Immutable]\n        end\n\n        subgraph "Processing Layer"\n            Glue[AWS Glue<br/>ETL Jobs<br/>Spark-based]\n            Crawler[Glue Crawler<br/>Schema discovery]\n        end\n\n        subgraph "Curated Storage - Silver/Gold"\n            S3Silver[S3 Bucket: curated-data<br/>\u2022 Parquet format<br/>\u2022 Partitioned<br/>\u2022 Validated]\n\n            FeatureStore[Feature Store<br/>SageMaker Feature Store<br/>\u2022 Training features<br/>\u2022 Online serving]\n        end\n\n        subgraph "Metadata & Governance"\n            GlueCatalog[Glue Data Catalog<br/>Schema registry<br/>Table metadata]\n            Lineage[Data Lineage<br/>Apache Atlas]\n        end\n\n        subgraph "Consumption"\n            Athena[Athena<br/>Ad-hoc SQL queries]\n            Jupyter[JupyterHub<br/>Data exploration]\n            SageMaker[SageMaker<br/>Model training]\n            Airflow[Airflow<br/>Orchestration]\n        end\n    end\n\n    Logs --\x3e Logstash\n    DB --\x3e CDC\n    Files --\x3e S3Upload\n    Tickets --\x3e S3Upload\n\n    Logstash --\x3e S3Bronze\n    CDC --\x3e S3Bronze\n    S3Upload --\x3e S3Bronze\n\n    S3Bronze --\x3e Crawler\n    Crawler --\x3e GlueCatalog\n\n    S3Bronze --\x3e Glue\n    Glue --\x3e S3Silver\n    S3Silver --\x3e FeatureStore\n\n    GlueCatalog --\x3e Athena\n    S3Silver --\x3e Athena\n\n    Athena --\x3e Jupyter\n    S3Silver --\x3e SageMaker\n    FeatureStore --\x3e SageMaker\n\n    Airflow -.-> Glue\n    Airflow -.-> SageMaker\n\n    GlueCatalog --\x3e Lineage'}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Key Decisions"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Storage Format"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Bronze (Raw)"}),": Original format (JSON, CSV) for lineage"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Silver (Curated)"}),": Parquet format for efficient analytics"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Partitioning"}),": By date (",(0,r.jsx)(n.code,{children:"/year=2024/month=01/day=15/"}),")"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Schema Management"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Glue Crawler for automatic schema discovery"}),"\n",(0,r.jsx)(n.li,{children:"Glue Data Catalog as centralized metadata store"}),"\n",(0,r.jsx)(n.li,{children:"Schema evolution support with Parquet"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Feature Engineering"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Spark jobs in AWS Glue for complex transformations"}),"\n",(0,r.jsx)(n.li,{children:"Feature Store for ML features (training + serving)"}),"\n",(0,r.jsx)(n.li,{children:"Separate training and inference pipelines"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Access Patterns"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Data Scientists"}),": Athena for SQL exploration, Jupyter notebooks"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ML Training"}),": SageMaker reading from S3/Feature Store"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Orchestration"}),": Airflow for scheduling ETL and training jobs"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Data Governance"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Data lineage tracking with Apache Atlas"}),"\n",(0,r.jsx)(n.li,{children:"S3 lifecycle policies for cost optimization (hot \u2192 cold storage)"}),"\n",(0,r.jsx)(n.li,{children:"IAM roles for fine-grained access control"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Cost Optimization"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"S3 Intelligent-Tiering for automatic cost optimization"}),"\n",(0,r.jsx)(n.li,{children:"Parquet compression (Snappy) reduces storage by 70%"}),"\n",(0,r.jsx)(n.li,{children:"Athena charges per query (pay only for scanned data)"}),"\n",(0,r.jsx)(n.li,{children:"Spot instances for Spark/SageMaker training jobs"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"scenario-3-enterprise-data-warehouse",children:"Scenario 3: Enterprise Data Warehouse"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Problem Statement"}),":\n",(0,r.jsx)(n.em,{children:'"Design a centralized data warehouse for a retail company. The warehouse should consolidate data from multiple OLTP databases, support 500 concurrent BI users, and enable complex analytical queries for sales reporting, inventory management, and customer analytics."'})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Requirements"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Data sources: 5 MySQL databases (orders, inventory, customers, products, stores)"}),"\n",(0,r.jsx)(n.li,{children:"Update frequency: Every 15 minutes"}),"\n",(0,r.jsx)(n.li,{children:"Query complexity: Complex joins across 10+ tables, window functions"}),"\n",(0,r.jsx)(n.li,{children:"Users: 500 concurrent BI users"}),"\n",(0,r.jsx)(n.li,{children:"SLA: 95% of queries under 10 seconds"}),"\n",(0,r.jsx)(n.li,{children:"Data retention: 5 years"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Design"}),":"]}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    subgraph "Enterprise Data Warehouse"\n        subgraph "Source Systems - OLTP"\n            Orders[(Orders DB<br/>MySQL)]\n            Inventory[(Inventory DB<br/>MySQL)]\n            Customers[(Customers DB<br/>MySQL)]\n            Products[(Products DB<br/>MySQL)]\n            Stores[(Stores DB<br/>MySQL)]\n        end\n\n        subgraph "Change Data Capture"\n            Debezium[Debezium CDC<br/>Captures changes]\n            Kafka[Kafka<br/>Change events<br/>15-min retention]\n        end\n\n        subgraph "ETL Processing"\n            Spark[Spark Streaming<br/>\u2022 Joins dimensions<br/>\u2022 SCD Type 2<br/>\u2022 Data quality checks]\n        end\n\n        subgraph "Staging Area"\n            S3[(S3 Staging<br/>Parquet files)]\n        end\n\n        subgraph "Data Warehouse - Snowflake"\n            subgraph "ETL Warehouse"\n                ETL_WH[ETL Virtual Warehouse<br/>X-Large<br/>Runs every 15 min]\n            end\n\n            subgraph "Star Schema"\n                FactSales[Fact: Sales<br/>Clustered by date]\n                FactInventory[Fact: Inventory<br/>Clustered by store_id]\n\n                DimCustomer[Dim: Customer<br/>SCD Type 2]\n                DimProduct[Dim: Product]\n                DimStore[Dim: Store]\n                DimDate[Dim: Date]\n            end\n\n            subgraph "BI Warehouse"\n                BI_WH[BI Virtual Warehouse<br/>Large<br/>Multi-cluster 1-10<br/>Auto-scale]\n            end\n\n            subgraph "Aggregates"\n                MV1[Materialized View<br/>Daily sales by region]\n                MV2[Materialized View<br/>Weekly inventory status]\n            end\n        end\n\n        subgraph "Semantic Layer"\n            dbt[dbt<br/>\u2022 Business logic<br/>\u2022 Metrics definitions<br/>\u2022 Data tests]\n        end\n\n        subgraph "BI Tools"\n            Tableau[Tableau<br/>Executive dashboards]\n            Looker[Looker<br/>Self-service analytics]\n            PowerBI[Power BI<br/>Operational reports]\n        end\n    end\n\n    Orders --\x3e Debezium\n    Inventory --\x3e Debezium\n    Customers --\x3e Debezium\n    Products --\x3e Debezium\n    Stores --\x3e Debezium\n\n    Debezium --\x3e Kafka\n    Kafka --\x3e Spark\n    Spark --\x3e S3\n\n    S3 --\x3e ETL_WH\n\n    ETL_WH --\x3e FactSales\n    ETL_WH --\x3e FactInventory\n    ETL_WH --\x3e DimCustomer\n    ETL_WH --\x3e DimProduct\n    ETL_WH --\x3e DimStore\n    ETL_WH --\x3e DimDate\n\n    FactSales -.-> MV1\n    FactInventory -.-> MV2\n\n    MV1 --\x3e dbt\n    MV2 --\x3e dbt\n    FactSales --\x3e dbt\n\n    dbt --\x3e BI_WH\n\n    BI_WH --\x3e Tableau\n    BI_WH --\x3e Looker\n    BI_WH --\x3e PowerBI'}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Key Decisions"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Data Ingestion Strategy"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"CDC with Debezium"}),": Captures changes from MySQL databases"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Kafka"}),": Buffers change events, enables replay"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"15-minute micro-batches"}),": Balances freshness with load"]}),"\n",(0,r.jsx)(n.li,{children:"Separate ETL warehouse from BI warehouse for workload isolation"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Data Modeling"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Star schema"})," for query performance (denormalized dimensions)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"SCD Type 2"})," for customer dimension (track history)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Clustering keys"}),": ",(0,r.jsx)(n.code,{children:"date"})," for fact_sales, ",(0,r.jsx)(n.code,{children:"store_id"})," for fact_inventory"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Materialized views"}),": Pre-compute common aggregations"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Snowflake Architecture"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ETL Warehouse"}),": X-Large, scheduled for 15-min loads"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"BI Warehouse"}),": Large with multi-cluster (1-10) for 500 users"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Auto-scaling"}),": Automatically adds clusters during peak usage"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Auto-suspend"}),": Warehouses suspend after 5 minutes of inactivity"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Performance Optimization"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Clustering keys on large tables (>100 GB)"}),"\n",(0,r.jsx)(n.li,{children:"Materialized views for daily/weekly aggregations"}),"\n",(0,r.jsx)(n.li,{children:"Result caching (24 hours)"}),"\n",(0,r.jsx)(n.li,{children:"Search optimization service for point lookups"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Semantic Layer (dbt)"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Centralized business logic and metric definitions"}),"\n",(0,r.jsx)(n.li,{children:"Data quality tests (not null, unique, referential integrity)"}),"\n",(0,r.jsx)(n.li,{children:"Documentation and lineage"}),"\n",(0,r.jsx)(n.li,{children:"Incremental models for large fact tables"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"High Availability"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Snowflake multi-AZ deployment"}),"\n",(0,r.jsx)(n.li,{children:"Kafka replication factor: 3"}),"\n",(0,r.jsx)(n.li,{children:"Spark cluster with multiple workers"}),"\n",(0,r.jsx)(n.li,{children:"BI tool load balancing"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Query Performance"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"95th percentile query time: 8 seconds"}),"\n",(0,r.jsx)(n.li,{children:"Cache hit rate: 60% (repeated queries)"}),"\n",(0,r.jsx)(n.li,{children:"Concurrent query support: 500+ users"}),"\n",(0,r.jsxs)(n.li,{children:["Auto-scaling responds to load in ",(0,r.jsx)(n.code,{children:"<1"})," minute"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},28453:(e,n,s)=>{s.d(n,{R:()=>t,x:()=>l});var r=s(96540);const i={},a=r.createContext(i);function t(e){const n=r.useContext(a);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:t(e.components),r.createElement(a.Provider,{value:n},e.children)}}}]);