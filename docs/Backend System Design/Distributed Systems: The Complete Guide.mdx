# Distributed Systems: The Complete Guide

## 1. What is a Distributed System?

A **distributed system** is a collection of independent computers (nodes/servers) that appear to the user as a single system. These nodes communicate via a network and work together to achieve a common goal.

### Examples
- **Google Search** → billions of queries handled by thousands of servers globally
- **Netflix** → streaming movies from servers close to you (CDNs)

## 2. Why Distributed Systems?

- **Scalability** → handle millions of users by adding servers
- **Fault Tolerance** → if one server fails, the system keeps running
- **Low Latency** → bring services closer to users (e.g., CDNs)
- **Cost Efficiency** → commodity hardware instead of giant supercomputers

## 3. Key Characteristics

- **Transparency**: Users don't know if data is on one machine or many
- **Concurrency**: Many users/tasks run simultaneously
- **Fault tolerance**: Survives machine/network failures
- **Scalability**: Can grow horizontally by adding more machines

## 4. Challenges in Distributed Systems

- Network latency & partitioning (messages may be delayed, dropped, or duplicated)
- Consistency across replicas (everyone should see the same data)
- Fault tolerance (what if a server crashes during a transaction?)
- Coordination between nodes
- Security (data traveling across networks)

## 5. Core Concepts

### CAP Theorem

Any distributed system can only guarantee **two out of three**:

- **Consistency** → every user sees the same data
- **Availability** → system always responds
- **Partition tolerance** → works even if network splits

#### Examples:
- **CP (Consistency + Partition Tolerance)** → MongoDB, HBase
- **AP (Availability + Partition Tolerance)** → DynamoDB, Cassandra

### Data Replication

- **Master-Slave (Primary-Replica)**: one node writes, others read
- **Multi-Master**: multiple nodes can write (more complex conflict resolution)

### Consensus

How do nodes agree on a value despite failures?

- **Paxos**
- **Raft**
- **ZAB** (used in ZooKeeper)

### Time & Ordering

- **Clock skew** → different servers have different times
- **Lamport Timestamps** & **Vector Clocks** help order events without perfect clocks

### Fault Tolerance

- **Replication** → multiple copies of data
- **Leader election** → pick a new leader if current one fails
- **Retries & Timeouts** → clients retry requests

## 6. Types of Distributed Systems

- **Distributed Databases** (Cassandra, MongoDB, DynamoDB)
- **Distributed File Systems** (HDFS, Google File System)
- **Distributed Messaging** (Kafka, RabbitMQ, Pulsar)
- **Microservices** (Netflix, Uber)
- **Peer-to-Peer** (BitTorrent, Blockchain)

## 7. Popular Architectures

- **Client-Server** → classic model (browser ↔ server)
- **Peer-to-Peer** → no central authority (BitTorrent)
- **Microservices** → services talk via APIs
- **Event-driven** → Kafka-based pipelines

## 8. Design Principles

- **Idempotency** → retrying an operation should not break things
- **Loose coupling** → components should be independent
- **Resiliency patterns**: Circuit breaker, Retry, Bulkhead, Fail-fast
- **Eventual Consistency** → common in AP systems

## 9. Real-World Systems

- **Google Spanner** → globally consistent DB
- **Amazon DynamoDB** → highly available NoSQL
- **Apache Kafka** → distributed event streaming
- **Hadoop HDFS** → distributed storage for big data

## 10. Distributed System in Practice

### Example: Loading your Instagram feed

1. Request goes to **load balancer**
2. Forwarded to nearest **application server**
3. Posts fetched from **distributed database** (replicas for availability)
4. Images served from **CDN** (Content Delivery Network)
5. Notifications handled by **message queues**

## 11. Learning Roadmap

### Foundations
- Networking basics (TCP, UDP, HTTP, RPC, gRPC)
- OS concepts (threads, processes, concurrency)

### Core Distributed Concepts
- CAP theorem, Consensus, Leader election
- Fault tolerance, Replication, Eventual consistency

### Systems & Tools
- **Messaging**: Kafka, ZooKeeper, Redis Cluster
- **Databases**: Cassandra, DynamoDB, MongoDB

### Practice Design
- Design a URL shortener
- Design a chat app (WhatsApp)
- Design Netflix / YouTube

## 12. Interview Tips

- Always ask about **scale** (users, requests/sec, data size)
- Understand **trade-offs**: Consistency vs Availability
- Know **fault tolerance strategies**
- Be ready to **draw diagrams** (replication, sharding)

---

## Quick Reference

| Concept | Description | Examples |
|---------|-------------|----------|
| **CAP Theorem** | Choose 2: Consistency, Availability, Partition Tolerance | CP: MongoDB, AP: Cassandra |
| **Consensus** | Algorithm for nodes to agree | Paxos, Raft, ZAB |
| **Replication** | Multiple copies of data | Master-Slave, Multi-Master |
| **Fault Tolerance** | System survives failures | Leader election, Retries |

# Fault Tolerance Strategies in Distributed Systems

## Core Fault Tolerance Strategies

### 1. Redundancy
Have extra components so if one fails, another takes over.

- **Hardware redundancy** → multiple servers, disks, power supplies
- **Software redundancy** → replicas of services or databases

**Examples:**
- Netflix keeps multiple server copies of the same video file
- RAID for disks (mirroring, striping with parity)

### 2. Replication
Store data in multiple places to survive failures.

- **Active replication**: All replicas serve requests simultaneously
- **Passive replication**: One leader handles requests, others stand by

**Examples:**
- Primary-Replica DB setup
- Kafka keeps multiple copies of logs across brokers

### 3. Failover & Leader Election
Automatically switch to a backup if the main service fails.

- **Static failover** → pre-defined backup
- **Dynamic failover** → system elects a new leader
- **Tools:** ZooKeeper, etcd, Consul

**Example:**
- If a Kafka broker (leader) dies, ZooKeeper elects a new leader for the partition

### 4. Retry, Timeout & Backoff
Handle temporary failures gracefully.

- **Retry** the request after failure
- **Timeouts** prevent waiting forever
- **Exponential backoff** → retry with increasing delays

**Example:**
- HTTP client retries API calls with exponential backoff

### 5. Idempotency
An operation can be retried without unintended side effects.

**Example:**
- Payment API: `POST /charge` should not double-charge if retried
- Instead, use an **idempotency key**

### 6. Quorum & Voting
Require agreement from a majority of replicas before committing.

- **Read/Write Quorums** ensure data consistency even if some nodes fail

**Example:**
- Cassandra/DynamoDB use quorum reads/writes (`R + W > N`)

### 7. Consensus Protocols
Nodes agree on a value even in presence of failures.

- **Paxos, Raft, Viewstamped Replication**

**Example:**
- Raft ensures replicated state machines stay consistent across failures

### 8. Graceful Degradation
The system continues to operate in a limited mode.

**Example:**
- Netflix disables personalized recommendations if the ML service is down, but streaming still works

### 9. Circuit Breakers
Stop calling a failing service until it recovers.

- Prevents cascading failures

**Example:**
- In microservices, if the payment service is down, the order service returns "Payment unavailable" instead of hanging

### 10. Checkpointing & Rollback
Save progress periodically so you can restart from a safe point.

**Example:**
- Hadoop/Spark jobs checkpoint data so if a node fails, they restart from the last checkpoint

### 11. Chaos Engineering (Proactive Strategy)
Intentionally break things to ensure fault tolerance works.

**Example:** Netflix's **Chaos Monkey** randomly kills servers to test resilience

## Fault Tolerance Strategy Summary

| Strategy | Goal | Example in Real Systems |
|----------|------|-------------------------|
| **Redundancy** | Backup hardware/software | RAID, multi-server setups |
| **Replication** | Extra copies of data/services | Kafka, MongoDB replicas |
| **Failover & Leader Election** | Auto-switch to backup | ZooKeeper, Kubernetes HA |
| **Retry/Timeout/Backoff** | Survive temporary errors | HTTP API retries |
| **Idempotency** | Safe retries | Payment APIs |
| **Quorum & Voting** | Majority agreement | Cassandra, DynamoDB |
| **Consensus** | Agreement in failures | Raft, Paxos |
| **Graceful Degradation** | Partial functionality | Netflix recommendations |
| **Circuit Breakers** | Prevent cascading failures | Hystrix, Resilience4j |
| **Checkpointing** | Recover progress | Hadoop, Spark |
| **Chaos Engineering** | Test resilience proactively | Netflix Chaos Monkey |

# Fault Tolerance Playbook (For System Design Interviews)

## 1. Web Applications
**Example: Designing Instagram/WhatsApp**

### Failure Scenarios & Strategies
- **Web server crashes** → Load balancer routes requests to another server
- **Too much traffic** → Auto-scaling adds new servers
- **Session loss** → Store sessions in Redis (replicated)

> **Interview Tip:** Always mention **load balancers + auto-scaling + caching** when designing user-facing apps.

## 2. Databases
**Example: Design Amazon DynamoDB / Cassandra**

### Failure Scenarios & Strategies
- **Primary DB down** → Replica takes over (failover)
- **Data center loss** → Multi-region replication
- **Conflicting writes** → Use **quorums** (`R+W > N`) or vector clocks

> **Interview Tip:** Explain **Replication Factor (RF)** and **quorum reads/writes** when asked about fault tolerance in databases.

## 3. Messaging Systems
**Example: Design Kafka / RabbitMQ**

### Failure Scenarios & Strategies
- **Broker fails** → Partitions have replicas, new leader is elected
- **Consumer crashes** → Offsets stored in Kafka, consumer restarts from last committed offset
- **Producer retries** → With idempotency enabled, no duplicate messages

> **Interview Tip:** Always highlight **leader election + replication + offset recovery** in messaging systems.

## 4. Microservices
**Example: Design Netflix / Uber backend**

### Failure Scenarios & Strategies
- **One service fails** → Circuit breaker prevents cascading failure
- **Network latency** → Retry with exponential backoff
- **High load** → Queue requests (message broker)

> **Interview Tip:** Use **graceful degradation**:
> - If recommendation service fails, just show trending movies
> - If payment service fails, allow users to save orders but mark them as "unpaid"

## 5. Storage Systems
**Example: Design Google Drive / Dropbox**

### Failure Scenarios & Strategies
- **File server dies** → Data replicated across multiple servers (erasure coding / 3x replication)
- **User upload interrupted** → Resume from checkpoint (multipart uploads)
- **Data corruption** → Checksums detect & repair from replicas

> **Interview Tip:** Talk about **replication across availability zones (AZs)** and **checksums for data integrity**.

## 6. Global Applications
**Example: Design WhatsApp / Netflix worldwide**

### Failure Scenarios & Strategies
- **Region goes offline** → Traffic routed to another region via DNS load balancing
- **Cross-region latency** → CDNs cache content near users
- **User consistency issues** → Eventual consistency with conflict resolution

> **Interview Tip:** If interviewer asks *"What if an entire data center goes down?"*, answer:
> - Multi-region replication (active-active or active-passive)
> - Global load balancers (Anycast DNS, GSLB)

## 7. Fault Injection & Testing
**Example: Netflix Chaos Monkey**

### Failure Scenarios & Strategies
- Randomly kill servers → System should keep running
- Inject latency → Ensure retries & backoff work
- Shut down a region → Verify failover

> **Interview Tip:** Mention **Chaos Engineering** as a proactive fault tolerance technique — it shows senior-level thinking.

## Interview Answer Template

When asked: *"How is your system fault tolerant?"* — reply in this structure:

1. **Component Redundancy**: multiple servers, load balancers
2. **Data Replication**: across machines & regions
3. **Automatic Failover**: leader election / replica promotion
4. **Retries & Idempotency**: safe recovery from transient failures
5. **Graceful Degradation**: partial functionality if a service is down
6. **Monitoring & Testing**: health checks, chaos testing

## Mini Example: WhatsApp Message Delivery

**Question:** *"How does WhatsApp ensure a message isn't lost if a server fails?"*

### Answer Outline:
- Message stored in **multiple replicas** (Kafka / database)
- Producer uses **acknowledgments (acks=all)** before confirming send
- If consumer (receiver's phone) is offline, message stored in **queue** until delivery
- If primary server dies, **replica takes over (leader election)**
- If all replicas die (rare), client retries (idempotent message ID)

## Key Takeaways

> **Distributed systems are about trade-offs.** You can't have perfect consistency, availability, and partition tolerance all at once. Choose wisely based on your use case.

- **Start simple** → Add distribution when you need it
- **Plan for failure** → Everything will fail eventually
- **Monitor everything** → Observability is crucial
- **Test failure scenarios** → Chaos engineering
